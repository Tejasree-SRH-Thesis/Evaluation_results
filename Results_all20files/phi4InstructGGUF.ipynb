{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5fddb27236fb414d80512f3ee76837a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_492a93164b68466aa083fe5386fe02e9"
          }
        },
        "5d7cac250d54420897c9e4c368ed67fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31f581ea4972402aa61b62545f7c3b23",
            "placeholder": "​",
            "style": "IPY_MODEL_9a665019ccd7434ab5d4428524257e0b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "43f024d93e734263a4be1f7b3e4c982f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_53cebe840a664b35adcb5a7d2287a74b",
            "placeholder": "​",
            "style": "IPY_MODEL_10d02beb4caf42acb27eb8fcb0d7ee80",
            "value": ""
          }
        },
        "fd118fdd97db4b7fa3522a1661325825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_c20efe00b0fd4644a595c50400e9f154",
            "style": "IPY_MODEL_7380ec1adacf493b8b667a1e71280444",
            "value": true
          }
        },
        "ab9eca826c3547a98017c7eeeafccba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_330796ec7f114c39b187f8be555603a2",
            "style": "IPY_MODEL_b4501130ffc94c95a45ea4eafb09b3eb",
            "tooltip": ""
          }
        },
        "d3e08848e443420b802e58b41d6bd432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_640da7e5cfab4aa98538621afb21c598",
            "placeholder": "​",
            "style": "IPY_MODEL_35a75f2dab964e8eb695afb139d02677",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "492a93164b68466aa083fe5386fe02e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "31f581ea4972402aa61b62545f7c3b23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a665019ccd7434ab5d4428524257e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53cebe840a664b35adcb5a7d2287a74b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10d02beb4caf42acb27eb8fcb0d7ee80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c20efe00b0fd4644a595c50400e9f154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7380ec1adacf493b8b667a1e71280444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "330796ec7f114c39b187f8be555603a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4501130ffc94c95a45ea4eafb09b3eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "640da7e5cfab4aa98538621afb21c598": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35a75f2dab964e8eb695afb139d02677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "730c8c8f9ee34075bbf752ea0c4bd549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d54cd5f659c846a7a9ca08a6c59671f8",
            "placeholder": "​",
            "style": "IPY_MODEL_61043120995b4790acc68954b5f0d6fe",
            "value": "Connecting..."
          }
        },
        "d54cd5f659c846a7a9ca08a6c59671f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61043120995b4790acc68954b5f0d6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f22c7abab8234f2e82d3e58c1aa1a7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dc93f34282a448596b39bb728d6724c",
              "IPY_MODEL_0d29c1562427474f9601b6a718ae8ef5",
              "IPY_MODEL_568ca67ea07d4922a1c05dc3e5a56a1b"
            ],
            "layout": "IPY_MODEL_d7592dc6138c4337b35b5fff5c6dc5b4"
          }
        },
        "1dc93f34282a448596b39bb728d6724c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0b8449bd58d4313924c66cbb28079d8",
            "placeholder": "​",
            "style": "IPY_MODEL_4813207dcf0b4a74a1f6a4f36b0b9e63",
            "value": "Phi-4-mini-instruct.Q4_K_S.gguf: 100%"
          }
        },
        "0d29c1562427474f9601b6a718ae8ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb3cfbeec9b743a4a595ebc7418eef04",
            "max": 2337733952,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1cd61414859442bb9f619bb4bb5ffb3",
            "value": 2337733952
          }
        },
        "568ca67ea07d4922a1c05dc3e5a56a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45a13d84ecd14c59a7050fcbdb8ad852",
            "placeholder": "​",
            "style": "IPY_MODEL_38247b5d24f04ae7a5281247ff603046",
            "value": " 2.34G/2.34G [00:15&lt;00:00, 228MB/s]"
          }
        },
        "d7592dc6138c4337b35b5fff5c6dc5b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0b8449bd58d4313924c66cbb28079d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4813207dcf0b4a74a1f6a4f36b0b9e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb3cfbeec9b743a4a595ebc7418eef04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1cd61414859442bb9f619bb4bb5ffb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45a13d84ecd14c59a7050fcbdb8ad852": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38247b5d24f04ae7a5281247ff603046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNAm5n6J6XRA",
        "outputId": "48c2855f-3b31-4250-d9ad-8dd7822eb9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# ✅ STEP 1: Install requirements\n",
        "!pip install llama-cpp-python==0.2.24 PyMuPDF tqdm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA9Gyyif6lTF",
        "outputId": "e83311ca-1600-4410-e8df-e51eb4a988d8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.2.24)\n",
            "Collecting llama-cpp-python\n",
            "  Using cached llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Installing collected packages: llama-cpp-python\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.24\n",
            "    Uninstalling llama_cpp_python-0.2.24:\n",
            "      Successfully uninstalled llama_cpp_python-0.2.24\n",
            "Successfully installed llama-cpp-python-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token when prompted\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "5fddb27236fb414d80512f3ee76837a0",
            "5d7cac250d54420897c9e4c368ed67fd",
            "43f024d93e734263a4be1f7b3e4c982f",
            "fd118fdd97db4b7fa3522a1661325825",
            "ab9eca826c3547a98017c7eeeafccba8",
            "d3e08848e443420b802e58b41d6bd432",
            "492a93164b68466aa083fe5386fe02e9",
            "31f581ea4972402aa61b62545f7c3b23",
            "9a665019ccd7434ab5d4428524257e0b",
            "53cebe840a664b35adcb5a7d2287a74b",
            "10d02beb4caf42acb27eb8fcb0d7ee80",
            "c20efe00b0fd4644a595c50400e9f154",
            "7380ec1adacf493b8b667a1e71280444",
            "330796ec7f114c39b187f8be555603a2",
            "b4501130ffc94c95a45ea4eafb09b3eb",
            "640da7e5cfab4aa98538621afb21c598",
            "35a75f2dab964e8eb695afb139d02677",
            "730c8c8f9ee34075bbf752ea0c4bd549",
            "d54cd5f659c846a7a9ca08a6c59671f8",
            "61043120995b4790acc68954b5f0d6fe"
          ]
        },
        "id": "doPqw9QR6vxa",
        "outputId": "187aefb2-4307-4f33-f292-993614ca8a1b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fddb27236fb414d80512f3ee76837a0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"MaziyarPanahi/Phi-4-mini-instruct-GGUF\",\n",
        "    filename=\"Phi-4-mini-instruct.Q4_K_S.gguf\",\n",
        "    local_dir=\"/content/models/phi\",\n",
        "    local_dir_use_symlinks=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "f22c7abab8234f2e82d3e58c1aa1a7c3",
            "1dc93f34282a448596b39bb728d6724c",
            "0d29c1562427474f9601b6a718ae8ef5",
            "568ca67ea07d4922a1c05dc3e5a56a1b",
            "d7592dc6138c4337b35b5fff5c6dc5b4",
            "b0b8449bd58d4313924c66cbb28079d8",
            "4813207dcf0b4a74a1f6a4f36b0b9e63",
            "eb3cfbeec9b743a4a595ebc7418eef04",
            "b1cd61414859442bb9f619bb4bb5ffb3",
            "45a13d84ecd14c59a7050fcbdb8ad852",
            "38247b5d24f04ae7a5281247ff603046"
          ]
        },
        "id": "6UmMBz7Q6zkc",
        "outputId": "e55239db-ea7d-4c8b-b352-7c3b20f7d5a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:933: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Phi-4-mini-instruct.Q4_K_S.gguf:   0%|          | 0.00/2.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f22c7abab8234f2e82d3e58c1aa1a7c3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "def load_phi_model(model_path):\n",
        "    try:\n",
        "        llm = Llama(\n",
        "            model_path=model_path,  # Path to your GGUF model\n",
        "            n_ctx=4096,             # Context window size\n",
        "            n_threads=8,            # Number of threads\n",
        "            n_gpu_layers=0,         # Set to 0 if using CPU-only\n",
        "            use_mmap=False,         # Use memory-mapped file (disabled here)\n",
        "            verbose=True            # Set to True for verbose output\n",
        "        )\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(\"Error loading model:\", e)\n",
        "        return None\n",
        "\n",
        "model_path = \"/content/models/phi/Phi-4-mini-instruct.Q4_K_S.gguf\"\n",
        "model = load_phi_model(model_path)\n",
        "\n",
        "if model is None:\n",
        "    print(\"Model loading failed.\")\n",
        "else:\n",
        "    print(\"Model loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R88hNHy7EB3",
        "outputId": "bdf258b5-7d2f-4b27-cb53-9f32772b30c7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 36 key-value pairs and 196 tensors from /content/models/phi/Phi-4-mini-instruct.Q4_K_S.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
            "llama_model_loader: - kv   2:                               general.type str              = model\n",
            "llama_model_loader: - kv   3:                               general.name str              = Phi 4 Mini Instruct\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Phi-4\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   7:                            general.license str              = mit\n",
            "llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\n",
            "llama_model_loader: - kv   9:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\n",
            "llama_model_loader: - kv  10:                          general.languages arr[str,24]      = [\"multilingual\", \"ar\", \"zh\", \"cs\", \"d...\n",
            "llama_model_loader: - kv  11:                        phi3.context_length u32              = 131072\n",
            "llama_model_loader: - kv  12:  phi3.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  13:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  14:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  15:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv  16:                  phi3.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  17:               phi3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  18:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  20:                        phi3.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  21:                          general.file_type u32              = 14\n",
            "llama_model_loader: - kv  22:              phi3.attention.sliding_window u32              = 262144\n",
            "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = gpt-4o\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,199742]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"e r\", ...\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 199999\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 199999\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 199999\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 199999\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   67 tensors\n",
            "llama_model_loader: - type q4_K:  124 tensors\n",
            "llama_model_loader: - type q5_K:    4 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Small\n",
            "print_info: file size   = 2.17 GiB (4.86 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 200024 '<|/tool|>' is not marked as EOG\n",
            "load: control token: 200023 '<|tool|>' is not marked as EOG\n",
            "load: control token: 200022 '<|system|>' is not marked as EOG\n",
            "load: control token: 200021 '<|user|>' is not marked as EOG\n",
            "load: control token: 200025 '<|tool_call|>' is not marked as EOG\n",
            "load: control token: 200027 '<|tool_response|>' is not marked as EOG\n",
            "load: control token: 200028 '<|tag|>' is not marked as EOG\n",
            "load: control token: 200026 '<|/tool_call|>' is not marked as EOG\n",
            "load: control token: 200018 '<|endofprompt|>' is not marked as EOG\n",
            "load: control token: 200019 '<|assistant|>' is not marked as EOG\n",
            "load: special tokens cache size = 12\n",
            "load: token to piece cache size = 1.3333 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 24\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 262144\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 3\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.84 B\n",
            "print_info: general.name     = Phi 4 Mini Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 200064\n",
            "print_info: n_merges         = 199742\n",
            "print_info: BOS token        = 199999 '<|endoftext|>'\n",
            "print_info: EOS token        = 199999 '<|endoftext|>'\n",
            "print_info: EOT token        = 199999 '<|endoftext|>'\n",
            "print_info: UNK token        = 199999 '<|endoftext|>'\n",
            "print_info: PAD token        = 199999 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 199999 '<|endoftext|>'\n",
            "print_info: EOG token        = 200020 '<|end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q6_K) (and 258 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:          CPU model buffer size =  2221.57 MiB\n",
            "load_all_data: no device found for buffer type CPU for async uploads\n",
            ".........................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_init_from_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.76 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   396.75 MiB\n",
            "llama_init_from_model: graph nodes  = 1286\n",
            "llama_init_from_model: graph splits = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% else %}{{ eos_token }}{% endif %}\", 'phi3.rope.scaling.original_context_length': '4096', 'phi3.rope.scaling.attn_factor': '1.190238', 'general.architecture': 'phi3', 'general.license': 'mit', 'phi3.context_length': '131072', 'general.type': 'model', 'general.license.link': 'https://huggingface.co/microsoft/Phi-4-mini-instruct/resolve/main/LICENSE', 'tokenizer.ggml.pre': 'gpt-4o', 'general.basename': 'Phi-4', 'tokenizer.ggml.padding_token_id': '199999', 'phi3.attention.head_count': '24', 'phi3.attention.head_count_kv': '8', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.embedding_length': '3072', 'phi3.rope.dimension_count': '96', 'general.finetune': 'instruct', 'general.file_type': '14', 'phi3.rope.freq_base': '10000.000000', 'phi3.attention.sliding_window': '262144', 'phi3.block_count': '32', 'tokenizer.ggml.model': 'gpt2', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi 4 Mini Instruct', 'tokenizer.ggml.bos_token_id': '199999', 'tokenizer.ggml.unknown_token_id': '199999', 'tokenizer.ggml.eos_token_id': '199999', 'general.size_label': 'mini', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.add_eos_token': 'false'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% else %}{{ eos_token }}{% endif %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_phi_model(model_path):\n",
        "    llm = Llama(\n",
        "    model_path=\"/content/models/phi/Phi-4-mini-instruct.Q4_K_S.gguf\",\n",
        "    n_ctx=4096,\n",
        "    n_threads=8,\n",
        "    n_gpu_layers=0,  # Reduce to 0 if CPU-only\n",
        "    use_mmap=True,\n",
        "    verbose=False\n",
        ")\n",
        "    return llm\n"
      ],
      "metadata": {
        "id": "MZ5c6kIP7HJE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "from llama_cpp import Llama  # Make sure to install llama-cpp\n",
        "\n",
        "# Extract clean JSON from output\n",
        "def extract_json(text):\n",
        "    text =re.sub(r'^json\\s*', '', text.strip())\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"Failed to extract JSON: {str(e)}\"}\n",
        "\n",
        "# Build prompt suited for TinyLLaMA-style chat model\n",
        "def build_prompt(text):\n",
        "    instruction = \"\"\"\n",
        "You are an information extraction engine. Return ONLY valid JSON, no explanations.\n",
        "\n",
        "JSON Structure:\n",
        "{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}\n",
        "\n",
        "Extract metadata from the following scientific paper:\n",
        "\"\"\"\n",
        "    return f\"<|user|>\\n{instruction.strip()}\\n{text[:2000]}\\n<|assistant|>\"\n",
        "\n",
        "\n",
        "# Call model and extract structured data\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_tokens=1024,             # allow enough room for full JSON\n",
        "        temperature=0,               # deterministic output\n",
        "        top_p=1.0,\n",
        "        stop=[\"<|end|>\", \"</s>\"],    # prevent model from rambling\n",
        "    )\n",
        "\n",
        "\n",
        "    # print(response)\n",
        "    raw_output = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract raw text from PDF using PyMuPDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "# Load LLaMA 3 GGUF Model\n",
        "\n",
        "# Path to your GGUF model\n",
        "model_path = \"/content/models/phi/Phi-4-mini-instruct.Q4_K_S.gguf\"\n",
        "# Load model\n",
        "model = load_phi_model(model_path)\n",
        "\n",
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/1.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa7cIJfO7Pgd",
        "outputId": "79ca6cbc-dd5d-46ea-81e4-807bdeceee12"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "json\n",
            "{\n",
            "  \"Title\": \"Extracting Scientific Figures with Distantly Supervised Neural Networks\",\n",
            "  \"Authors\": [\"Noah Siegel\", \"Nicholas Lourie\", \"Russell Power\", \"Waleed Ammar\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Figure Extraction\", \"Distant Supervision\", \"Deep Learning\", \"Neural Networks\", \"Computer Vision\"],\n",
            "  \"Abstract\": \"Non-textual components such as charts, diagrams and tables provide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels—4,000 times larger than the previous largest figure extraction dataset—with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar, a large-scale academic search engine, and used to extract figures in 13 million scientific documents.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not provided\"\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Scientific Figures with Distantly Supervised Neural Networks\",\n",
            "    \"Authors\": [\n",
            "        \"Noah Siegel\",\n",
            "        \"Nicholas Lourie\",\n",
            "        \"Russell Power\",\n",
            "        \"Waleed Ammar\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Figure Extraction\",\n",
            "        \"Distant Supervision\",\n",
            "        \"Deep Learning\",\n",
            "        \"Neural Networks\",\n",
            "        \"Computer Vision\"\n",
            "    ],\n",
            "    \"Abstract\": \"Non-textual components such as charts, diagrams and tables provide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels\\u20144,000 times larger than the previous largest figure extraction dataset\\u2014with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar, a large-scale academic search engine, and used to extract figures in 13 million scientific documents.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not provided\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/2.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9txvbPNvJB1D",
        "outputId": "611179e3-ab16-471c-dff1-9f03dba37419"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "{\n",
            "  \"Title\": \"Automatic Recognition of Learning Resource Category in a Digital Library\",\n",
            "  \"Authors\": [\"Soumya Banerjee\", \"Debarshi Kumar Sanyal\", \"Samiran Chattopadhyay\", \"Plaban Kumar Bhowmick\", \"Partha Pratim Das\"],\n",
            "  \"DOI\": \"Not available\",\n",
            "  \"Keywords\": [\"deep learning\", \"transfer learning\", \"digital library\"],\n",
            "  \"Abstract\": \"Digital libraries generally need to process a large volume of diverse document types. The collection and tagging of metadata is a long, error-prone, manpower-consuming task. We are attempting to build an automatic metadata extractor for digital libraries. In this work, we present the Heterogeneous Learning Resources (HLR) dataset for document image classification. The individual learning resource is first decomposed into its constituent document images (sheets) which are then passed through an OCR tool to obtain the textual representation. The document image and its textual content are classified with state-of-the-art classifiers. Finally, the labels of the constituent document images are used to predict the label of the overall document.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not available\"\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Automatic Recognition of Learning Resource Category in a Digital Library\",\n",
            "    \"Authors\": [\n",
            "        \"Soumya Banerjee\",\n",
            "        \"Debarshi Kumar Sanyal\",\n",
            "        \"Samiran Chattopadhyay\",\n",
            "        \"Plaban Kumar Bhowmick\",\n",
            "        \"Partha Pratim Das\"\n",
            "    ],\n",
            "    \"DOI\": \"Not available\",\n",
            "    \"Keywords\": [\n",
            "        \"deep learning\",\n",
            "        \"transfer learning\",\n",
            "        \"digital library\"\n",
            "    ],\n",
            "    \"Abstract\": \"Digital libraries generally need to process a large volume of diverse document types. The collection and tagging of metadata is a long, error-prone, manpower-consuming task. We are attempting to build an automatic metadata extractor for digital libraries. In this work, we present the Heterogeneous Learning Resources (HLR) dataset for document image classification. The individual learning resource is first decomposed into its constituent document images (sheets) which are then passed through an OCR tool to obtain the textual representation. The document image and its textual content are classified with state-of-the-art classifiers. Finally, the labels of the constituent document images are used to predict the label of the overall document.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not available\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/3.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-iBrr2wJJX0",
        "outputId": "31dff474-325d-422d-a2cb-1b95f20351bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "json\n",
            "{\n",
            "  \"Title\": \"Model Selection with Model Zoo via Graph\",\n",
            "  \"Authors\": [\"Ziyu Li\", \"Hilco van der Wilk\", \"Danning Zhan\", \"Megha Khosla\", \"Alessandro Bozzon\", \"Rihan Hai\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Pre-trained deep learning models\", \"model zoos\", \"model selection\", \"TransferGraph\", \"graph learning\", \"model-dataset relationships\"],\n",
            "  \"Abstract\": \"Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph’s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not provided\"\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Model Selection with Model Zoo via Graph\",\n",
            "    \"Authors\": [\n",
            "        \"Ziyu Li\",\n",
            "        \"Hilco van der Wilk\",\n",
            "        \"Danning Zhan\",\n",
            "        \"Megha Khosla\",\n",
            "        \"Alessandro Bozzon\",\n",
            "        \"Rihan Hai\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Pre-trained deep learning models\",\n",
            "        \"model zoos\",\n",
            "        \"model selection\",\n",
            "        \"TransferGraph\",\n",
            "        \"graph learning\",\n",
            "        \"model-dataset relationships\"\n",
            "    ],\n",
            "    \"Abstract\": \"Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph\\u2019s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not provided\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/4.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjD15cboJLQo",
        "outputId": "1f8e63b6-0f4c-421c-f333-31c990a36934"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "{\n",
            "  \"Title\": \"Extracting Decision Model and Notation models from text using deep learning techniques\",\n",
            "  \"Authors\": [\"Alexandre Goossens\", \"Johannes De Smedt\", \"Jan Vanthienen\"],\n",
            "  \"DOI\": \"Not available\",\n",
            "  \"Keywords\": [\"Deep learning\", \"Decision Model and Notation\", \"DMN\", \"Decision model extraction\"],\n",
            "  \"Abstract\": \"Companies and organizations often use manuals and guidelines to communicate and execute operational decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions. Modeling a decision from a textual source, however, is a time intensive and complex activity hence a need for shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic extraction of decision models from text.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not available\"\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Decision Model and Notation models from text using deep learning techniques\",\n",
            "    \"Authors\": [\n",
            "        \"Alexandre Goossens\",\n",
            "        \"Johannes De Smedt\",\n",
            "        \"Jan Vanthienen\"\n",
            "    ],\n",
            "    \"DOI\": \"Not available\",\n",
            "    \"Keywords\": [\n",
            "        \"Deep learning\",\n",
            "        \"Decision Model and Notation\",\n",
            "        \"DMN\",\n",
            "        \"Decision model extraction\"\n",
            "    ],\n",
            "    \"Abstract\": \"Companies and organizations often use manuals and guidelines to communicate and execute operational decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions. Modeling a decision from a textual source, however, is a time intensive and complex activity hence a need for shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic extraction of decision models from text.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not available\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/5.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVDPT974JNGT",
        "outputId": "4918824e-882c-4abe-8dd9-b856c3232955"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "{\n",
            "  \"Title\": \"Automated data extraction is increasingly used to develop databases in materials science and other ﬁelds1. Many databases have been created using automated methods, but there is a growing interest in developing databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys.\",\n",
            "  \"Authors\": [\"Maciej P. Polak\", \"Dane Morgan\"],\n",
            "  \"DOI\": \"https://doi.org/10.1038/s41467-024-45914-8\",\n",
            "  \"Keywords\": [\"Automated data extraction\", \"Materials science\", \"Critical cooling rates\", \"Yield strengths\", \"High entropy alloys\"],\n",
            "  \"Abstract\": \"In this work, we propose the ChatExtract method that can fully automate very accurate data extraction with minimal initial effort and background, using an advanced conversational LLM. ChatExtract consists of a set of engineered prompts applied to a conversational LLM that both identify sentences with data, extract that data, and assure the data’s correctness through a series of follow-up questions. These follow-up questions largely overcome known issues with LLMs providing factually inaccurate responses. ChatExtract can be applied with any conversational LLMs and yields very high quality data extraction. In tests on materials data, we ﬁnd precision and recall both close to 90% from the best conversational LLMs, like GPT-4. We demonstrate that the exceptional performance is enabled by the information retention in a conversational model combined with purposeful redundancy and introducing uncertainty through follow-up prompts. These results suggest that approaches similar to ChatExtract, due to their simplicity, transferability, and accuracy are likely to become powerful tools for data extraction in the near future. Finally, databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys are developed using ChatExtract.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Automated data extraction is increasingly used to develop databases in materials science and other \\ufb01elds1. Many databases have been created using automated methods, but there is a growing interest in developing databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys.\",\n",
            "    \"Authors\": [\n",
            "        \"Maciej P. Polak\",\n",
            "        \"Dane Morgan\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1038/s41467-024-45914-8\",\n",
            "    \"Keywords\": [\n",
            "        \"Automated data extraction\",\n",
            "        \"Materials science\",\n",
            "        \"Critical cooling rates\",\n",
            "        \"Yield strengths\",\n",
            "        \"High entropy alloys\"\n",
            "    ],\n",
            "    \"Abstract\": \"In this work, we propose the ChatExtract method that can fully automate very accurate data extraction with minimal initial effort and background, using an advanced conversational LLM. ChatExtract consists of a set of engineered prompts applied to a conversational LLM that both identify sentences with data, extract that data, and assure the data\\u2019s correctness through a series of follow-up questions. These follow-up questions largely overcome known issues with LLMs providing factually inaccurate responses. ChatExtract can be applied with any conversational LLMs and yields very high quality data extraction. In tests on materials data, we \\ufb01nd precision and recall both close to 90% from the best conversational LLMs, like GPT-4. We demonstrate that the exceptional performance is enabled by the information retention in a conversational model combined with purposeful redundancy and introducing uncertainty through follow-up prompts. These results suggest that approaches similar to ChatExtract, due to their simplicity, transferability, and accuracy are likely to become powerful tools for data extraction in the near future. Finally, databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys are developed using ChatExtract.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "from llama_cpp import Llama  # Make sure to install llama-cpp\n",
        "\n",
        "# Extract clean JSON from output\n",
        "def extract_json(text):\n",
        "    text =re.sub(r'^json\\s*', '', text.strip())\n",
        "    text = re.sub(r'```json', '', text)\n",
        "    text = re.sub(r'```', '', text)\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"Failed to extract JSON: {str(e)}\"}\n",
        "\n",
        "# Build prompt suited for TinyLLaMA-style chat model\n",
        "def build_prompt(text):\n",
        "    instruction = \"\"\"\n",
        "You are an information extraction engine. Return ONLY valid JSON, no explanations.\n",
        "\n",
        "JSON Structure:\n",
        "{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}\n",
        "\n",
        "Extract metadata from the following scientific paper:\n",
        "\"\"\"\n",
        "    return f\"<|user|>\\n{instruction.strip()}\\n{text[:2000]}\\n<|assistant|>\"\n",
        "\n",
        "\n",
        "# Call model and extract structured data\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_tokens=1024,             # allow enough room for full JSON\n",
        "        temperature=0,               # deterministic output\n",
        "        top_p=1.0,\n",
        "        stop=[\"<|end|>\", \"</s>\"],    # prevent model from rambling\n",
        "    )\n",
        "\n",
        "\n",
        "    # print(response)\n",
        "    raw_output = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract raw text from PDF using PyMuPDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "# Load LLaMA 3 GGUF Model\n",
        "\n",
        "# Path to your GGUF model\n",
        "model_path = \"/content/models/phi/Phi-4-mini-instruct.Q4_K_S.gguf\"\n",
        "# Load model\n",
        "model = load_phi_model(model_path)\n",
        "\n",
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/6.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRcrjbvsJOor",
        "outputId": "31d530d1-343f-441f-8f51-64f3acf083a6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Smart Learning Environments: Prerequisites-based course recommendation: recommending learning objects using concept prerequisites and metadata matching\",\n",
            "  \"Authors\": [\"Abdessamad Chanaa\", \"Nour-eddine El Faddouli\"],\n",
            "  \"DOI\": \"https://doi.org/10.1186/s40561-024-00301-0\",\n",
            "  \"Keywords\": [\"Smart Learning Environments\", \"Prerequisites-based course recommendation\", \"Learning objects\", \"Concept prerequisites\", \"Metadata matching\"],\n",
            "  \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. However, classical recommender systems usually suffer from item cold-start issues. Besides, unlike other fields like e-commerce or entertainment, e-learning recommendations must ensure that learners have the adequate background knowledge to cognitively receive the recommended learning objects. For that reason, when designing an efficient e-learning recommendation method, these challenges should be considered. To address those issues, in this paper, we first propose extracting pairs concept prerequisites using Linked Open Data (LOD). Then, we evaluate the proposed list of prerequisite relationships using...\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Smart Learning Environments: Prerequisites-based course recommendation: recommending learning objects using concept prerequisites and metadata matching\",\n",
            "    \"Authors\": [\n",
            "        \"Abdessamad Chanaa\",\n",
            "        \"Nour-eddine El Faddouli\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1186/s40561-024-00301-0\",\n",
            "    \"Keywords\": [\n",
            "        \"Smart Learning Environments\",\n",
            "        \"Prerequisites-based course recommendation\",\n",
            "        \"Learning objects\",\n",
            "        \"Concept prerequisites\",\n",
            "        \"Metadata matching\"\n",
            "    ],\n",
            "    \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. However, classical recommender systems usually suffer from item cold-start issues. Besides, unlike other fields like e-commerce or entertainment, e-learning recommendations must ensure that learners have the adequate background knowledge to cognitively receive the recommended learning objects. For that reason, when designing an efficient e-learning recommendation method, these challenges should be considered. To address those issues, in this paper, we first propose extracting pairs concept prerequisites using Linked Open Data (LOD). Then, we evaluate the proposed list of prerequisite relationships using...\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/7.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dJpqXbCJQOF",
        "outputId": "043821d1-5155-4777-d97e-b821352665a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Deep Transfer Learning for Metadata\",\n",
            "  \"Authors\": [\"Zeyd Boukhers\", \"Nada Beili\", \"Timo Hartmann\", \"Prantik Goswami\", \"Muhammad Arslan Zafar\"],\n",
            "  \"DOI\": \"Not available\",\n",
            "  \"Keywords\": [\"transfer learning\", \"metadata extraction\", \"neural networks\"],\n",
            "  \"Abstract\": \"In contrast to most of the English scientific publications that follow standard and simple layouts, the order, content, position and size of metadata in German publications vary greatly among publications. This variety makes traditional NLP methods fail to accurately extract metadata from these publications. In this paper, we present a method that extracts metadata from PDF documents with different layouts and styles by viewing the document as an image. We used Mask R-CNN which is trained on COCO dataset and ﬁnetuned with PubLayNet dataset that consists of 200K PDF snapshots with ﬁve basic classes (e.g. text, ﬁgure, etc). We reﬁne-tuned the model on our proposed synthetic dataset consisting of 30K article snapshots to extract nine patterns (i.e. author, title, etc). Our synthetic dataset is generated using contents in both languages German and English and a ﬁnite set of challenging templates obtained from German publications. Our method achieved an average accuracy of around 90% which validates its capability to accurately extract metadata from a variety of PDF documents with challenging templates.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not available\"\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Deep Transfer Learning for Metadata\",\n",
            "    \"Authors\": [\n",
            "        \"Zeyd Boukhers\",\n",
            "        \"Nada Beili\",\n",
            "        \"Timo Hartmann\",\n",
            "        \"Prantik Goswami\",\n",
            "        \"Muhammad Arslan Zafar\"\n",
            "    ],\n",
            "    \"DOI\": \"Not available\",\n",
            "    \"Keywords\": [\n",
            "        \"transfer learning\",\n",
            "        \"metadata extraction\",\n",
            "        \"neural networks\"\n",
            "    ],\n",
            "    \"Abstract\": \"In contrast to most of the English scientific publications that follow standard and simple layouts, the order, content, position and size of metadata in German publications vary greatly among publications. This variety makes traditional NLP methods fail to accurately extract metadata from these publications. In this paper, we present a method that extracts metadata from PDF documents with different layouts and styles by viewing the document as an image. We used Mask R-CNN which is trained on COCO dataset and \\ufb01netuned with PubLayNet dataset that consists of 200K PDF snapshots with \\ufb01ve basic classes (e.g. text, \\ufb01gure, etc). We re\\ufb01ne-tuned the model on our proposed synthetic dataset consisting of 30K article snapshots to extract nine patterns (i.e. author, title, etc). Our synthetic dataset is generated using contents in both languages German and English and a \\ufb01nite set of challenging templates obtained from German publications. Our method achieved an average accuracy of around 90% which validates its capability to accurately extract metadata from a variety of PDF documents with challenging templates.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not available\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/8.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iBsHsqw8JT5J",
        "outputId": "d68da46c-0cf5-4d9e-eaa2-36695c07888a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "json\n",
            "{\n",
            "  \"Title\": \"ERNIE 3.0: LARGE-SCALE KNOWLEDGE ENHANCED PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND GENERATION\",\n",
            "  \"Authors\": [\"Yu Sun\", \"Shuohuan Wang\", \"Shikun Feng\", \"Siyu Ding\", \"Chao Pang\", \"Junyuan Shang\", \"Jiaxiang Liu\", \"Xuyi Chen\", \"Yanbin Zhao\", \"Yuxiang Lu\", \"Weixin Liu\", \"Zhihua Wu\", \"Weibao Gong\", \"Jianzhong Liang\", \"Zhizhou Shang\", \"Peng Sun\", \"Wei Liu\", \"Xuan Ouyang\", \"Dianhai Yu\", \"Hao Tian\", \"Hua Wu\", \"Haifeng Wang\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"ERNIE 3.0\", \"Large-scale knowledge enhanced pre-training\", \"Language understanding\", \"Language generation\", \"Zero-shot learning\", \"Few-shot learning\", \"Fine-tuning\", \"Natural Language Processing\", \"Chinese NLP tasks\", \"SuperGLUE benchmark\"],\n",
            "  \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE [3] benchmark (July 3, 2021), surpassing human performance by +0.8% (90.6% vs. 89.8%).\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 1\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"ERNIE 3.0: LARGE-SCALE KNOWLEDGE ENHANCED PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND GENERATION\",\n",
            "    \"Authors\": [\n",
            "        \"Yu Sun\",\n",
            "        \"Shuohuan Wang\",\n",
            "        \"Shikun Feng\",\n",
            "        \"Siyu Ding\",\n",
            "        \"Chao Pang\",\n",
            "        \"Junyuan Shang\",\n",
            "        \"Jiaxiang Liu\",\n",
            "        \"Xuyi Chen\",\n",
            "        \"Yanbin Zhao\",\n",
            "        \"Yuxiang Lu\",\n",
            "        \"Weixin Liu\",\n",
            "        \"Zhihua Wu\",\n",
            "        \"Weibao Gong\",\n",
            "        \"Jianzhong Liang\",\n",
            "        \"Zhizhou Shang\",\n",
            "        \"Peng Sun\",\n",
            "        \"Wei Liu\",\n",
            "        \"Xuan Ouyang\",\n",
            "        \"Dianhai Yu\",\n",
            "        \"Hao Tian\",\n",
            "        \"Hua Wu\",\n",
            "        \"Haifeng Wang\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"ERNIE 3.0\",\n",
            "        \"Large-scale knowledge enhanced pre-training\",\n",
            "        \"Language understanding\",\n",
            "        \"Language generation\",\n",
            "        \"Zero-shot learning\",\n",
            "        \"Few-shot learning\",\n",
            "        \"Fine-tuning\",\n",
            "        \"Natural Language Processing\",\n",
            "        \"Chinese NLP tasks\",\n",
            "        \"SuperGLUE benchmark\"\n",
            "    ],\n",
            "    \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE [3] benchmark (July 3, 2021), surpassing human performance by +0.8% (90.6% vs. 89.8%).\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 1\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/9.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtMbMNr_SAB2",
        "outputId": "4b7e0c4c-d7f9-48dd-adb0-5a9a5955f9e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Few-shot named entity recognition framework for forestry science\",\n",
            "  \"Authors\": [\"Yuquan Fan\", \"Hong Xiao\", \"Min Wang\", \"Junchi Wang\", \"Wenchao Jiang\", \"Chang Zhu\"],\n",
            "  \"DOI\": \"https://doi.org/10.1007/s12652-023-04740-4\",\n",
            "  \"Keywords\": [\"metadata extraction\", \"named entity recognition\", \"forestry science\", \"few-shot learning\", \"data augmentation\", \"semantic comprehension\"],\n",
            "  \"Abstract\": \"The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the efficient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is challenging, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic comprehension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture an intricate understanding of the semantic relationships within the text.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Few-shot named entity recognition framework for forestry science\",\n",
            "    \"Authors\": [\n",
            "        \"Yuquan Fan\",\n",
            "        \"Hong Xiao\",\n",
            "        \"Min Wang\",\n",
            "        \"Junchi Wang\",\n",
            "        \"Wenchao Jiang\",\n",
            "        \"Chang Zhu\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1007/s12652-023-04740-4\",\n",
            "    \"Keywords\": [\n",
            "        \"metadata extraction\",\n",
            "        \"named entity recognition\",\n",
            "        \"forestry science\",\n",
            "        \"few-shot learning\",\n",
            "        \"data augmentation\",\n",
            "        \"semantic comprehension\"\n",
            "    ],\n",
            "    \"Abstract\": \"The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the efficient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is challenging, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic comprehension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture an intricate understanding of the semantic relationships within the text.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzpaiA7rif_9",
        "outputId": "fb3c19fa-ac68-4e13-8fef-ba52964822c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "from llama_cpp import Llama  # Make sure to install llama-cpp\n",
        "\n",
        "# Extract clean JSON from output\n",
        "def extract_json(text):\n",
        "    text =re.sub(r'^json\\s*', '', text.strip())\n",
        "    text = re.sub(r'```json', '', text)\n",
        "    text = re.sub(r'```', '', text)\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"Failed to extract JSON: {str(e)}\"}\n",
        "\n",
        "# Build prompt suited for TinyLLaMA-style chat model\n",
        "def build_prompt(text):\n",
        "    instruction = \"\"\"\n",
        "You are an information extraction engine. Return ONLY valid JSON, no explanations.\n",
        "\n",
        "JSON Structure:\n",
        "{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}\n",
        "\n",
        "Extract metadata from the following scientific paper:\n",
        "\"\"\"\n",
        "    return f\"<|user|>\\n{instruction.strip()}\\n{text[:2000]}\\n<|assistant|>\"\n",
        "\n",
        "\n",
        "# Call model and extract structured data\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_tokens=1024,             # allow enough room for full JSON\n",
        "        temperature=0,               # deterministic output\n",
        "        top_p=1.0,\n",
        "        stop=[\"<|end|>\", \"</s>\"],    # prevent model from rambling\n",
        "    )\n",
        "\n",
        "\n",
        "    # print(response)\n",
        "    raw_output = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract raw text from PDF using PyMuPDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "# Load LLaMA 3 GGUF Model\n",
        "\n",
        "# Path to your GGUF model\n",
        "model_path = \"/content/models/phi/Phi-4-mini-instruct.Q4_K_S.gguf\"\n",
        "# Load model\n",
        "model = load_phi_model(model_path)\n",
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/10.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T27A29j5iTns",
        "outputId": "628d9ff9-06b9-458f-ca86-46ee145a3ba4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Extracting structured knowledge from scientific text with large language models\",\n",
            "  \"Authors\": [\"John Dagdelen\", \"Alexander Dunn\", \"Sanghoon Lee\", \"Nicholas Walker\", \"Andrew S. Rosen\", \"Gerbrand Ceder\", \"Kristin A. Persson\", \"Anubhav Jain\"],\n",
            "  \"DOI\": \"https://doi.org/10.1038/s41467-024-45563-x\",\n",
            "  \"Keywords\": [\"scientific text\", \"large language models\", \"joint named entity recognition\", \"relation extraction\", \"materials chemistry\", \"metal-organic frameworks\", \"composition\", \"phase\", \"morphology\", \"application\"],\n",
            "  \"Abstract\": \"The majority of scientific knowledge about solid-state materials is scattered across the text, tables, and figures of millions of academic research papers. Thus, it is difficult for researchers to properly understand the full body of past work and effectively leverage existing knowledge when designing experiments. Moreover, machine learning models for direct property prediction are being increasingly employed as screening steps for materials discovery and design workflows. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting structured knowledge from scientific text with large language models\",\n",
            "    \"Authors\": [\n",
            "        \"John Dagdelen\",\n",
            "        \"Alexander Dunn\",\n",
            "        \"Sanghoon Lee\",\n",
            "        \"Nicholas Walker\",\n",
            "        \"Andrew S. Rosen\",\n",
            "        \"Gerbrand Ceder\",\n",
            "        \"Kristin A. Persson\",\n",
            "        \"Anubhav Jain\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1038/s41467-024-45563-x\",\n",
            "    \"Keywords\": [\n",
            "        \"scientific text\",\n",
            "        \"large language models\",\n",
            "        \"joint named entity recognition\",\n",
            "        \"relation extraction\",\n",
            "        \"materials chemistry\",\n",
            "        \"metal-organic frameworks\",\n",
            "        \"composition\",\n",
            "        \"phase\",\n",
            "        \"morphology\",\n",
            "        \"application\"\n",
            "    ],\n",
            "    \"Abstract\": \"The majority of scientific knowledge about solid-state materials is scattered across the text, tables, and figures of millions of academic research papers. Thus, it is difficult for researchers to properly understand the full body of past work and effectively leverage existing knowledge when designing experiments. Moreover, machine learning models for direct property prediction are being increasingly employed as screening steps for materials discovery and design workflows. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/11.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2-KwfXQl5tK",
        "outputId": "46971996-aca6-4f9a-cdc4-d0df692301b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Extracting structured knowledge from scientific text with large language models\",\n",
            "  \"Authors\": [\"John Dagdelen\", \"Alexander Dunn\", \"Sanghoon Lee\", \"Nicholas Walker\", \"Andrew S. Rosen\", \"Gerbrand Ceder\", \"Kristin A. Persson\", \"Anubhav Jain\"],\n",
            "  \"DOI\": \"https://doi.org/10.1038/s41467-024-45563-x\",\n",
            "  \"Keywords\": [\"scientific text\", \"large language models\", \"joint named entity recognition\", \"relation extraction\", \"materials chemistry\", \"metal-organic frameworks\", \"composition\", \"phase\", \"morphology\", \"application\"],\n",
            "  \"Abstract\": \"Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting structured knowledge from scientific text with large language models\",\n",
            "    \"Authors\": [\n",
            "        \"John Dagdelen\",\n",
            "        \"Alexander Dunn\",\n",
            "        \"Sanghoon Lee\",\n",
            "        \"Nicholas Walker\",\n",
            "        \"Andrew S. Rosen\",\n",
            "        \"Gerbrand Ceder\",\n",
            "        \"Kristin A. Persson\",\n",
            "        \"Anubhav Jain\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1038/s41467-024-45563-x\",\n",
            "    \"Keywords\": [\n",
            "        \"scientific text\",\n",
            "        \"large language models\",\n",
            "        \"joint named entity recognition\",\n",
            "        \"relation extraction\",\n",
            "        \"materials chemistry\",\n",
            "        \"metal-organic frameworks\",\n",
            "        \"composition\",\n",
            "        \"phase\",\n",
            "        \"morphology\",\n",
            "        \"application\"\n",
            "    ],\n",
            "    \"Abstract\": \"Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "from llama_cpp import Llama  # Make sure to install llama-cpp\n",
        "\n",
        "# Extract clean JSON from output\n",
        "def extract_json(text):\n",
        "    # Clean up any markdown or prefix like 'json' or '```json'\n",
        "    text = re.sub(r'^json\\s*', '', text.strip(), flags=re.IGNORECASE)\n",
        "    text = re.sub(r'```json', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'```', '', text)\n",
        "    text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')  # remove any weird hidden chars\n",
        "\n",
        "    try:\n",
        "        # Parse string into JSON\n",
        "        metadata = json.loads(text)\n",
        "        if \"Authors\" in metadata:\n",
        "            cleaned_authors = []\n",
        "            for name in metadata['Authors']:\n",
        "                # Remove empty parentheses '()', and any other unwanted characters within parentheses\n",
        "                name = re.sub(r'\\(\\)', '', name)  # Removes empty parentheses '()'\n",
        "                name = re.sub(r'\\([^)]*\\)', '', name)  # Removes content inside parentheses, like (xx)\n",
        "                cleaned_authors.append(name.strip())  # Append cleaned name\n",
        "\n",
        "            metadata['Authors'] = cleaned_authors\n",
        "        return metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"Failed to extract JSON: {str(e)}\"}\n",
        "\n",
        "# Build prompt suited for TinyLLaMA-style chat model\n",
        "def build_prompt(text):\n",
        "    instruction = \"\"\"\n",
        "You are an information extraction engine. Return ONLY valid JSON, no explanations.\n",
        "\n",
        "JSON Structure:\n",
        "{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}\n",
        "\n",
        "Extract metadata from the following scientific paper:\n",
        "\"\"\"\n",
        "    return f\"<|user|>\\n{instruction.strip()}\\n{text[:2000]}\\n<|assistant|>\"\n",
        "\n",
        "\n",
        "# Call model and extract structured data\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_tokens=1024,             # allow enough room for full JSON\n",
        "        temperature=0,               # deterministic output\n",
        "        top_p=1.0,\n",
        "        stop=[\"<|end|>\", \"</s>\"],    # prevent model from rambling\n",
        "    )\n",
        "\n",
        "\n",
        "    # print(response)\n",
        "    raw_output = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract raw text from PDF using PyMuPDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "# Load LLaMA 3 GGUF Model\n",
        "\n",
        "# Path to your GGUF model\n",
        "model_path = \"/content/models/phi/Phi-4-mini-instruct.Q4_K_S.gguf\"\n",
        "# Load model\n",
        "model = load_phi_model(model_path)\n",
        "\n",
        "\n",
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/12.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jchsjpJ_l7eR",
        "outputId": "94f4fd3d-ca6e-4054-ff2b-d653fa15ae69"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "{\n",
            "  \"Title\": \"Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community\",\n",
            "  \"Authors\": [\n",
            "    \"Qingyao AIa\",\n",
            "    \"Ting BAIb\",\n",
            "    \"Zhao CAOc\",\n",
            "    \"Yi CHANGd\",\n",
            "    \"Jiawei CHEN(\u0000)e\",\n",
            "    \"Zhumin CHENf\",\n",
            "    \"Zhiyong CHENGg\",\n",
            "    \"Shoubin DONGh\",\n",
            "    \"Zhicheng DOUi\",\n",
            "    \"Fuli FENG j\",\n",
            "    \"Shen GAO f\",\n",
            "    \"Jiafeng GUOk\",\n",
            "    \"Xiangnan HE(\u0000) j\",\n",
            "    \"Yanyan LANa\",\n",
            "    \"Chenliang LIl\",\n",
            "    \"Yiqun LIUa\",\n",
            "    \"Ziyu LYUm\",\n",
            "    \"Weizhi MAa\",\n",
            "    \"Jun MAf\",\n",
            "    \"Zhaochun REN f\",\n",
            "    \"Pengjie REN f\",\n",
            "    \"Zhiqiang WANGn\",\n",
            "    \"Mingwen WANGo\",\n",
            "    \"Ji-Rong WENi\",\n",
            "    \"Le WUp\",\n",
            "    \"Xin XIN f\",\n",
            "    \"Jun XUi\",\n",
            "    \"Dawei YINq\",\n",
            "    \"Peng ZHANG(\u0000)r\",\n",
            "    \"Fan ZHANGl\",\n",
            "    \"Weinan ZHANGs\",\n",
            "    \"Min ZHANGa\",\n",
            "    \"Xiaofei ZHUt\"\n",
            "  ],\n",
            "  \"DOI\": \"10.1007/s11740-023-01417-7\",\n",
            "  \"Keywords\": [\n",
            "    \"Information Retrieval\",\n",
            "    \"Large Language Models\",\n",
            "    \"Synergistic Relationship\",\n",
            "    \"Generative Retrieval\",\n",
            "    \"User Understanding\",\n",
            "    \"Model Evaluation\",\n",
            "    \"User-System Interactions\"\n",
            "  ],\n",
            "  \"Abstract\": \"The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans contribute contextual understanding.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Error\": \"Failed to extract JSON: Invalid control character at: line 8 column 18 (char 214)\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/13.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1xfnuD3l9Ry",
        "outputId": "32e5823f-ba70-49ee-d86c-954b18654504"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "{\n",
            "  \"Title\": \"Building an annotated corpus for automatic metadata extraction from multilingual journal article references\",\n",
            "  \"Authors\": [\"Wonjun Choi\", \"Hwa-Mook Yoon\", \"Mi-Hwan Hyun\", \"Hye-Jin Lee\", \"Jae-Wook Seol\", \"Kangsan\", \"Dajeong Lee\", \"Young Joon Yoon\", \"Hyesoo Kong\"],\n",
            "  \"DOI\": \"Not available\",\n",
            "  \"Keywords\": [\"Bibliographic references\", \"Citation information\", \"Academic literature\", \"Metadata extraction\", \"Multilingual references\", \"BERT-based transfer-learning model\", \"GROBID\", \"CERMINE\"],\n",
            "  \"Abstract\": \"Bibliographic references containing citation information of academic literature play an important role as a medium connecting earlier and recent studies. As references contain machine-readable metadata such as author name, title, or publication year, they have been widely used in the field of citation information services including search services for scholarly information and research trend analysis. Many institutions around the world manually extract and continuously accumulate reference metadata to provide various scholarly services. However, manually collection of reference metadata every year continues to be a burden because of the associated cost and time consumption. With the accumulation of a large volume of academic literature, several tools, including GROBID and CERMINE, that automatically extract reference metadata have been released. However, these tools have some limitations. For example, they are only applicable to references written in English, the types of extractable metadata are limited for each tool, and the performance of the tools is insufficient to replace the manual extraction of reference metadata. Therefore, in this study, we focused on constructing a high-quality corpus to automatically extract metadata from multilingual journal article references. Using our constructed corpus, we trained and evaluated a BERT-based transfer-learning model. Furthermore, we compared the performance of the BERT-based model with that of the existing model, GROBID. Currently, our corpus contains 3,815,987 multilingual references, mainly in English and Korean, with labels f\",\n",
            "  \"Document Type\": \"Research Article\",\n",
            "  \"Number of References\": \"Not specified\"\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Building an annotated corpus for automatic metadata extraction from multilingual journal article references\",\n",
            "    \"Authors\": [\n",
            "        \"Wonjun Choi\",\n",
            "        \"Hwa-Mook Yoon\",\n",
            "        \"Mi-Hwan Hyun\",\n",
            "        \"Hye-Jin Lee\",\n",
            "        \"Jae-Wook Seol\",\n",
            "        \"Kangsan\",\n",
            "        \"Dajeong Lee\",\n",
            "        \"Young Joon Yoon\",\n",
            "        \"Hyesoo Kong\"\n",
            "    ],\n",
            "    \"DOI\": \"Not available\",\n",
            "    \"Keywords\": [\n",
            "        \"Bibliographic references\",\n",
            "        \"Citation information\",\n",
            "        \"Academic literature\",\n",
            "        \"Metadata extraction\",\n",
            "        \"Multilingual references\",\n",
            "        \"BERT-based transfer-learning model\",\n",
            "        \"GROBID\",\n",
            "        \"CERMINE\"\n",
            "    ],\n",
            "    \"Abstract\": \"Bibliographic references containing citation information of academic literature play an important role as a medium connecting earlier and recent studies. As references contain machine-readable metadata such as author name, title, or publication year, they have been widely used in the field of citation information services including search services for scholarly information and research trend analysis. Many institutions around the world manually extract and continuously accumulate reference metadata to provide various scholarly services. However, manually collection of reference metadata every year continues to be a burden because of the associated cost and time consumption. With the accumulation of a large volume of academic literature, several tools, including GROBID and CERMINE, that automatically extract reference metadata have been released. However, these tools have some limitations. For example, they are only applicable to references written in English, the types of extractable metadata are limited for each tool, and the performance of the tools is insufficient to replace the manual extraction of reference metadata. Therefore, in this study, we focused on constructing a high-quality corpus to automatically extract metadata from multilingual journal article references. Using our constructed corpus, we trained and evaluated a BERT-based transfer-learning model. Furthermore, we compared the performance of the BERT-based model with that of the existing model, GROBID. Currently, our corpus contains 3,815,987 multilingual references, mainly in English and Korean, with labels f\",\n",
            "    \"Document Type\": \"Research Article\",\n",
            "    \"Number of References\": \"Not specified\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/14.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joBL-tgUl-z-",
        "outputId": "14f365e1-164f-4f93-d231-df61a20cc760"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "{\n",
            "  \"Title\": \"LAME: Layout-Aware Metadata Extraction Approach for Research Articles\",\n",
            "  \"Authors\": [\"JONGYUN CHOI\", \"HYESOO KONG\", \"HWAMOOK YOON\", \"HEUNG-SEON OH\", \"YUCHUL JUNG\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Automatic layout analysis\", \"Layout-MetaBERT\", \"Metadata extraction\", \"Research article\"],\n",
            "  \"Abstract\": \"The volume of academic literature, such as academic conference papers and journals, has increased rapidly worldwide, and research on metadata extraction is ongoing. However, high-performing metadata extraction is still challenging due to diverse layout formats according to journal publishers. To accommodate the diversity of the layouts of academic journals, we propose a novel LAyout-aware Metadata Extraction (LAME) framework equipped with the three characteristics (e.g., design of an automatic layout analysis, construction of a large meta-data training set, and construction of Layout-MetaBERT). We designed an automatic layout analysis using PDFMiner. Based on the layout analysis, a large volume of metadata-separated training data, including the title, abstract, author name, author affiliated organization, and keywords, were automatically extracted. Moreover, we constructed Layout-MetaBERT to extract the metadata from academic journals with varying layout formats. The experimental results with Layout-MetaBERT exhibited robust performance (Macro-F1, 93.27%) in metadata extraction for unseen journals with different layout formats.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not provided\"\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"LAME: Layout-Aware Metadata Extraction Approach for Research Articles\",\n",
            "    \"Authors\": [\n",
            "        \"JONGYUN CHOI\",\n",
            "        \"HYESOO KONG\",\n",
            "        \"HWAMOOK YOON\",\n",
            "        \"HEUNG-SEON OH\",\n",
            "        \"YUCHUL JUNG\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Automatic layout analysis\",\n",
            "        \"Layout-MetaBERT\",\n",
            "        \"Metadata extraction\",\n",
            "        \"Research article\"\n",
            "    ],\n",
            "    \"Abstract\": \"The volume of academic literature, such as academic conference papers and journals, has increased rapidly worldwide, and research on metadata extraction is ongoing. However, high-performing metadata extraction is still challenging due to diverse layout formats according to journal publishers. To accommodate the diversity of the layouts of academic journals, we propose a novel LAyout-aware Metadata Extraction (LAME) framework equipped with the three characteristics (e.g., design of an automatic layout analysis, construction of a large meta-data training set, and construction of Layout-MetaBERT). We designed an automatic layout analysis using PDFMiner. Based on the layout analysis, a large volume of metadata-separated training data, including the title, abstract, author name, author affiliated organization, and keywords, were automatically extracted. Moreover, we constructed Layout-MetaBERT to extract the metadata from academic journals with varying layout formats. The experimental results with Layout-MetaBERT exhibited robust performance (Macro-F1, 93.27%) in metadata extraction for unseen journals with different layout formats.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not provided\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/15.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLVRHhE_mASs",
        "outputId": "6a5a1805-c8f0-4ef7-cac8-440c6a84fa59"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "json\n",
            "{\n",
            "  \"Title\": \"Layout Aware Semantic Element Extraction for Sustainable Science & Technology Decision Support\",\n",
            "  \"Authors\": [\"Hyuntae Kim\", \"Jongyun Choi\", \"Soyoung Park\", \"Yuchul Jung\"],\n",
            "  \"DOI\": \"https://doi.org/10.3390/su14052802\",\n",
            "  \"Keywords\": [\"Layout Aware\", \"Semantic Element Extraction\", \"Sustainable Science\", \"Technology Decision Support\", \"Knowledge Graph\", \"Metadata Extraction\", \"Vision Recognition\", \"Automated Text Mining\"],\n",
            "  \"Abstract\": \"New scientific and technological (S&T) knowledge is being introduced rapidly, and hence, analysis efforts to understand and analyze new published S&T documents are increasing daily. Automated text mining and vision recognition techniques alleviate the burden somewhat, but the various document layout formats and knowledge content granularities across the S&T ﬁeld make it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph construction framework that simultaneously extracts meta-information and useful image objects from S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME), which can accurately extract metadata from various layout formats, and implement a transformer-based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize the vision.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Layout Aware Semantic Element Extraction for Sustainable Science & Technology Decision Support\",\n",
            "    \"Authors\": [\n",
            "        \"Hyuntae Kim\",\n",
            "        \"Jongyun Choi\",\n",
            "        \"Soyoung Park\",\n",
            "        \"Yuchul Jung\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.3390/su14052802\",\n",
            "    \"Keywords\": [\n",
            "        \"Layout Aware\",\n",
            "        \"Semantic Element Extraction\",\n",
            "        \"Sustainable Science\",\n",
            "        \"Technology Decision Support\",\n",
            "        \"Knowledge Graph\",\n",
            "        \"Metadata Extraction\",\n",
            "        \"Vision Recognition\",\n",
            "        \"Automated Text Mining\"\n",
            "    ],\n",
            "    \"Abstract\": \"New scientific and technological (S&T) knowledge is being introduced rapidly, and hence, analysis efforts to understand and analyze new published S&T documents are increasing daily. Automated text mining and vision recognition techniques alleviate the burden somewhat, but the various document layout formats and knowledge content granularities across the S&T \\ufb01eld make it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph construction framework that simultaneously extracts meta-information and useful image objects from S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME), which can accurately extract metadata from various layout formats, and implement a transformer-based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize the vision.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/16.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "id": "Q0iv_wS3LW5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d020fd-1a59-48d3-cc7f-9ed92dbb0121"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Convolutional Neural Networks for Sentence Classification\",\n",
            "  \"Authors\": [\"Yoon Kim\"],\n",
            "  \"DOI\": \"Not available\",\n",
            "  \"Keywords\": [\"Convolutional Neural Networks\", \"Sentence Classification\", \"Word Vectors\", \"Deep Learning\", \"Natural Language Processing\"],\n",
            "  \"Abstract\": \"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not available\"\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Convolutional Neural Networks for Sentence Classification\",\n",
            "    \"Authors\": [\n",
            "        \"Yoon Kim\"\n",
            "    ],\n",
            "    \"DOI\": \"Not available\",\n",
            "    \"Keywords\": [\n",
            "        \"Convolutional Neural Networks\",\n",
            "        \"Sentence Classification\",\n",
            "        \"Word Vectors\",\n",
            "        \"Deep Learning\",\n",
            "        \"Natural Language Processing\"\n",
            "    ],\n",
            "    \"Abstract\": \"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not available\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/17.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x7494wNmTk5",
        "outputId": "e4e64ec1-35bb-449c-f8ac-24fbc51e7c0d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "{\n",
            "  \"Title\": \"Information extraction from research papers using conditional random fields\",\n",
            "  \"Authors\": [\"Fuchun Peng\", \"Andrew McCallum\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Information extraction\", \"Constraint information extraction\", \"Conditional random fields\", \"Regularization\"],\n",
            "  \"Abstract\": \"With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This article employs conditional random fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features and global layout features. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint co-reference information extraction; i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system significantly improves extraction performance, with an error rate reduction of 6–14%.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not provided\"\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Information extraction from research papers using conditional random fields\",\n",
            "    \"Authors\": [\n",
            "        \"Fuchun Peng\",\n",
            "        \"Andrew McCallum\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Information extraction\",\n",
            "        \"Constraint information extraction\",\n",
            "        \"Conditional random fields\",\n",
            "        \"Regularization\"\n",
            "    ],\n",
            "    \"Abstract\": \"With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This article employs conditional random fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features and global layout features. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint co-reference information extraction; i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system significantly improves extraction performance, with an error rate reduction of 6\\u201314%.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not provided\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/19.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1q1JCQxmXOB",
        "outputId": "a7453f27-02f0-470a-eb45-e216d433b7a6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Predicting movies’ eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features\",\n",
            "  \"Authors\": [\"Elham Motamedi\", \"Danial Khosh Kholgh\", \"Sorush Saghari\", \"Mehdi Elahi\", \"Francesco Barile\", \"Marko Tkalcic\"],\n",
            "  \"DOI\": \"Unavailable\",\n",
            "  \"Keywords\": [\"Eudaimonia\", \"Hedonia\", \"Machine learning approach\", \"Movie recommender systems\"],\n",
            "  \"Abstract\": \"In the task of modeling user preferences for movie recommender systems, recent research has demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and H scores), which reflect the depth of their message and the level of fun experience they provide, respectively. So far, the labeling of movies with their E and H scores has been done manually using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue, we propose an automatic approach for predicting E and H scores. Specifically, we collected E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this dataset with metadata, audio, and low-level and high-level visual features, and trained machine learning models for predicting the E and H scores of movies. This study investigates the use of machine learning models in predicting the E and H scores of movies using various feature sets, including audio, low-level and high-level visual features, and metadata. We compared the performance of pre-trained models and custom-built models for predicting E and H scores.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Predicting movies\\u2019 eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features\",\n",
            "    \"Authors\": [\n",
            "        \"Elham Motamedi\",\n",
            "        \"Danial Khosh Kholgh\",\n",
            "        \"Sorush Saghari\",\n",
            "        \"Mehdi Elahi\",\n",
            "        \"Francesco Barile\",\n",
            "        \"Marko Tkalcic\"\n",
            "    ],\n",
            "    \"DOI\": \"Unavailable\",\n",
            "    \"Keywords\": [\n",
            "        \"Eudaimonia\",\n",
            "        \"Hedonia\",\n",
            "        \"Machine learning approach\",\n",
            "        \"Movie recommender systems\"\n",
            "    ],\n",
            "    \"Abstract\": \"In the task of modeling user preferences for movie recommender systems, recent research has demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and H scores), which reflect the depth of their message and the level of fun experience they provide, respectively. So far, the labeling of movies with their E and H scores has been done manually using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue, we propose an automatic approach for predicting E and H scores. Specifically, we collected E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this dataset with metadata, audio, and low-level and high-level visual features, and trained machine learning models for predicting the E and H scores of movies. This study investigates the use of machine learning models in predicting the E and H scores of movies using various feature sets, including audio, low-level and high-level visual features, and metadata. We compared the performance of pre-trained models and custom-built models for predicting E and H scores.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/18.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPexCl9gmT74",
        "outputId": "fff1aeaf-3a3c-4a47-fca4-205383902c3b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "json\n",
            "{\n",
            "  \"Title\": \"Information extraction from scientific articles: a survey\",\n",
            "  \"Authors\": [\"Zara Nasar\", \"Syed Waqar Jaffry\", \"Muhammad Kamran Malik\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Information extraction\", \"Scientific articles\", \"World Wide Web\", \"Automatic analysis\", \"Literature review\", \"Computational approaches\", \"Rule-based approaches\", \"Hidden Markov Models\", \"Conditional Random Fields\", \"Support Vector Machines\", \"Naı¨ve-Bayes classification\", \"Deep Learning\"],\n",
            "  \"Abstract\": \"In last few decades, with the advent of World Wide Web (WWW), world is being overloaded with huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and efficient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scientiﬁc literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scientiﬁc articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scientiﬁc articles. The information insights extracted from scientiﬁc articles are classiﬁed in two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a signiﬁcant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Naı¨ve-Bayes classiﬁcation and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to speciﬁc information needs from scientiﬁc articles. Hence, in this study, state-o of the art in automatic information extraction from scientiﬁc articles is presented.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": null\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Information extraction from scientific articles: a survey\",\n",
            "    \"Authors\": [\n",
            "        \"Zara Nasar\",\n",
            "        \"Syed Waqar Jaffry\",\n",
            "        \"Muhammad Kamran Malik\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Information extraction\",\n",
            "        \"Scientific articles\",\n",
            "        \"World Wide Web\",\n",
            "        \"Automatic analysis\",\n",
            "        \"Literature review\",\n",
            "        \"Computational approaches\",\n",
            "        \"Rule-based approaches\",\n",
            "        \"Hidden Markov Models\",\n",
            "        \"Conditional Random Fields\",\n",
            "        \"Support Vector Machines\",\n",
            "        \"Na\\u0131\\u00a8ve-Bayes classification\",\n",
            "        \"Deep Learning\"\n",
            "    ],\n",
            "    \"Abstract\": \"In last few decades, with the advent of World Wide Web (WWW), world is being overloaded with huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and efficient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scienti\\ufb01c literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scienti\\ufb01c articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scienti\\ufb01c articles. The information insights extracted from scienti\\ufb01c articles are classi\\ufb01ed in two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a signi\\ufb01cant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Na\\u0131\\u00a8ve-Bayes classi\\ufb01cation and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to speci\\ufb01c information needs from scienti\\ufb01c articles. Hence, in this study, state-o of the art in automatic information extraction from scienti\\ufb01c articles is presented.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/20.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iLN0PRDmZk8",
        "outputId": "660a81cf-b60f-468e-f2fa-38e018a5f199"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "{\n",
            "  \"Title\": \"Deep Learning-based Extraction of Algorithmic Metadata in Full-Text Scholarly Documents\",\n",
            "  \"Authors\": [\"Iqra Safdera\", \"Saeed-Ul Hassana\", \"Anna Visvizib\", \"Thanapon Norasetc\", \"Raheel Nawazd\", \"Suppawong Tuarobc\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Knowledge-based Systems\", \"Algorithmic Metadata\", \"Algorithm Search\", \"Deep Learning\", \"Bi-Directional LSTM\", \"Information Retrieval\", \"Full-text Articles\"],\n",
            "  \"Abstract\": \"The advancements of search engines for traditional text documents have enabled the effective retrieval of massive textual information in a resource-efficient manner. However, such conventional search methodologies often suffer from poor retrieval accuracy especially when documents exhibit unique properties that behoove specialized and deeper semantic extraction. Recently, AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and shallow textual metadata from scientific publications and treats them as traditional documents so that the conventional search engine methodology could be applied. However, such a system fails to facilitate user search queries that seek to identify algorithm-specific information, such as the datasets on which algorithms operate, the performance of algorithms, and runtime complexity, etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are presented. Specifically, we propose a set of methods to automatically identify and extract algorithmic pseudo-codes and the sentence-level metadata that describe the algorithms.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not provided\"\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Deep Learning-based Extraction of Algorithmic Metadata in Full-Text Scholarly Documents\",\n",
            "    \"Authors\": [\n",
            "        \"Iqra Safdera\",\n",
            "        \"Saeed-Ul Hassana\",\n",
            "        \"Anna Visvizib\",\n",
            "        \"Thanapon Norasetc\",\n",
            "        \"Raheel Nawazd\",\n",
            "        \"Suppawong Tuarobc\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Knowledge-based Systems\",\n",
            "        \"Algorithmic Metadata\",\n",
            "        \"Algorithm Search\",\n",
            "        \"Deep Learning\",\n",
            "        \"Bi-Directional LSTM\",\n",
            "        \"Information Retrieval\",\n",
            "        \"Full-text Articles\"\n",
            "    ],\n",
            "    \"Abstract\": \"The advancements of search engines for traditional text documents have enabled the effective retrieval of massive textual information in a resource-efficient manner. However, such conventional search methodologies often suffer from poor retrieval accuracy especially when documents exhibit unique properties that behoove specialized and deeper semantic extraction. Recently, AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and shallow textual metadata from scientific publications and treats them as traditional documents so that the conventional search engine methodology could be applied. However, such a system fails to facilitate user search queries that seek to identify algorithm-specific information, such as the datasets on which algorithms operate, the performance of algorithms, and runtime complexity, etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are presented. Specifically, we propose a set of methods to automatically identify and extract algorithmic pseudo-codes and the sentence-level metadata that describe the algorithms.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not provided\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=\"/content/models/phi/Phi-4-mini-instruct.Q4_K_S.gguf\")\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(output[\"choices\"][0][\"message\"][\"content\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9Kzh_lu2E9H2",
        "outputId": "12a650bd-fb57-4583-cd2f-b97ef31535e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 36 key-value pairs and 196 tensors from /content/models/phi/Phi-4-mini-instruct.Q4_K_S.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
            "llama_model_loader: - kv   2:                               general.type str              = model\n",
            "llama_model_loader: - kv   3:                               general.name str              = Phi 4 Mini Instruct\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Phi-4\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   7:                            general.license str              = mit\n",
            "llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\n",
            "llama_model_loader: - kv   9:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\n",
            "llama_model_loader: - kv  10:                          general.languages arr[str,24]      = [\"multilingual\", \"ar\", \"zh\", \"cs\", \"d...\n",
            "llama_model_loader: - kv  11:                        phi3.context_length u32              = 131072\n",
            "llama_model_loader: - kv  12:  phi3.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  13:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  14:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  15:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv  16:                  phi3.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  17:               phi3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  18:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  20:                        phi3.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  21:                          general.file_type u32              = 14\n",
            "llama_model_loader: - kv  22:              phi3.attention.sliding_window u32              = 262144\n",
            "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = gpt-4o\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,199742]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"e r\", ...\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 199999\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 199999\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 199999\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 199999\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   67 tensors\n",
            "llama_model_loader: - type q4_K:  124 tensors\n",
            "llama_model_loader: - type q5_K:    4 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Small\n",
            "print_info: file size   = 2.17 GiB (4.86 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 200024 '<|/tool|>' is not marked as EOG\n",
            "load: control token: 200023 '<|tool|>' is not marked as EOG\n",
            "load: control token: 200022 '<|system|>' is not marked as EOG\n",
            "load: control token: 200021 '<|user|>' is not marked as EOG\n",
            "load: control token: 200025 '<|tool_call|>' is not marked as EOG\n",
            "load: control token: 200027 '<|tool_response|>' is not marked as EOG\n",
            "load: control token: 200028 '<|tag|>' is not marked as EOG\n",
            "load: control token: 200026 '<|/tool_call|>' is not marked as EOG\n",
            "load: control token: 200018 '<|endofprompt|>' is not marked as EOG\n",
            "load: control token: 200019 '<|assistant|>' is not marked as EOG\n",
            "load: special tokens cache size = 12\n",
            "load: token to piece cache size = 1.3333 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 24\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 262144\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 3\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.84 B\n",
            "print_info: general.name     = Phi 4 Mini Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 200064\n",
            "print_info: n_merges         = 199742\n",
            "print_info: BOS token        = 199999 '<|endoftext|>'\n",
            "print_info: EOS token        = 199999 '<|endoftext|>'\n",
            "print_info: EOT token        = 199999 '<|endoftext|>'\n",
            "print_info: UNK token        = 199999 '<|endoftext|>'\n",
            "print_info: PAD token        = 199999 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 199999 '<|endoftext|>'\n",
            "print_info: EOG token        = 200020 '<|end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q6_K) (and 258 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  2221.57 MiB\n",
            ".........................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_init_from_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.76 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   409.76 MiB\n",
            "llama_init_from_model: graph nodes  = 1286\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% else %}{{ eos_token }}{% endif %}\", 'phi3.rope.scaling.original_context_length': '4096', 'phi3.rope.scaling.attn_factor': '1.190238', 'general.architecture': 'phi3', 'general.license': 'mit', 'phi3.context_length': '131072', 'general.type': 'model', 'general.license.link': 'https://huggingface.co/microsoft/Phi-4-mini-instruct/resolve/main/LICENSE', 'tokenizer.ggml.pre': 'gpt-4o', 'general.basename': 'Phi-4', 'tokenizer.ggml.padding_token_id': '199999', 'phi3.attention.head_count': '24', 'phi3.attention.head_count_kv': '8', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.embedding_length': '3072', 'phi3.rope.dimension_count': '96', 'general.finetune': 'instruct', 'general.file_type': '14', 'phi3.rope.freq_base': '10000.000000', 'phi3.attention.sliding_window': '262144', 'phi3.block_count': '32', 'tokenizer.ggml.model': 'gpt2', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi 4 Mini Instruct', 'tokenizer.ggml.bos_token_id': '199999', 'tokenizer.ggml.unknown_token_id': '199999', 'tokenizer.ggml.eos_token_id': '199999', 'general.size_label': 'mini', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.add_eos_token': 'false'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% else %}{{ eos_token }}{% endif %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <|endoftext|>\n",
            "llama_perf_context_print:        load time =    4690.28 ms\n",
            "llama_perf_context_print: prompt eval time =    4690.03 ms /    18 tokens (  260.56 ms per token,     3.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3116.64 ms /     7 runs   (  445.23 ms per token,     2.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    7825.12 ms /    25 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db9bd7143b784aa8ba11205d0e732e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_01b60a6caccf4857bc73a7da5aa19569"
          }
        },
        "5ba427993e9748229c6c8b044aa089ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd3535397fd244bcbb82f887383530d9",
            "placeholder": "​",
            "style": "IPY_MODEL_026b34ed4a754a9aa71949b6aaaec2c5",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "316dd058216342e7b165d6bb1e9ac9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_de94a146de5d46f88e7d6b6ca34a80bd",
            "placeholder": "​",
            "style": "IPY_MODEL_cb7f5af19d0843e3bba2f738a4c1fab8",
            "value": ""
          }
        },
        "5f75314a92214fc8a7a146c7d1c63a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_57cb9cc0e48c49c4b64cdd8991c5ee8a",
            "style": "IPY_MODEL_08812ad919c34f798a2b401939bee9c4",
            "value": true
          }
        },
        "54f9c97f52b84a1ba87dab0488156331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_fc6dd6dd7f22489baf57c16662bcc699",
            "style": "IPY_MODEL_0a5e376ddd5b4804afc8688f35b7e52c",
            "tooltip": ""
          }
        },
        "fc3eb1dbc7f545188d0ecd3901063edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e83aabd326c94226b617c17f8310b96b",
            "placeholder": "​",
            "style": "IPY_MODEL_f66ff164e5ec44e0977add8cba2c7965",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "01b60a6caccf4857bc73a7da5aa19569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fd3535397fd244bcbb82f887383530d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "026b34ed4a754a9aa71949b6aaaec2c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de94a146de5d46f88e7d6b6ca34a80bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb7f5af19d0843e3bba2f738a4c1fab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57cb9cc0e48c49c4b64cdd8991c5ee8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08812ad919c34f798a2b401939bee9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc6dd6dd7f22489baf57c16662bcc699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a5e376ddd5b4804afc8688f35b7e52c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e83aabd326c94226b617c17f8310b96b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66ff164e5ec44e0977add8cba2c7965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d842e992634483482a70d53b10b7a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5dcbd3d333448909a4ad168a3336d94",
            "placeholder": "​",
            "style": "IPY_MODEL_da375ae79f0147d3a90baa1b355cdb44",
            "value": "Connecting..."
          }
        },
        "f5dcbd3d333448909a4ad168a3336d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da375ae79f0147d3a90baa1b355cdb44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d3bc68e10784da2b15a77689ac2f918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_982aea238d354ebc88237c14150d850c",
              "IPY_MODEL_8372c33f2a8a470887246d604c7e2d85",
              "IPY_MODEL_237844b1dd4c4be7bc1dafb80b2fba13"
            ],
            "layout": "IPY_MODEL_9e6f6c5fa68f4fee8b1422f2a1fde112"
          }
        },
        "982aea238d354ebc88237c14150d850c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_714e60940ee24d74b7d828fa2a7b9bb9",
            "placeholder": "​",
            "style": "IPY_MODEL_96e8c8cd3d2f4198832bb88b576be9e7",
            "value": "Phi-3.5-mini-instruct-Q5_K_S.gguf: 100%"
          }
        },
        "8372c33f2a8a470887246d604c7e2d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e2ebc12e8124beca3677dbba74f2970",
            "max": 2641474848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b3a09224d0e46b491096488a32cd023",
            "value": 2641474848
          }
        },
        "237844b1dd4c4be7bc1dafb80b2fba13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d390a0fa53e54352bd9f73678d678e42",
            "placeholder": "​",
            "style": "IPY_MODEL_0f0239a42cf7456c8097f211e154f5c3",
            "value": " 2.64G/2.64G [00:23&lt;00:00, 31.3MB/s]"
          }
        },
        "9e6f6c5fa68f4fee8b1422f2a1fde112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "714e60940ee24d74b7d828fa2a7b9bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e8c8cd3d2f4198832bb88b576be9e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e2ebc12e8124beca3677dbba74f2970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b3a09224d0e46b491096488a32cd023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d390a0fa53e54352bd9f73678d678e42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0239a42cf7456c8097f211e154f5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip -q install fitz\n",
        "!pip -q install transformers\n",
        "!pip -q install torch\n",
        "!pip install -q tiktoken einops accelerate bitsandbytes\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr_BUl-sjqPt",
        "outputId": "0c348de1-af21-4d1d-e9c1-7c751a861cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import fitz\n",
        "import json\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline,BitsAndBytesConfig\n",
        "import re"
      ],
      "metadata": {
        "id": "m1nDCuYWnyAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ STEP 1: Install requirements\n",
        "!pip install llama-cpp-python==0.2.24 PyMuPDF tqdm --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05Kz7Feo7dBR",
        "outputId": "9d4b01eb-c2d6-4ef6-a0c6-d240a1d57df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gosr3AmZArHF",
        "outputId": "f851221c-4b30-4fae-aff2-4e029d2c5e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.2.24)\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl size=5959970 sha256=6d7b3a44b6c4fa8141bb0dbb266cacf29b726fdc98364f31eee6ce90d8227bdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/03/66/eb3810eafd55d921b2be32896d1f44313996982360663aa80b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.24\n",
            "    Uninstalling llama_cpp_python-0.2.24:\n",
            "      Successfully uninstalled llama_cpp_python-0.2.24\n",
            "Successfully installed llama-cpp-python-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token when prompted\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "db9bd7143b784aa8ba11205d0e732e3b",
            "5ba427993e9748229c6c8b044aa089ab",
            "316dd058216342e7b165d6bb1e9ac9b1",
            "5f75314a92214fc8a7a146c7d1c63a26",
            "54f9c97f52b84a1ba87dab0488156331",
            "fc3eb1dbc7f545188d0ecd3901063edb",
            "01b60a6caccf4857bc73a7da5aa19569",
            "fd3535397fd244bcbb82f887383530d9",
            "026b34ed4a754a9aa71949b6aaaec2c5",
            "de94a146de5d46f88e7d6b6ca34a80bd",
            "cb7f5af19d0843e3bba2f738a4c1fab8",
            "57cb9cc0e48c49c4b64cdd8991c5ee8a",
            "08812ad919c34f798a2b401939bee9c4",
            "fc6dd6dd7f22489baf57c16662bcc699",
            "0a5e376ddd5b4804afc8688f35b7e52c",
            "e83aabd326c94226b617c17f8310b96b",
            "f66ff164e5ec44e0977add8cba2c7965",
            "7d842e992634483482a70d53b10b7a18",
            "f5dcbd3d333448909a4ad168a3336d94",
            "da375ae79f0147d3a90baa1b355cdb44"
          ]
        },
        "id": "Od5O3RVd-V-I",
        "outputId": "77a787f7-5217-460d-c5be-d10ac72cd79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db9bd7143b784aa8ba11205d0e732e3b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"bartowski/Phi-3.5-mini-instruct-GGUF\",\n",
        "    filename=\"Phi-3.5-mini-instruct-Q5_K_S.gguf\",\n",
        "    local_dir=\"/content/models/phi3.5\",\n",
        "    local_dir_use_symlinks=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "7d3bc68e10784da2b15a77689ac2f918",
            "982aea238d354ebc88237c14150d850c",
            "8372c33f2a8a470887246d604c7e2d85",
            "237844b1dd4c4be7bc1dafb80b2fba13",
            "9e6f6c5fa68f4fee8b1422f2a1fde112",
            "714e60940ee24d74b7d828fa2a7b9bb9",
            "96e8c8cd3d2f4198832bb88b576be9e7",
            "8e2ebc12e8124beca3677dbba74f2970",
            "2b3a09224d0e46b491096488a32cd023",
            "d390a0fa53e54352bd9f73678d678e42",
            "0f0239a42cf7456c8097f211e154f5c3"
          ]
        },
        "id": "XYS_uF4W-a7W",
        "outputId": "b802f46b-ef02-491f-87f7-d5d6e5c7e035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:933: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Phi-3.5-mini-instruct-Q5_K_S.gguf:   0%|          | 0.00/2.64G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d3bc68e10784da2b15a77689ac2f918"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "def load_phi_model(model_path):\n",
        "    try:\n",
        "        llm = Llama(\n",
        "            model_path=model_path,  # Path to your GGUF model\n",
        "            n_ctx=4096,             # Context window size\n",
        "            n_threads=8,            # Number of threads\n",
        "            n_gpu_layers=0,         # Set to 0 if using CPU-only\n",
        "            use_mmap=False,         # Use memory-mapped file (disabled here)\n",
        "            verbose=True            # Set to True for verbose output\n",
        "        )\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(\"Error loading model:\", e)\n",
        "        return None\n",
        "\n",
        "model_path = \"/content/models/phi3.5/Phi-3.5-mini-instruct-Q5_K_S.gguf\"\n",
        "model = load_phi_model(model_path)\n",
        "\n",
        "if model is None:\n",
        "    print(\"Model loading failed.\")\n",
        "else:\n",
        "    print(\"Model loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kG2ZT1XsAwgu",
        "outputId": "7598233f-e644-4701-fbf4-aa1f003e16d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 40 key-value pairs and 197 tensors from /content/models/phi3.5/Phi-3.5-mini-instruct-Q5_K_S.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Phi-3.5\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   6:                            general.license str              = mit\n",
            "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\n",
            "llama_model_loader: - kv   8:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\n",
            "llama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"multilingual\"]\n",
            "llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  14:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 16\n",
            "llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\n",
            "llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
            "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/Phi-3.5-mini-instruct-GGU...\n",
            "llama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 128\n",
            "llama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 151\n",
            "llama_model_loader: - type  f32:   67 tensors\n",
            "llama_model_loader: - type q5_K:  129 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q5_K - Small\n",
            "print_info: file size   = 2.46 GiB (5.53 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:  32008 '<|placeholder5|>' is not marked as EOG\n",
            "load: control token:  32006 '<|system|>' is not marked as EOG\n",
            "load: control token:  32002 '<|placeholder1|>' is not marked as EOG\n",
            "load: control token:  32001 '<|assistant|>' is not marked as EOG\n",
            "load: control token:  32004 '<|placeholder3|>' is not marked as EOG\n",
            "load: control token:  32003 '<|placeholder2|>' is not marked as EOG\n",
            "load: control token:      0 '<unk>' is not marked as EOG\n",
            "load: control token:  32005 '<|placeholder4|>' is not marked as EOG\n",
            "load: control token:  32010 '<|user|>' is not marked as EOG\n",
            "load: control token:  32009 '<|placeholder6|>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special tokens cache size = 14\n",
            "load: token to piece cache size = 0.1685 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 262144\n",
            "print_info: n_embd_head_k    = 96\n",
            "print_info: n_embd_head_v    = 96\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 3072\n",
            "print_info: n_embd_v_gqa     = 3072\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi 3.5 Mini Instruct\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q5_K) (and 258 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:          CPU model buffer size =  2518.40 MiB\n",
            "load_all_data: no device found for buffer type CPU for async uploads\n",
            "....................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB\n",
            "llama_init_from_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   300.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1286\n",
            "llama_init_from_model: graph splits = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.chunks_count': '151', 'quantize.imatrix.entries_count': '128', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.file': '/models_out/Phi-3.5-mini-instruct-GGUF/Phi-3.5-mini-instruct.imatrix', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\", 'phi3.rope.scaling.original_context_length': '4096', 'general.architecture': 'phi3', 'phi3.rope.scaling.attn_factor': '1.190238', 'general.license': 'mit', 'phi3.context_length': '131072', 'general.type': 'model', 'general.license.link': 'https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE', 'tokenizer.ggml.pre': 'default', 'general.basename': 'Phi-3.5', 'tokenizer.ggml.padding_token_id': '32000', 'phi3.attention.head_count': '32', 'phi3.attention.head_count_kv': '32', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.embedding_length': '3072', 'phi3.rope.dimension_count': '96', 'general.finetune': 'instruct', 'general.file_type': '16', 'phi3.rope.freq_base': '10000.000000', 'phi3.attention.sliding_window': '262144', 'phi3.block_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi 3.5 Mini Instruct', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.size_label': 'mini', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.add_eos_token': 'false'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% else %}{{ eos_token }}{% endif %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_phi_model(model_path):\n",
        "    llm = Llama(\n",
        "    model_path=\"/content/models/phi3.5/Phi-3.5-mini-instruct-Q5_K_S.gguf\",\n",
        "    n_ctx=4096,\n",
        "    n_threads=8,\n",
        "    n_gpu_layers=0,  # Reduce to 0 if CPU-only\n",
        "    use_mmap=True,\n",
        "    verbose=False\n",
        ")\n",
        "    return llm\n"
      ],
      "metadata": {
        "id": "dnzhfqyh-1un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "from llama_cpp import Llama  # Make sure to install llama-cpp\n",
        "\n",
        "# Extract clean JSON from output\n",
        "def extract_json(text):\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"Failed to extract JSON: {str(e)}\"}\n",
        "\n",
        "# Build prompt suited for TinyLLaMA-style chat model\n",
        "def build_prompt(text):\n",
        "    instruction = \"\"\"\n",
        "You are an information extraction engine. Return ONLY valid JSON, no explanations.\n",
        "\n",
        "JSON Structure:\n",
        "{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}\n",
        "\n",
        "Extract metadata from the following scientific paper:\n",
        "\"\"\"\n",
        "    return f\"<|user|>\\n{instruction.strip()}\\n{text[:2000]}\\n<|assistant|>\"\n",
        "\n",
        "\n",
        "# Call model and extract structured data\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_tokens=1024,             # allow enough room for full JSON\n",
        "        temperature=0,               # deterministic output\n",
        "        top_p=1.0,\n",
        "        stop=[\"<|end|>\", \"</s>\"],    # prevent model from rambling\n",
        "    )\n",
        "\n",
        "\n",
        "    # print(response)\n",
        "    raw_output = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract raw text from PDF using PyMuPDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "# Load LLaMA 3 GGUF Model\n",
        "\n",
        "# Path to your GGUF model\n",
        "model_path = \"/content/models/phi3.5/Phi-3.5-mini-instruct.Q8_0.gguf\"\n",
        "# Load model\n",
        "model = load_phi_model(model_path)\n",
        "\n",
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/1.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM56CjZIYQu3",
        "outputId": "c3e8bd13-5d3e-45fb-c491-25da53ba867b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Extracting Scientific Figures with Distantly Supervised Neural Networks\",\n",
            "  \"Authors\": [\"Noah Siegel\", \"Nicholas Lourie\", \"Russell Power\", \"Waleed Ammar\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Figure Extraction\", \"Distant Supervision\", \"Deep Learning\", \"Neural Networks\", \"Computer Vision\"],\n",
            "  \"Abstract\": \"Non-textual components such as charts, diagrams and tables provide key information in many scientific documents. However, the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. We leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels—4,000 times larger than the previous largest figure extraction dataset—with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar, a large-scale academic search engine, and used to extract figures in 13 million scientific documents.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not provided\"\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Scientific Figures with Distantly Supervised Neural Networks\",\n",
            "    \"Authors\": [\n",
            "        \"Noah Siegel\",\n",
            "        \"Nicholas Lourie\",\n",
            "        \"Russell Power\",\n",
            "        \"Waleed Ammar\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Figure Extraction\",\n",
            "        \"Distant Supervision\",\n",
            "        \"Deep Learning\",\n",
            "        \"Neural Networks\",\n",
            "        \"Computer Vision\"\n",
            "    ],\n",
            "    \"Abstract\": \"Non-textual components such as charts, diagrams and tables provide key information in many scientific documents. However, the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. We leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels\\u20144,000 times larger than the previous largest figure extraction dataset\\u2014with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar, a large-scale academic search engine, and used to extract figures in 13 million scientific documents.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not provided\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/2.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0ZBGVqAt2zV",
        "outputId": "708e164b-e52b-4e53-9b7a-a53b559f93bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Automatic Recognition of Learning Resource\",\n",
            "  \"Authors\": [\"Soumya Banerjee\", \"Debarshi Kumar Sanyal\", \"Samiran Chattopadhyay\", \"Plaban Kumar Bhowmick\", \"Partha Pratim Das\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"deep learning\", \"transfer learning\", \"digital library\"],\n",
            "  \"Abstract\": \"Digital libraries generally need to process a large volume of diverse document types. The collection and tagging of metadata is a long, error-prone, manpower-consuming task. We are attempting to build an automatic metadata extractor for digital libraries. In this work, we present the Heterogeneous Learning Resources (HLR) dataset for document image classification. The individual learning resource is first decomposed into its constituent document images (sheets) which are then passed through an OCR tool to obtain the textual representation. The document image and its textual content are classified with state-of-the-art classifiers. Finally, the labels of the constituent document images are used to predict the label of the overall document.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": null\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Automatic Recognition of Learning Resource\",\n",
            "    \"Authors\": [\n",
            "        \"Soumya Banerjee\",\n",
            "        \"Debarshi Kumar Sanyal\",\n",
            "        \"Samiran Chattopadhyay\",\n",
            "        \"Plaban Kumar Bhowmick\",\n",
            "        \"Partha Pratim Das\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"deep learning\",\n",
            "        \"transfer learning\",\n",
            "        \"digital library\"\n",
            "    ],\n",
            "    \"Abstract\": \"Digital libraries generally need to process a large volume of diverse document types. The collection and tagging of metadata is a long, error-prone, manpower-consuming task. We are attempting to build an automatic metadata extractor for digital libraries. In this work, we present the Heterogeneous Learning Resources (HLR) dataset for document image classification. The individual learning resource is first decomposed into its constituent document images (sheets) which are then passed through an OCR tool to obtain the textual representation. The document image and its textual content are classified with state-of-the-art classifiers. Finally, the labels of the constituent document images are used to predict the label of the overall document.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/3.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsB1zTWCvcVd",
        "outputId": "a0e60eb7-503e-4083-f153-c2169d2c4824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Model Selection with Model Zoo via Graph Learning\",\n",
            "  \"Authors\": [\"Ziyu Li\", \"Hilco van der Wilk\", \"Danning Zhan\", \"Megha Khosla\", \"Alessandro Bozzon\", \"Rihan Hai\"],\n",
            "  \"DOI\": \"https://doi.org/10.1016/j.mlr.2023.01.001\",\n",
            "  \"Keywords\": [\"Deep Learning\", \"Model Selection\", \"Model Zoo\", \"Graph Learning\", \"TransferGraph\"],\n",
            "  \"Abstract\": \"Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph’s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 25\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Model Selection with Model Zoo via Graph Learning\",\n",
            "    \"Authors\": [\n",
            "        \"Ziyu Li\",\n",
            "        \"Hilco van der Wilk\",\n",
            "        \"Danning Zhan\",\n",
            "        \"Megha Khosla\",\n",
            "        \"Alessandro Bozzon\",\n",
            "        \"Rihan Hai\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1016/j.mlr.2023.01.001\",\n",
            "    \"Keywords\": [\n",
            "        \"Deep Learning\",\n",
            "        \"Model Selection\",\n",
            "        \"Model Zoo\",\n",
            "        \"Graph Learning\",\n",
            "        \"TransferGraph\"\n",
            "    ],\n",
            "    \"Abstract\": \"Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph\\u2019s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 25\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/4.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jECQ5ACavdrf",
        "outputId": "6a3f1d31-13d1-421d-bed4-c2d4f56b2d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Extracting Decision Model and Notation models from text using deep learning techniques\",\n",
            "  \"Authors\": [\"Alexandre Goossens\", \"Johannes De Smedt\", \"Jan Vanthienen\"],\n",
            "  \"DOI\": \"https://doi.org/10.1016/j.eswa.2022.118667\",\n",
            "  \"Keywords\": [\"Deep learning\", \"Decision Model and Notation\", \"DMN\", \"Decision model extraction\"],\n",
            "  \"Abstract\": \"Companies and organizations often use manuals and guidelines to communicate and execute operational decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions. Modeling a decision from a textual source, however, is a time intensive and complex activity hence a need for shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic extraction of decision models from text.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Decision Model and Notation models from text using deep learning techniques\",\n",
            "    \"Authors\": [\n",
            "        \"Alexandre Goossens\",\n",
            "        \"Johannes De Smedt\",\n",
            "        \"Jan Vanthienen\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1016/j.eswa.2022.118667\",\n",
            "    \"Keywords\": [\n",
            "        \"Deep learning\",\n",
            "        \"Decision Model and Notation\",\n",
            "        \"DMN\",\n",
            "        \"Decision model extraction\"\n",
            "    ],\n",
            "    \"Abstract\": \"Companies and organizations often use manuals and guidelines to communicate and execute operational decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions. Modeling a decision from a textual source, however, is a time intensive and complex activity hence a need for shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic extraction of decision models from text.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/5.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeP2cW6tvgKf",
        "outputId": "7dcac282-377b-43c9-a3d2-b3444dab5f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Automated data extraction with conversational LLMs for materials science\",\n",
            "  \"Authors\": [\"Maciej P. Polak\", \"Dane Morgan\"],\n",
            "  \"DOI\": \"10.1038/s41467-024-45914-8\",\n",
            "  \"Keywords\": [\"Automated data extraction\", \"Conversational LLMs\", \"Materials science\", \"Data accuracy\", \"ChatExtract\"],\n",
            "  \"Abstract\": \"This work introduces ChatExtract, a method for automating accurate data extraction from research papers using conversational language models. ChatExtract employs a set of engineered prompts to identify, extract, and verify data through follow-up questions, addressing issues of factual inaccuracy. The method demonstrates high precision and recall, achieving near-90% accuracy with LLMs like GPT-4. The paper also presents databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys developed using ChatExtract. The simplicity, transferability, and accuracy of ChatExtract suggest its potential as a powerful tool for data extraction in materials science and beyond.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Automated data extraction with conversational LLMs for materials science\",\n",
            "    \"Authors\": [\n",
            "        \"Maciej P. Polak\",\n",
            "        \"Dane Morgan\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1038/s41467-024-45914-8\",\n",
            "    \"Keywords\": [\n",
            "        \"Automated data extraction\",\n",
            "        \"Conversational LLMs\",\n",
            "        \"Materials science\",\n",
            "        \"Data accuracy\",\n",
            "        \"ChatExtract\"\n",
            "    ],\n",
            "    \"Abstract\": \"This work introduces ChatExtract, a method for automating accurate data extraction from research papers using conversational language models. ChatExtract employs a set of engineered prompts to identify, extract, and verify data through follow-up questions, addressing issues of factual inaccuracy. The method demonstrates high precision and recall, achieving near-90% accuracy with LLMs like GPT-4. The paper also presents databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys developed using ChatExtract. The simplicity, transferability, and accuracy of ChatExtract suggest its potential as a powerful tool for data extraction in materials science and beyond.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/6.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxkniPAYxKAq",
        "outputId": "b38e70c4-3029-4465-fc5e-521d7d8a771c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Smart Learning Environments: Prerequisites-based course recommendation using concept prerequisites and metadata matching\",\n",
            "  \"Authors\": [\"Abdessamad Chanaa\", \"Nour-eddine El Faddouli\"],\n",
            "  \"DOI\": \"10.1186/s40561-024-00301-0\",\n",
            "  \"Keywords\": [\"Smart Learning Environments\", \"Prerequisites-based course recommendation\", \"Concept prerequisites\", \"Metadata matching\"],\n",
            "  \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. However, classical recommender systems usually suffer from item cold-start issues. Besides, unlike other fields like e-commerce or entertainment, e-learning recommendations must ensure that learners have the adequate background knowledge to cognitively receive the recommended learning objects. For that reason, when designing an efficient e-learning recommendation method, these challenges should be considered. To address those issues, in this paper, we first propose extracting pairs of concept prerequisites using Linked Open Data (LOD). Then, we evaluate the proposed list of prerequisite relationships using metadata matching.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Smart Learning Environments: Prerequisites-based course recommendation using concept prerequisites and metadata matching\",\n",
            "    \"Authors\": [\n",
            "        \"Abdessamad Chanaa\",\n",
            "        \"Nour-eddine El Faddouli\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1186/s40561-024-00301-0\",\n",
            "    \"Keywords\": [\n",
            "        \"Smart Learning Environments\",\n",
            "        \"Prerequisites-based course recommendation\",\n",
            "        \"Concept prerequisites\",\n",
            "        \"Metadata matching\"\n",
            "    ],\n",
            "    \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. However, classical recommender systems usually suffer from item cold-start issues. Besides, unlike other fields like e-commerce or entertainment, e-learning recommendations must ensure that learners have the adequate background knowledge to cognitively receive the recommended learning objects. For that reason, when designing an efficient e-learning recommendation method, these challenges should be considered. To address those issues, in this paper, we first propose extracting pairs of concept prerequisites using Linked Open Data (LOD). Then, we evaluate the proposed list of prerequisite relationships using metadata matching.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/7.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awgyD9UQxLmi",
        "outputId": "69da4cbf-32c1-43f0-bf13-14c7f877df8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Deep Transfer Learning for Metadata Extraction from German Publications\",\n",
            "  \"Authors\": [\"Zeyd Boukhers\", \"Nada Beili\", \"Timo Hartmann\", \"Prantik Goswami\", \"Muhammad Arslan Zafar\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"transfer learning\", \"metadata extraction\", \"neural networks\"],\n",
            "  \"Abstract\": \"In contrast to most of the English scientiﬁc publications that follow standard and simple layouts, the order, content, position and size of metadata in German publications vary greatly among publications. This variety makes traditional NLP methods fail to accurately extract metadata from these publications. In this paper, we present a method that extracts metadata from PDF documents with different layouts and styles by viewing the document as an image. We used Mask R-CNN which is trained on COCO dataset and finetuned with PubLayNet dataset that consists of 200K PDF snapshots with five basic classes (e.g. text, figure, etc). We refine-tuned the model on our proposed synthetic dataset consisting of 30K article snapshots to extract nine patterns (i.e. author, title, etc). Our method achieved an average accuracy of around 90% which validates its capability to accurately extract metadata from a variety of PDF documents with challenging templates.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": null\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Deep Transfer Learning for Metadata Extraction from German Publications\",\n",
            "    \"Authors\": [\n",
            "        \"Zeyd Boukhers\",\n",
            "        \"Nada Beili\",\n",
            "        \"Timo Hartmann\",\n",
            "        \"Prantik Goswami\",\n",
            "        \"Muhammad Arslan Zafar\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"transfer learning\",\n",
            "        \"metadata extraction\",\n",
            "        \"neural networks\"\n",
            "    ],\n",
            "    \"Abstract\": \"In contrast to most of the English scienti\\ufb01c publications that follow standard and simple layouts, the order, content, position and size of metadata in German publications vary greatly among publications. This variety makes traditional NLP methods fail to accurately extract metadata from these publications. In this paper, we present a method that extracts metadata from PDF documents with different layouts and styles by viewing the document as an image. We used Mask R-CNN which is trained on COCO dataset and finetuned with PubLayNet dataset that consists of 200K PDF snapshots with five basic classes (e.g. text, figure, etc). We refine-tuned the model on our proposed synthetic dataset consisting of 30K article snapshots to extract nine patterns (i.e. author, title, etc). Our method achieved an average accuracy of around 90% which validates its capability to accurately extract metadata from a variety of PDF documents with challenging templates.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "from llama_cpp import Llama  # Make sure to install llama-cpp\n",
        "\n",
        "# Extract clean JSON from output\n",
        "def extract_json(text):\n",
        "    text = re.sub(r'(?<!\\\\)\\n', ' ', text)\n",
        "    text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
        "    text = re.sub(r\"\\(.,*?\\)\", \"\", text)  # Remove any text inside parentheses\n",
        "    text = re.sub(r\"[\\n\\t]\", \" \", text)  # Remove newline and tab characters\n",
        "    text = re.sub(r\"(ERNIE 3\\.0):\", r\"\\1\", text)\n",
        "    print(\"raw text here\",text)\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"Failed to extract JSON: {str(e)}\"}\n",
        "\n",
        "# Build prompt suited for TinyLLaMA-style chat model\n",
        "def build_prompt(text):\n",
        "    instruction = \"\"\"\n",
        "You are an information extraction engine. Return ONLY valid JSON, no explanations.\n",
        "\n",
        "JSON Structure:\n",
        "{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}\n",
        "\n",
        "Extract metadata from the following scientific paper:\n",
        "\"\"\"\n",
        "    return f\"<|user|>\\n{instruction.strip()}\\n{text[:2000]}\\n<|assistant|>\"\n",
        "\n",
        "\n",
        "# Call model and extract structured data\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_tokens=1024,             # allow enough room for full JSON\n",
        "        temperature=0,               # deterministic output\n",
        "        top_p=1.0,\n",
        "        stop=[\"<|end|>\", \"</s>\"],    # prevent model from rambling\n",
        "    )\n",
        "\n",
        "\n",
        "    # print(response)\n",
        "    raw_output = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract raw text from PDF using PyMuPDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "# Load LLaMA 3 GGUF Model\n",
        "\n",
        "# Path to your GGUF model\n",
        "model_path = \"/content/models/phi3.5/Phi-3.5-mini-instruct.Q8_0.gguf\"\n",
        "# Load model\n",
        "model = load_phi_model(model_path)\n",
        "\n",
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/8.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRCTxRA4xNR-",
        "outputId": "70495c31-3749-408b-9408-f344dffac8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"ERNIE 3.0: LARGE-SCALE KNOWLEDGE ENHANCED\n",
            "PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND\n",
            "GENERATION\",\n",
            "  \"Authors\": [\"Yu Sun\", \"Shuohuan Wang\", \"Shikun Feng\", \"Siyu Ding\", \"Chao Pang\", \"Junyuan Shang\", \"Jiaxiang Liu\", \"Xuyi Chen\", \"Yanbin Zhao\", \"Yuxiang Lu\", \"Weixin Liu\", \"Zhihua Wu\", \"Weibao Gong\", \"Jianzhong Liang\", \"Zhizhou Shang\", \"Peng Sun\", \"Wei Liu\", \"Xuan Ouyang\", \"Dianhai Yu\", \"Hao Tian\", \"Hua Wu\", \"Haifeng Wang\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"ERNIE 3.0\", \"large-scale knowledge enhanced models\", \"pre-training\", \"language understanding\", \"language generation\", \"zero-shot learning\", \"few-shot learning\", \"auto-regressive network\", \"auto-encoding network\"],\n",
            "  \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing\n",
            "(NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained\n",
            "language models can improve their generalization abilities. Particularly, the GPT-3 model with 175\n",
            "billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite\n",
            "their success, these large-scale models are trained on plain texts without introducing knowledge such\n",
            "as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an\n",
            "auto-regressive way. As a result, this kind of traditional ﬁne-tuning approach demonstrates relatively\n",
            "weak performance when solving downstream language understanding tasks. In order to solve the\n",
            "above problems, we propose a uniﬁed framework named ERNIE 3.0 for pre-training large-scale\n",
            "knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that\n",
            "the trained model can be easily tailored for both natural language understanding and generation\n",
            "tasks with zero-shot learning, few-shot learning or ﬁne-tuning. We trained the model with 10 billion\n",
            "parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical\n",
            "results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its\n",
            "English version achieves the ﬁrst place on the SuperGLUE [3] benchmark (July 3, 2021), surpassing\n",
            "the human performance by +0.8% (90.6% vs. 89.8%).\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "\n",
            "\n",
            "## Response\n",
            "\n",
            "{\n",
            "  \"Title\": \"ERNIE 3.0: LARGE-SCALE KNOWLEDGE ENHANCED\n",
            "PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND\n",
            "GENERATION\",\n",
            "  \"Authors\": [\"Yu Sun\", \"Shuohuan Wang\", \"Shikun Feng\", \"Siyu Ding\", \"Chao Pang\", \"Junyuan Shang\", \"Jiaxiang Liu\", \"Xuyi Chen\", \"Yanbin Zhao\", \"Yuxiang Lu\", \"Weixin Liu\", \"Zhihua Wu\", \"Weibao Gong\", \"Jianzhong Liang\", \"Zhizhou Shang\", \"Peng Sun\", \"Wei Liu\", \"Xuan Ouyang\", \"Dianhai Yu\", \"Hao Tian\", \"Hua Wu\", \"Haifeng Wang\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"ERNIE 3.0\", \"large-scale knowledge enhanced models\", \"pre-training\", \"language understanding\", \"language generation\", \"zero-shot learning\", \"few-shot learning\", \"auto-regressive network\", \"auto-encoding network\"],\n",
            "  \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing\n",
            "(N\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "raw text here  {   \"Title\": \"ERNIE 3.0 LARGE-SCALE KNOWLEDGE ENHANCED PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND GENERATION\",   \"Authors\": [\"Yu Sun\", \"Shuohuan Wang\", \"Shikun Feng\", \"Siyu Ding\", \"Chao Pang\", \"Junyuan Shang\", \"Jiaxiang Liu\", \"Xuyi Chen\", \"Yanbin Zhao\", \"Yuxiang Lu\", \"Weixin Liu\", \"Zhihua Wu\", \"Weibao Gong\", \"Jianzhong Liang\", \"Zhizhou Shang\", \"Peng Sun\", \"Wei Liu\", \"Xuan Ouyang\", \"Dianhai Yu\", \"Hao Tian\", \"Hua Wu\", \"Haifeng Wang\"],   \"DOI\": \"Not provided\",   \"Keywords\": [\"ERNIE 3.0\", \"large-scale knowledge enhanced models\", \"pre-training\", \"language understanding\", \"language generation\", \"zero-shot learning\", \"few-shot learning\", \"auto-regressive network\", \"auto-encoding network\"],   \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional ﬁne-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a uniﬁed framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or ﬁne-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the ﬁrst place on the SuperGLUE [3] benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).\",   \"Document Type\": \"Research Paper\",   \"Number of References\": 0 }   ## Response  {   \"Title\": \"ERNIE 3.0 LARGE-SCALE KNOWLEDGE ENHANCED PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND GENERATION\",   \"Authors\": [\"Yu Sun\", \"Shuohuan Wang\", \"Shikun Feng\", \"Siyu Ding\", \"Chao Pang\", \"Junyuan Shang\", \"Jiaxiang Liu\", \"Xuyi Chen\", \"Yanbin Zhao\", \"Yuxiang Lu\", \"Weixin Liu\", \"Zhihua Wu\", \"Weibao Gong\", \"Jianzhong Liang\", \"Zhizhou Shang\", \"Peng Sun\", \"Wei Liu\", \"Xuan Ouyang\", \"Dianhai Yu\", \"Hao Tian\", \"Hua Wu\", \"Haifeng Wang\"],   \"DOI\": \"Not provided\",   \"Keywords\": [\"ERNIE 3.0\", \"large-scale knowledge enhanced models\", \"pre-training\", \"language understanding\", \"language generation\", \"zero-shot learning\", \"few-shot learning\", \"auto-regressive network\", \"auto-encoding network\"],   \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (N\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Error\": \"Failed to extract JSON: Extra data: line 1 column 2299 (char 2298)\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/9.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PADD36z3TC9",
        "outputId": "acf7490e-719a-4959-dce6-91fd152afaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Few-shot named entity recognition framework for forestry science metadata extraction\",\n",
            "  \"Authors\": [\"Yuquan Fan\", \"Hong Xiao\", \"Min Wang\", \"Junchi Wang\", \"Wenchao Jiang\", \"Chang Zhu\"],\n",
            "  \"DOI\": \"10.1007/s12652-023-04740-4\",\n",
            "  \"Keywords\": [\"Few-shot Named Entity Recognition\", \"Metadata Extraction\", \"Forestry Science\", \"Data Augmentation\", \"Multi-granularity Dilated Convolution Neural Network\"],\n",
            "  \"Abstract\": \"The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the efficient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is challenging inherent, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic comprehension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture an\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Few-shot named entity recognition framework for forestry science metadata extraction\",\n",
            "    \"Authors\": [\n",
            "        \"Yuquan Fan\",\n",
            "        \"Hong Xiao\",\n",
            "        \"Min Wang\",\n",
            "        \"Junchi Wang\",\n",
            "        \"Wenchao Jiang\",\n",
            "        \"Chang Zhu\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1007/s12652-023-04740-4\",\n",
            "    \"Keywords\": [\n",
            "        \"Few-shot Named Entity Recognition\",\n",
            "        \"Metadata Extraction\",\n",
            "        \"Forestry Science\",\n",
            "        \"Data Augmentation\",\n",
            "        \"Multi-granularity Dilated Convolution Neural Network\"\n",
            "    ],\n",
            "    \"Abstract\": \"The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the efficient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is challenging inherent, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic comprehension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture an\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/10.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyRH6npn3VwU",
        "outputId": "6d3aec3b-d9a1-4625-d2a5-bd621e7a155b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Extracting structured knowledge from scientific text with large language models\",\n",
            "  \"Authors\": [\"John Dagdelen\", \"Alexander Dunn\", \"Sanghoon Lee\", \"Nicholas Walker\", \"Andrew S. Rosen\", \"Gerbrand Ceder\", \"Kristin A. Persson\", \"Anubhav Jain\"],\n",
            "  \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "  \"Keywords\": [\"structured knowledge extraction\", \"named entity recognition\", \"relation extraction\", \"large language models\", \"GPT-3\", \"Llama-2\", \"materials chemistry\", \"dopants\", \"host materials\", \"metal-organic frameworks\", \"composition\", \"phase\", \"morphology\", \"application\"],\n",
            "  \"Abstract\": \"Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pre-trained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting structured knowledge from scientific text with large language models\",\n",
            "    \"Authors\": [\n",
            "        \"John Dagdelen\",\n",
            "        \"Alexander Dunn\",\n",
            "        \"Sanghoon Lee\",\n",
            "        \"Nicholas Walker\",\n",
            "        \"Andrew S. Rosen\",\n",
            "        \"Gerbrand Ceder\",\n",
            "        \"Kristin A. Persson\",\n",
            "        \"Anubhav Jain\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "    \"Keywords\": [\n",
            "        \"structured knowledge extraction\",\n",
            "        \"named entity recognition\",\n",
            "        \"relation extraction\",\n",
            "        \"large language models\",\n",
            "        \"GPT-3\",\n",
            "        \"Llama-2\",\n",
            "        \"materials chemistry\",\n",
            "        \"dopants\",\n",
            "        \"host materials\",\n",
            "        \"metal-organic frameworks\",\n",
            "        \"composition\",\n",
            "        \"phase\",\n",
            "        \"morphology\",\n",
            "        \"application\"\n",
            "    ],\n",
            "    \"Abstract\": \"Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pre-trained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/11.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zloy2BBSEeWE",
        "outputId": "afd63ded-7c44-47cc-e1d4-86d06f826901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Extracting structured knowledge from scientific text with large language models\",\n",
            "  \"Authors\": [\"John Dagdelen\", \"Alexander Dunn\", \"Sanghoon Lee\", \"Nicholas Walker\", \"Andrew S. Rosen\", \"Gerbrand Ceder\", \"Kristin A. Persson\", \"Anubhav Jain\"],\n",
            "  \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "  \"Keywords\": [\"structured knowledge extraction\", \"named entity recognition\", \"relation extraction\", \"large language models\", \"GPT-3\", \"Llama-2\", \"materials chemistry\", \"dopants\", \"host materials\", \"metal-organic frameworks\", \"composition\", \"phase\", \"morphology\", \"application\"],\n",
            "  \"Abstract\": \"Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pre-trained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting structured knowledge from scientific text with large language models\",\n",
            "    \"Authors\": [\n",
            "        \"John Dagdelen\",\n",
            "        \"Alexander Dunn\",\n",
            "        \"Sanghoon Lee\",\n",
            "        \"Nicholas Walker\",\n",
            "        \"Andrew S. Rosen\",\n",
            "        \"Gerbrand Ceder\",\n",
            "        \"Kristin A. Persson\",\n",
            "        \"Anubhav Jain\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "    \"Keywords\": [\n",
            "        \"structured knowledge extraction\",\n",
            "        \"named entity recognition\",\n",
            "        \"relation extraction\",\n",
            "        \"large language models\",\n",
            "        \"GPT-3\",\n",
            "        \"Llama-2\",\n",
            "        \"materials chemistry\",\n",
            "        \"dopants\",\n",
            "        \"host materials\",\n",
            "        \"metal-organic frameworks\",\n",
            "        \"composition\",\n",
            "        \"phase\",\n",
            "        \"morphology\",\n",
            "        \"application\"\n",
            "    ],\n",
            "    \"Abstract\": \"Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pre-trained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/12.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDj413LlEf4E",
        "outputId": "7e03947d-b29a-4c9f-9220-24f45a2112ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community\",\n",
            "  \"Authors\": [\n",
            "    \"Qingyao AIa\",\n",
            "    \"Ting BAIb\",\n",
            "    \"Zhao CAOc\",\n",
            "    \"Yi CHANGd\",\n",
            "    \"Jiawei CHEN(́)\",\n",
            "    \"Zhumin CHENf\",\n",
            "    \"Zhiyong CHENGg\",\n",
            "    \"Shoubin DONGh\",\n",
            "    \"Zhicheng DOUi\",\n",
            "    \"Fuli FENG j\",\n",
            "    \"Shen GAO f\",\n",
            "    \"Jiafeng GUOk\",\n",
            "    \"Xiangnan HE(́)\",\n",
            "    \"Yanyan LANa\",\n",
            "    \"Chenliang LIl\",\n",
            "    \"Yiqun LIUa\",\n",
            "    \"Ziyu LYUm\",\n",
            "    \"Weizhi MAa\",\n",
            "    \"Jun MAf\",\n",
            "    \"Zhaochun REN f\",\n",
            "    \"Pengjie REN f\",\n",
            "    \"Zhiqiang WANGn\",\n",
            "    \"Mingwen WANGo\",\n",
            "    \"Ji-Rong WENi\",\n",
            "    \"Le WUp\",\n",
            "    \"Xin XIN f\",\n",
            "    \"Jun XUi\",\n",
            "    \"Dawei YINq\",\n",
            "    \"Peng ZHANG(́)\",\n",
            "    \"Fan ZHANGl\",\n",
            "    \"Weinan ZHANGs\",\n",
            "    \"Min ZHANGa\",\n",
            "    \"Xiaofei ZHUt\",\n",
            "    \"Tsinghua University\",\n",
            "    \"Beijing University of Posts and Telecommunications\",\n",
            "    \"Huawei Technologies Ltd. Co\",\n",
            "    \"Jilin University\",\n",
            "    \"Zhejiang University\",\n",
            "    \"Shandong University\",\n",
            "    \"Shandong Artificial Intelligence Institute\",\n",
            "    \"South China University of Technology\",\n",
            "    \"Renmin University of China\",\n",
            "    \"University of Science and Technology of China\",\n",
            "    \"Institute of Computing Technology, Chinese Academy of Sciences\",\n",
            "    \"Wuhan University\",\n",
            "    \"Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\",\n",
            "    \"Shanxi University\",\n",
            "    \"Jiangxi Normal University\",\n",
            "    \"Hefei University of Technology\",\n",
            "    \"Baidu Inc.\",\n",
            "    \"Tianjin University\",\n",
            "    \"Shanghai Jiao Tong University\",\n",
            "    \"Chongqing University of Technology\"\n",
            "  ],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\n",
            "    \"Information Retrieval\",\n",
            "    \"Large Language Models\",\n",
            "    \"Text Understanding\",\n",
            "    \"Generative Retrieval\",\n",
            "    \"User Understanding\",\n",
            "    \"Model Evaluation\",\n",
            "    \"User-System Interaction\",\n",
            "    \"Technical Paradigm\"\n",
            "  ],\n",
            "  \"Abstract\": \"The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": null\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community\",\n",
            "    \"Authors\": [\n",
            "        \"Qingyao AIa\",\n",
            "        \"Ting BAIb\",\n",
            "        \"Zhao CAOc\",\n",
            "        \"Yi CHANGd\",\n",
            "        \"Jiawei CHEN(\\u0301)\",\n",
            "        \"Zhumin CHENf\",\n",
            "        \"Zhiyong CHENGg\",\n",
            "        \"Shoubin DONGh\",\n",
            "        \"Zhicheng DOUi\",\n",
            "        \"Fuli FENG j\",\n",
            "        \"Shen GAO f\",\n",
            "        \"Jiafeng GUOk\",\n",
            "        \"Xiangnan HE(\\u0301)\",\n",
            "        \"Yanyan LANa\",\n",
            "        \"Chenliang LIl\",\n",
            "        \"Yiqun LIUa\",\n",
            "        \"Ziyu LYUm\",\n",
            "        \"Weizhi MAa\",\n",
            "        \"Jun MAf\",\n",
            "        \"Zhaochun REN f\",\n",
            "        \"Pengjie REN f\",\n",
            "        \"Zhiqiang WANGn\",\n",
            "        \"Mingwen WANGo\",\n",
            "        \"Ji-Rong WENi\",\n",
            "        \"Le WUp\",\n",
            "        \"Xin XIN f\",\n",
            "        \"Jun XUi\",\n",
            "        \"Dawei YINq\",\n",
            "        \"Peng ZHANG(\\u0301)\",\n",
            "        \"Fan ZHANGl\",\n",
            "        \"Weinan ZHANGs\",\n",
            "        \"Min ZHANGa\",\n",
            "        \"Xiaofei ZHUt\",\n",
            "        \"Tsinghua University\",\n",
            "        \"Beijing University of Posts and Telecommunications\",\n",
            "        \"Huawei Technologies Ltd. Co\",\n",
            "        \"Jilin University\",\n",
            "        \"Zhejiang University\",\n",
            "        \"Shandong University\",\n",
            "        \"Shandong Artificial Intelligence Institute\",\n",
            "        \"South China University of Technology\",\n",
            "        \"Renmin University of China\",\n",
            "        \"University of Science and Technology of China\",\n",
            "        \"Institute of Computing Technology, Chinese Academy of Sciences\",\n",
            "        \"Wuhan University\",\n",
            "        \"Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\",\n",
            "        \"Shanxi University\",\n",
            "        \"Jiangxi Normal University\",\n",
            "        \"Hefei University of Technology\",\n",
            "        \"Baidu Inc.\",\n",
            "        \"Tianjin University\",\n",
            "        \"Shanghai Jiao Tong University\",\n",
            "        \"Chongqing University of Technology\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Information Retrieval\",\n",
            "        \"Large Language Models\",\n",
            "        \"Text Understanding\",\n",
            "        \"Generative Retrieval\",\n",
            "        \"User Understanding\",\n",
            "        \"Model Evaluation\",\n",
            "        \"User-System Interaction\",\n",
            "        \"Technical Paradigm\"\n",
            "    ],\n",
            "    \"Abstract\": \"The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/13.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK1Wr94WEhil",
        "outputId": "43de16ee-6717-4610-c47a-edef97fec379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Building an annotated corpus for automatic metadata extraction from multilingual journal article references\",\n",
            "  \"Authors\": [\"Wonjun Choi\", \"Hwa-Mook Yoon\", \"Mi-Hwan Hyun\", \"Hye-Jin Lee\", \"Jae-Wook Seol\", \"Kangsan Dajeong Lee\", \"Young Joon Yoon\", \"Hyesoo Kong\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"metadata extraction\", \"multilingual references\", \"automatic annotation\", \"BERT-based model\", \"GROBID\"],\n",
            "  \"Abstract\": \"Bibliographic references containing citation information of academic literature play an important role as a medium connecting earlier and recent studies. As references contain machine-readable metadata such as author name, title, or publication year, they have been widely used in the field of citation information services including search services for scholarly information and research trend analysis. Many institutions around the world manually extract and continuously accumulate reference metadata to provide various scholarly services. However, manually collection of reference metadata every year continues to be a burden because of the associated cost and time consumption. With the accumulation of a large volume of academic literature, several tools, including GROBID and CERMINE, that automatically extract reference metadata have been released. However, these tools have some limitations. For example, they are only applicable to references written in English, the types of extractable metadata are limited for each tool, and the performance of the tools is insufficient to replace the manual extraction of reference metadata. Therefore, in this study, we focused on constructing a high-quality corpus to automatically extract metadata from multilingual journal article references. Using our constructed corpus, we trained and evaluated a BERT-based transfer-learning model. Furthermore, we compared the performance of the BERT-based model with that of the existing model, GROBID.\",\n",
            "  \"Document Type\": \"Research Article\",\n",
            "  \"Number of References\": null\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Building an annotated corpus for automatic metadata extraction from multilingual journal article references\",\n",
            "    \"Authors\": [\n",
            "        \"Wonjun Choi\",\n",
            "        \"Hwa-Mook Yoon\",\n",
            "        \"Mi-Hwan Hyun\",\n",
            "        \"Hye-Jin Lee\",\n",
            "        \"Jae-Wook Seol\",\n",
            "        \"Kangsan Dajeong Lee\",\n",
            "        \"Young Joon Yoon\",\n",
            "        \"Hyesoo Kong\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"metadata extraction\",\n",
            "        \"multilingual references\",\n",
            "        \"automatic annotation\",\n",
            "        \"BERT-based model\",\n",
            "        \"GROBID\"\n",
            "    ],\n",
            "    \"Abstract\": \"Bibliographic references containing citation information of academic literature play an important role as a medium connecting earlier and recent studies. As references contain machine-readable metadata such as author name, title, or publication year, they have been widely used in the field of citation information services including search services for scholarly information and research trend analysis. Many institutions around the world manually extract and continuously accumulate reference metadata to provide various scholarly services. However, manually collection of reference metadata every year continues to be a burden because of the associated cost and time consumption. With the accumulation of a large volume of academic literature, several tools, including GROBID and CERMINE, that automatically extract reference metadata have been released. However, these tools have some limitations. For example, they are only applicable to references written in English, the types of extractable metadata are limited for each tool, and the performance of the tools is insufficient to replace the manual extraction of reference metadata. Therefore, in this study, we focused on constructing a high-quality corpus to automatically extract metadata from multilingual journal article references. Using our constructed corpus, we trained and evaluated a BERT-based transfer-learning model. Furthermore, we compared the performance of the BERT-based model with that of the existing model, GROBID.\",\n",
            "    \"Document Type\": \"Research Article\",\n",
            "    \"Number of References\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/14.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbWz4036EjHZ",
        "outputId": "1b8e9936-32d2-4646-c8c2-b7d7dcf4cfc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"LAME: Layout-Aware Metadata Extraction Approach for Research Articles\",\n",
            "  \"Authors\": [\"Jongyun Cho\", \"Hyesoo Kong\", \"Hwamook Yoon\", \"Heung-seon Oh\", \"Yuchul Jung\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Automatic layout analysis\", \"Layout-MetaBERT\", \"Metadata extraction\", \"Research article\"],\n",
            "  \"Abstract\": \"The volume of academic literature, such as academic conference papers and journals, has increased rapidly worldwide, and research on metadata extraction is ongoing. However, high-performing metadata extraction is still challenging due to diverse layout formats according to journal publishers. To accommodate the diversity of the layouts of academic journals, we propose a novel LAyout-aware Metadata Extraction (LAME) framework equipped with the three characteristics (e.g., design of an automatic layout analysis, construction of a large meta-data training set, and construction of Layout-MetaBERT). We designed an automatic layout analysis using PDFMiner. Based on the layout analysis, a large volume of metadata-separated training data, including the title, abstract, author name, author affiliated organization, and keywords, were automatically extracted. Moreover, we constructed Layout-MetaBERT to extract the metadata from academic journals with varying layout formats. The experimental results with Layout-MetaBERT exhibited robust performance (Macro-F1, 93.27%) in metadata extraction for unseen journals with different layout formats.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not provided\"\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"LAME: Layout-Aware Metadata Extraction Approach for Research Articles\",\n",
            "    \"Authors\": [\n",
            "        \"Jongyun Cho\",\n",
            "        \"Hyesoo Kong\",\n",
            "        \"Hwamook Yoon\",\n",
            "        \"Heung-seon Oh\",\n",
            "        \"Yuchul Jung\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Automatic layout analysis\",\n",
            "        \"Layout-MetaBERT\",\n",
            "        \"Metadata extraction\",\n",
            "        \"Research article\"\n",
            "    ],\n",
            "    \"Abstract\": \"The volume of academic literature, such as academic conference papers and journals, has increased rapidly worldwide, and research on metadata extraction is ongoing. However, high-performing metadata extraction is still challenging due to diverse layout formats according to journal publishers. To accommodate the diversity of the layouts of academic journals, we propose a novel LAyout-aware Metadata Extraction (LAME) framework equipped with the three characteristics (e.g., design of an automatic layout analysis, construction of a large meta-data training set, and construction of Layout-MetaBERT). We designed an automatic layout analysis using PDFMiner. Based on the layout analysis, a large volume of metadata-separated training data, including the title, abstract, author name, author affiliated organization, and keywords, were automatically extracted. Moreover, we constructed Layout-MetaBERT to extract the metadata from academic journals with varying layout formats. The experimental results with Layout-MetaBERT exhibited robust performance (Macro-F1, 93.27%) in metadata extraction for unseen journals with different layout formats.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not provided\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/15.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G1mckUkBLQ2a",
        "outputId": "a9db4b82-109e-4ce8-f14c-9acde382b962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Layout Aware Semantic Element Extraction for Sustainable Science & Technology Decision Support\",\n",
            "  \"Authors\": [\"Hyuntae Kim\", \"Jongyun Choi\", \"Soyoung Park\", \"Yuchul Jung\"],\n",
            "  \"DOI\": \"10.3390/su14052802\",\n",
            "  \"Keywords\": [\"Science & Technology\", \"Automated Text Mining\", \"Vision Recognition\", \"Knowledge Graph Construction\"],\n",
            "  \"Abstract\": \"New scientiﬁc and technological (S&T) knowledge is being introduced rapidly, and hence, analysis efforts to understand and analyze new published S&T documents are increasing daily. Automated text mining and vision recognition techniques alleviate the burden somewhat, but the various document layout formats and knowledge content granularities across the S&T ﬁeld make it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph construction framework that simultaneously extracts meta-information and useful image objects from S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME), which can accurately extract metadata from various layout formats, and implement a transformer-based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize the vision recognition.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Layout Aware Semantic Element Extraction for Sustainable Science & Technology Decision Support\",\n",
            "    \"Authors\": [\n",
            "        \"Hyuntae Kim\",\n",
            "        \"Jongyun Choi\",\n",
            "        \"Soyoung Park\",\n",
            "        \"Yuchul Jung\"\n",
            "    ],\n",
            "    \"DOI\": \"10.3390/su14052802\",\n",
            "    \"Keywords\": [\n",
            "        \"Science & Technology\",\n",
            "        \"Automated Text Mining\",\n",
            "        \"Vision Recognition\",\n",
            "        \"Knowledge Graph Construction\"\n",
            "    ],\n",
            "    \"Abstract\": \"New scienti\\ufb01c and technological (S&T) knowledge is being introduced rapidly, and hence, analysis efforts to understand and analyze new published S&T documents are increasing daily. Automated text mining and vision recognition techniques alleviate the burden somewhat, but the various document layout formats and knowledge content granularities across the S&T \\ufb01eld make it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph construction framework that simultaneously extracts meta-information and useful image objects from S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME), which can accurately extract metadata from various layout formats, and implement a transformer-based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize the vision recognition.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "from llama_cpp import Llama  # Make sure to install llama-cpp\n",
        "\n",
        "# Extract clean JSON from output\n",
        "def extract_json(text):\n",
        "    cleaned_output = re.sub(r'(?<!\\\\)\\n', ' ', text)\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"Failed to extract JSON: {str(e)}\"}\n",
        "\n",
        "# Build prompt suited for TinyLLaMA-style chat model\n",
        "def build_prompt(text):\n",
        "    instruction = \"\"\"\n",
        "You are an information extraction engine. Return ONLY valid JSON, no explanations.\n",
        "\n",
        "JSON Structure:\n",
        "{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}\n",
        "\n",
        "Extract metadata from the following scientific paper:\n",
        "\"\"\"\n",
        "    return f\"<|user|>\\n{instruction.strip()}\\n{text[:2000]}\\n<|assistant|>\"\n",
        "\n",
        "\n",
        "# Call model and extract structured data\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_tokens=1024,             # allow enough room for full JSON\n",
        "        temperature=0,               # deterministic output\n",
        "        top_p=1.0,\n",
        "        stop=[\"<|end|>\", \"</s>\"],    # prevent model from rambling\n",
        "    )\n",
        "\n",
        "\n",
        "    # print(response)\n",
        "    raw_output = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract raw text from PDF using PyMuPDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "# Load LLaMA 3 GGUF Model\n",
        "\n",
        "# Path to your GGUF model\n",
        "model_path = \"/content/models/phi3.5/Phi-3.5-mini-instruct.Q8_0.gguf\"\n",
        "# Load model\n",
        "model = load_phi_model(model_path)\n",
        "\n",
        "\n",
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/16.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oK95-F_4LTdF",
        "outputId": "2d0b6b87-d2d0-43fe-985c-558425566197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Convolutional Neural Networks for Sentence Classification\",\n",
            "  \"Authors\": [\"Yoon Kim\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Convolutional Neural Networks\", \"Sentence Classification\", \"Word Vectors\", \"Fine-tuning\"],\n",
            "  \"Abstract\": \"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": null\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Convolutional Neural Networks for Sentence Classification\",\n",
            "    \"Authors\": [\n",
            "        \"Yoon Kim\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Convolutional Neural Networks\",\n",
            "        \"Sentence Classification\",\n",
            "        \"Word Vectors\",\n",
            "        \"Fine-tuning\"\n",
            "    ],\n",
            "    \"Abstract\": \"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/17.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lXIuAlQOLVHR",
        "outputId": "7a746e4c-2753-4ba8-aeb5-dc98d96435ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Employing Conditional Random Fields for Information Extraction in Research Papers\",\n",
            "  \"Authors\": [\"Fuchun Peng\", \"Andrew McCallum\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Information extraction\", \"Constraint information extraction\", \"Conditional random fields\", \"Regularization\"],\n",
            "  \"Abstract\": \"With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This article employs conditional random fields (CRFs) for the task of extracting various common fields from the headers and citations of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features, and global layout features. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace, and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint information extraction; i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system significantly improves extraction performance, with an error rate reduction of 6–14%.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Employing Conditional Random Fields for Information Extraction in Research Papers\",\n",
            "    \"Authors\": [\n",
            "        \"Fuchun Peng\",\n",
            "        \"Andrew McCallum\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Information extraction\",\n",
            "        \"Constraint information extraction\",\n",
            "        \"Conditional random fields\",\n",
            "        \"Regularization\"\n",
            "    ],\n",
            "    \"Abstract\": \"With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This article employs conditional random fields (CRFs) for the task of extracting various common fields from the headers and citations of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features, and global layout features. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace, and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint information extraction; i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system significantly improves extraction performance, with an error rate reduction of 6\\u201314%.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/18.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0iv_wS3LW5b",
        "outputId": "0ba62257-9720-4d68-820b-a81ad7f2d575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Information extraction from scientific articles: a survey\",\n",
            "  \"Authors\": [\"Zara Nasar\", \"Syed Waqar Jaffry\", \"Muhammad Kamran Malik\"],\n",
            "  \"DOI\": \"https://doi.org/10.1007/978-3-030-20654-5_1\",\n",
            "  \"Keywords\": [\"Information extraction\", \"Scientific articles\", \"World Wide Web\", \"Automatic analysis\", \"Metadata\", \"Key-insights\", \"Benchmark datasets\", \"Computational approaches\", \"Rule-based approaches\", \"Hidden Markov Models\", \"Conditional Random Fields\", \"Support Vector Machines\", \"Naive-Bayes classification\", \"Deep Learning\"],\n",
            "  \"Abstract\": \"In last few decades, with the advent of World Wide Web (WWW), world is being over-loaded with huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and efﬁcient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scientiﬁc literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scientiﬁc articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scientiﬁc articles. The information insights extracted from scientiﬁc articles are classiﬁed in two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a signiﬁcant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Naı¨ve-Bayes classiﬁcation and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to speciﬁc information needs from scientiﬁc articles. Hence, in this study, state-of-the-art developments in automatic information extraction from scientific articles are discussed.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Information extraction from scientific articles: a survey\",\n",
            "    \"Authors\": [\n",
            "        \"Zara Nasar\",\n",
            "        \"Syed Waqar Jaffry\",\n",
            "        \"Muhammad Kamran Malik\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1007/978-3-030-20654-5_1\",\n",
            "    \"Keywords\": [\n",
            "        \"Information extraction\",\n",
            "        \"Scientific articles\",\n",
            "        \"World Wide Web\",\n",
            "        \"Automatic analysis\",\n",
            "        \"Metadata\",\n",
            "        \"Key-insights\",\n",
            "        \"Benchmark datasets\",\n",
            "        \"Computational approaches\",\n",
            "        \"Rule-based approaches\",\n",
            "        \"Hidden Markov Models\",\n",
            "        \"Conditional Random Fields\",\n",
            "        \"Support Vector Machines\",\n",
            "        \"Naive-Bayes classification\",\n",
            "        \"Deep Learning\"\n",
            "    ],\n",
            "    \"Abstract\": \"In last few decades, with the advent of World Wide Web (WWW), world is being over-loaded with huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and ef\\ufb01cient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scienti\\ufb01c literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scienti\\ufb01c articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scienti\\ufb01c articles. The information insights extracted from scienti\\ufb01c articles are classi\\ufb01ed in two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a signi\\ufb01cant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Na\\u0131\\u00a8ve-Bayes classi\\ufb01cation and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to speci\\ufb01c information needs from scienti\\ufb01c articles. Hence, in this study, state-of-the-art developments in automatic information extraction from scientific articles are discussed.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "from llama_cpp import Llama  # Make sure to install llama-cpp\n",
        "\n",
        "# Extract clean JSON from output\n",
        "def extract_json(text):\n",
        "    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group()\n",
        "        try:\n",
        "            metadata = json.loads(json_str)\n",
        "            return metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"Error\": f\"Failed to extract JSON: {str(e)}\"}\n",
        "    else:\n",
        "        return {\"Error\": \"No valid JSON object found in the text.\"}\n",
        "\n",
        "\n",
        "\n",
        "    # text = text.strip()\n",
        "    # text = re.sub(r'(?<!\\\\)\\n', ' ', text)\n",
        "    # text = re.sub(r'(?i)^json\\s*', '', text.strip())             # Remove \"JSON:\" or \"json\"\n",
        "    # text = re.sub(r'```json', '', text, flags=re.IGNORECASE)     # Remove ```json\n",
        "    # text = re.sub(r'```', '', text)\n",
        "    # text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
        "    # try:\n",
        "    #     return json.loads(text)\n",
        "    # except Exception as e:\n",
        "    #     return {\"Error\": f\"Failed to extract JSON: {str(e)}\"}\n",
        "\n",
        "# Build prompt suited for TinyLLaMA-style chat model\n",
        "# def build_prompt(text):\n",
        "#     instruction = \"\"\"\n",
        "# You are an information extraction engine. Return ONLY valid JSON, no explanations.\n",
        "\n",
        "# JSON Structure:\n",
        "# {\n",
        "#   \"Title\": \"Paper title\",\n",
        "#   \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "#   \"DOI\": \"DOI if available\",\n",
        "#   \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "#   \"Abstract\": \"Abstract text\",\n",
        "#   \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "#   \"Number of References\": 10\n",
        "# }\n",
        "\n",
        "# Extract metadata from the following scientific paper:\n",
        "# \"\"\"\n",
        "#     return f\"<|user|>\\n{instruction.strip()}\\n{text[:2000]}\\n<|assistant|>\"\n",
        "\n",
        "def build_prompt(text):\n",
        "    return f\"\"\"Extract the following metadata from the paper below and output ONLY valid JSON:\n",
        "\n",
        "Title\n",
        "Authors\n",
        "DOI\n",
        "Keywords\n",
        "Abstract\n",
        "Document Type\n",
        "Number of References\n",
        "\n",
        "Paper content:\n",
        "{text[:2000]}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Call model and extract structured data\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_tokens=1024,             # allow enough room for full JSON\n",
        "        temperature=0,               # deterministic output\n",
        "        top_p=0.9,\n",
        "        stop=[\"<|end|>\", \"</s>\"],    # prevent model from rambling\n",
        "    )\n",
        "\n",
        "\n",
        "    # print(response)\n",
        "    raw_output = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract raw text from PDF using PyMuPDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "# Load LLaMA 3 GGUF Model\n",
        "\n",
        "# Path to your GGUF model\n",
        "model_path = \"/content/models/phi3.5/Phi-3.5-mini-instruct.Q8_0.gguf\"\n",
        "# Load model\n",
        "model = load_phi_model(model_path)\n",
        "\n",
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/19.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKP_IYdBmMOe",
        "outputId": "af21e1bf-1657-4b91-e724-60acc206ce87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "\n",
            "JSON:\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Predicting movies’ eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features\",\n",
            "  \"Authors\": [\n",
            "    \"Elham Motamedi\",\n",
            "    \"Danial Khosh Kholgh\",\n",
            "    \"Sorush Saghari\",\n",
            "    \"Mehdi Elahi\",\n",
            "    \"Francesco Barile\",\n",
            "    \"Marko Tkalcic\"\n",
            "  ],\n",
            "  \"DOI\": \"10.1016/j.infpro.2023.103610\",\n",
            "  \"Keywords\": [\n",
            "    \"Eudaimonia\",\n",
            "    \"Hedonia\",\n",
            "    \"Machine learning approach\",\n",
            "    \"Movie recommender systems\"\n",
            "  ],\n",
            "  \"Abstract\": \"In the task of modeling user preferences for movie recommender systems, recent research has demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and H scores), which reflect the depth of their message and the level of fun experience they provide, respectively. So far, the labeling of movies with their E and H scores has been done manually using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue, we propose an automatic approach for predicting E and H scores. Specifically, we collected E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this dataset with metadata, audio, and low-level and high-level visual features, and trained machine learning models for predicting the E and H scores of movies. This study investigates the use of machine learning models in predicting the E and H scores of movies using various feature sets, including audio, low-level and high-level visual features, and metadata.\",\n",
            "  \"Document Type\": \"Information Processing and Management\",\n",
            "  \"Number of References\": \"Not specified\"\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Predicting movies\\u2019 eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features\",\n",
            "    \"Authors\": [\n",
            "        \"Elham Motamedi\",\n",
            "        \"Danial Khosh Kholgh\",\n",
            "        \"Sorush Saghari\",\n",
            "        \"Mehdi Elahi\",\n",
            "        \"Francesco Barile\",\n",
            "        \"Marko Tkalcic\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1016/j.infpro.2023.103610\",\n",
            "    \"Keywords\": [\n",
            "        \"Eudaimonia\",\n",
            "        \"Hedonia\",\n",
            "        \"Machine learning approach\",\n",
            "        \"Movie recommender systems\"\n",
            "    ],\n",
            "    \"Abstract\": \"In the task of modeling user preferences for movie recommender systems, recent research has demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and H scores), which reflect the depth of their message and the level of fun experience they provide, respectively. So far, the labeling of movies with their E and H scores has been done manually using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue, we propose an automatic approach for predicting E and H scores. Specifically, we collected E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this dataset with metadata, audio, and low-level and high-level visual features, and trained machine learning models for predicting the E and H scores of movies. This study investigates the use of machine learning models in predicting the E and H scores of movies using various feature sets, including audio, low-level and high-level visual features, and metadata.\",\n",
            "    \"Document Type\": \"Information Processing and Management\",\n",
            "    \"Number of References\": \"Not specified\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "    print(\"Model loading failed. Exiting.\")\n",
        "    exit()  # Or handle the error appropriately\n",
        "else:\n",
        "# Extract text from PDF\n",
        "  extracted_text = extract_text_from_pdf(\"/content/20.pdf\")\n",
        "\n",
        "  if extracted_text.startswith(\"Error:\"):\n",
        "      print(\"No extractable text found in the PDF.\")\n",
        "  else:\n",
        "    # Extract metadata using the LLaMA 3 model\n",
        "      metadata = extract_metadata(model, extracted_text)\n",
        "      print(\"\\n==== Extracted Metadata ====\")\n",
        "      print(json.dumps(metadata, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s4Qq3ncmMzu",
        "outputId": "6900b84c-123e-4a2c-9c83-ed85f1abe18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            " {\n",
            "  \"Title\": \"Deep Learning-based Extraction of Algorithmic Metadata in Full-Text Scholarly Documents\",\n",
            "  \"Authors\": [\"Iqra Safdera\", \"Saeed-Ul Hassana\", \"Anna Visvizib\", \"Thanapon Norasetc\", \"Raheel Nawazd\", \"Suppawong Tuarobc\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Knowledge-based Systems\", \"Algorithmic Metadata\", \"Algorithm Search\", \"Deep Learning\", \"Bi-Directional LSTM\", \"Information Retrieval\", \"Full-text Articles\"],\n",
            "  \"Abstract\": \"The advancements of search engines for traditional text documents have enabled the effective retrieval of massive textual information in a resource-efficient manner. However, such conventional search methodologies often suffer from poor retrieval accuracy especially when documents exhibit unique properties that behoove specialized and deeper semantic extraction. Recently, AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and shallow textual metadata from scientific publications and treats them as traditional documents so that the conventional search engine methodology could be applied. However, such a system fails to facilitate user search queries that seek to identify algorithm-specific information, such as the datasets on which algorithms operate, the performance of algorithms, and runtime complexity, etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are presented. Specifically, we propose a set of methods to automatically identify and extract algorithmic pseudo-codes and the sentential context surrounding them. Furthermore, we introduce a novel deep learning architecture that leverages Bi-Directional Long Short-Term Memory (Bi-LSTM) networks to enhance the extraction process. Our approach not only improves the precision of algorithmic metadata extraction but also significantly reduces the computational overhead, making it suitable for large-scale applications.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": \"Not provided\"\n",
            "}\n",
            "\n",
            "\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Deep Learning-based Extraction of Algorithmic Metadata in Full-Text Scholarly Documents\",\n",
            "    \"Authors\": [\n",
            "        \"Iqra Safdera\",\n",
            "        \"Saeed-Ul Hassana\",\n",
            "        \"Anna Visvizib\",\n",
            "        \"Thanapon Norasetc\",\n",
            "        \"Raheel Nawazd\",\n",
            "        \"Suppawong Tuarobc\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Knowledge-based Systems\",\n",
            "        \"Algorithmic Metadata\",\n",
            "        \"Algorithm Search\",\n",
            "        \"Deep Learning\",\n",
            "        \"Bi-Directional LSTM\",\n",
            "        \"Information Retrieval\",\n",
            "        \"Full-text Articles\"\n",
            "    ],\n",
            "    \"Abstract\": \"The advancements of search engines for traditional text documents have enabled the effective retrieval of massive textual information in a resource-efficient manner. However, such conventional search methodologies often suffer from poor retrieval accuracy especially when documents exhibit unique properties that behoove specialized and deeper semantic extraction. Recently, AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and shallow textual metadata from scientific publications and treats them as traditional documents so that the conventional search engine methodology could be applied. However, such a system fails to facilitate user search queries that seek to identify algorithm-specific information, such as the datasets on which algorithms operate, the performance of algorithms, and runtime complexity, etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are presented. Specifically, we propose a set of methods to automatically identify and extract algorithmic pseudo-codes and the sentential context surrounding them. Furthermore, we introduce a novel deep learning architecture that leverages Bi-Directional Long Short-Term Memory (Bi-LSTM) networks to enhance the extraction process. Our approach not only improves the precision of algorithmic metadata extraction but also significantly reduces the computational overhead, making it suitable for large-scale applications.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": \"Not provided\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}
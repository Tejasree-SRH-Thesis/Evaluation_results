{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30991a9b02e54560a8012aded2b778ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fe1efecc4a444d99490cb1ec6fb92e9",
              "IPY_MODEL_01bcb4a91e044c91aa1f0412737bbcd8",
              "IPY_MODEL_40199096b08b446bb95e8b53107ce5d5"
            ],
            "layout": "IPY_MODEL_fcd24ae8ed1d47f5987d56f8b770195b"
          }
        },
        "6fe1efecc4a444d99490cb1ec6fb92e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4af77dfbb4774ef6ab9346770dd3a7de",
            "placeholder": "​",
            "style": "IPY_MODEL_4a45bd8c6057457c8a48267e1abaddbe",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "01bcb4a91e044c91aa1f0412737bbcd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63b2d9dd46f40b497dc1e49d809ffc8",
            "max": 7305,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3bb022a26ef4093b3539bb6de59c0b4",
            "value": 7305
          }
        },
        "40199096b08b446bb95e8b53107ce5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c725d67a7c6406880982180b2b967bc",
            "placeholder": "​",
            "style": "IPY_MODEL_26892973781843b785181dd0a60d5318",
            "value": " 7.30k/7.30k [00:00&lt;00:00, 183kB/s]"
          }
        },
        "fcd24ae8ed1d47f5987d56f8b770195b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4af77dfbb4774ef6ab9346770dd3a7de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a45bd8c6057457c8a48267e1abaddbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d63b2d9dd46f40b497dc1e49d809ffc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3bb022a26ef4093b3539bb6de59c0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c725d67a7c6406880982180b2b967bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26892973781843b785181dd0a60d5318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cef33709391d43c5b376d4389302043e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5754c14c73844e1e9ead503db9880fa3",
              "IPY_MODEL_a8148ff1bcbe4631a73bc479354af4d9",
              "IPY_MODEL_a7fd781178124db484901f697f727c1c"
            ],
            "layout": "IPY_MODEL_a8daa8865c994093a4d3a474166a16d5"
          }
        },
        "5754c14c73844e1e9ead503db9880fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a023cc5d290d4d3eb640713ca70dd12b",
            "placeholder": "​",
            "style": "IPY_MODEL_dcfd237366d64ee9a5cae1719363092c",
            "value": "vocab.json: 100%"
          }
        },
        "a8148ff1bcbe4631a73bc479354af4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07c20e8271f445cabf32a191bbe3cc95",
            "max": 2776833,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7b64f35cd824457a10545fe5afad2fb",
            "value": 2776833
          }
        },
        "a7fd781178124db484901f697f727c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05166c5dba26458cb4bf4ab5a03f4c3d",
            "placeholder": "​",
            "style": "IPY_MODEL_0383a9c3daa046fa9c49bdd456d7deb3",
            "value": " 2.78M/2.78M [00:00&lt;00:00, 13.6MB/s]"
          }
        },
        "a8daa8865c994093a4d3a474166a16d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a023cc5d290d4d3eb640713ca70dd12b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfd237366d64ee9a5cae1719363092c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07c20e8271f445cabf32a191bbe3cc95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b64f35cd824457a10545fe5afad2fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05166c5dba26458cb4bf4ab5a03f4c3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0383a9c3daa046fa9c49bdd456d7deb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c0330d80e5546ebb42f3e4b31e96ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fed92b630f6456db75361305481e7cf",
              "IPY_MODEL_8b4c1f283571473296ca275e0b0b36f0",
              "IPY_MODEL_2189e28779f542bb91ddd0dd08f15284"
            ],
            "layout": "IPY_MODEL_b4aabf825a564be4a43e2f737b1cbf9f"
          }
        },
        "4fed92b630f6456db75361305481e7cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21d8124838fb47c8832a19015f1ba8c1",
            "placeholder": "​",
            "style": "IPY_MODEL_be15c7420e524bcb904ffa87ff710f65",
            "value": "merges.txt: 100%"
          }
        },
        "8b4c1f283571473296ca275e0b0b36f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f06f3204368a4e9888bf8131d8d47f60",
            "max": 1671839,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5212f0d8a79a449f9afed87222848bdf",
            "value": 1671839
          }
        },
        "2189e28779f542bb91ddd0dd08f15284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a3e8f7f5fda4a1798267624f6fd02f1",
            "placeholder": "​",
            "style": "IPY_MODEL_654c69a405524f839d79bb13766502e1",
            "value": " 1.67M/1.67M [00:00&lt;00:00, 20.3MB/s]"
          }
        },
        "b4aabf825a564be4a43e2f737b1cbf9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21d8124838fb47c8832a19015f1ba8c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be15c7420e524bcb904ffa87ff710f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f06f3204368a4e9888bf8131d8d47f60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5212f0d8a79a449f9afed87222848bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a3e8f7f5fda4a1798267624f6fd02f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "654c69a405524f839d79bb13766502e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e603f3a020b84ef78d1c14e3f7cd23f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_465a13d5434d48c990bfe90416a12ec2",
              "IPY_MODEL_735eff4fa65e4091bb2115abc59510cd",
              "IPY_MODEL_f27e1b725131442ea835800c3077c643"
            ],
            "layout": "IPY_MODEL_7b97b723574b45338cf48dd2d52a6aec"
          }
        },
        "465a13d5434d48c990bfe90416a12ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a66d1c436a741b184c1e7c13a8a643e",
            "placeholder": "​",
            "style": "IPY_MODEL_26618a9bd62c4858970a97aabfa966af",
            "value": "tokenizer.json: 100%"
          }
        },
        "735eff4fa65e4091bb2115abc59510cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55999e4affc549cca7929905d93e4917",
            "max": 7031645,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed7ebb0e1c384f6780f56f3724f92773",
            "value": 7031645
          }
        },
        "f27e1b725131442ea835800c3077c643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5269bebdcd04e6cadaad29d0cf9c294",
            "placeholder": "​",
            "style": "IPY_MODEL_011645cfe4a84c3f8cf838c908c2b7fb",
            "value": " 7.03M/7.03M [00:00&lt;00:00, 41.5MB/s]"
          }
        },
        "7b97b723574b45338cf48dd2d52a6aec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a66d1c436a741b184c1e7c13a8a643e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26618a9bd62c4858970a97aabfa966af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55999e4affc549cca7929905d93e4917": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed7ebb0e1c384f6780f56f3724f92773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5269bebdcd04e6cadaad29d0cf9c294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "011645cfe4a84c3f8cf838c908c2b7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "231572b94fda4c86ac6e1eb9aa15fa2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0affc10c32d41169754f1e3ccd0803a",
              "IPY_MODEL_ae815f3fc5fa4070b22e7cde81c3c118",
              "IPY_MODEL_e6535a0287ff4154bfe89e5362296210"
            ],
            "layout": "IPY_MODEL_de1740ae40c341ebb379ed8959403f59"
          }
        },
        "d0affc10c32d41169754f1e3ccd0803a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b928722953e432e8e06c61efeba3615",
            "placeholder": "​",
            "style": "IPY_MODEL_7979c2ff186f41fbb4e4b667591ed4ce",
            "value": "config.json: 100%"
          }
        },
        "ae815f3fc5fa4070b22e7cde81c3c118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d29ae38ce84b48d9937f4ffb584c2bc9",
            "max": 660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2de49cd6bd514e9f88a0c950c270e4d1",
            "value": 660
          }
        },
        "e6535a0287ff4154bfe89e5362296210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd373e91966f414f87b20524f81ae6ec",
            "placeholder": "​",
            "style": "IPY_MODEL_158a990d8ea241eea36921805b6e8240",
            "value": " 660/660 [00:00&lt;00:00, 9.87kB/s]"
          }
        },
        "de1740ae40c341ebb379ed8959403f59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b928722953e432e8e06c61efeba3615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7979c2ff186f41fbb4e4b667591ed4ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d29ae38ce84b48d9937f4ffb584c2bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2de49cd6bd514e9f88a0c950c270e4d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd373e91966f414f87b20524f81ae6ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "158a990d8ea241eea36921805b6e8240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f4baada096144bda5a5bf05e453488d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c02714710a044fda962f7f329081469d",
              "IPY_MODEL_38c6f432cba64888827687ef1d27e885",
              "IPY_MODEL_d9d112a3c59e472ca89429082224f56e"
            ],
            "layout": "IPY_MODEL_ba752ad3e33b4f4bb106fc7147660766"
          }
        },
        "c02714710a044fda962f7f329081469d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a74d0078a504ea09dae9348170cd6b4",
            "placeholder": "​",
            "style": "IPY_MODEL_669ba7a3157f401781387c46c1427e92",
            "value": "model.safetensors: 100%"
          }
        },
        "38c6f432cba64888827687ef1d27e885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c1f6e4099c2447987615147fa5ad623",
            "max": 3087467144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3f9e575e2024f718ddc2ae8fb085673",
            "value": 3087467144
          }
        },
        "d9d112a3c59e472ca89429082224f56e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1cbfae331004fc1baaa1965a6a932d5",
            "placeholder": "​",
            "style": "IPY_MODEL_cfd40d0bc6944fee84d63c194ae178fa",
            "value": " 3.09G/3.09G [00:38&lt;00:00, 114MB/s]"
          }
        },
        "ba752ad3e33b4f4bb106fc7147660766": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a74d0078a504ea09dae9348170cd6b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "669ba7a3157f401781387c46c1427e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c1f6e4099c2447987615147fa5ad623": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3f9e575e2024f718ddc2ae8fb085673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1cbfae331004fc1baaa1965a6a932d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfd40d0bc6944fee84d63c194ae178fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4a22e16c3344b65928d44e530308e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04f859a1933b4456934471b35db5f038",
              "IPY_MODEL_d70f07835d524ac28f1380eabb2e3abd",
              "IPY_MODEL_796f00eca92545248db6458e83c5d8dd"
            ],
            "layout": "IPY_MODEL_4de9c412593f4dc5b25e946a6c939a01"
          }
        },
        "04f859a1933b4456934471b35db5f038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf2577d23165423c8e9d1fcbb33465f8",
            "placeholder": "​",
            "style": "IPY_MODEL_ab50f96bbf24445e92a21b98a22c61e3",
            "value": "generation_config.json: 100%"
          }
        },
        "d70f07835d524ac28f1380eabb2e3abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_287c7715a5d54fb5b223bb008d771846",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_216c3d9b91bf459e81904e673996f411",
            "value": 242
          }
        },
        "796f00eca92545248db6458e83c5d8dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f479c629e6f4925837e75c9aa329f81",
            "placeholder": "​",
            "style": "IPY_MODEL_410b74232457497dab46b5ee9f8ad55a",
            "value": " 242/242 [00:00&lt;00:00, 21.4kB/s]"
          }
        },
        "4de9c412593f4dc5b25e946a6c939a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf2577d23165423c8e9d1fcbb33465f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab50f96bbf24445e92a21b98a22c61e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "287c7715a5d54fb5b223bb008d771846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "216c3d9b91bf459e81904e673996f411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f479c629e6f4925837e75c9aa329f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "410b74232457497dab46b5ee9f8ad55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip -q install fitz\n",
        "!pip -q install transformers\n",
        "!pip -q install torch"
      ],
      "metadata": {
        "id": "hmqiLHUP0s6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4708b6ff-a2c1-452f-86fd-1525c1462813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken einops accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITJW1dpV1Iex",
        "outputId": "0364c41f-063d-4786-a21b-b7c374fed786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjByUXZS2UhA",
        "outputId": "70ed77d6-7feb-4431-f24e-a19f3fcbafef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogT15EAb0JWi"
      },
      "outputs": [],
      "source": [
        "# import streamlit as st\n",
        "import tempfile\n",
        "import fitz  # PyMuPDF for PDF text extraction\n",
        "import json  # For handling JSON output\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,BitsAndBytesConfig\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_NAME = \"natong19/Qwen2-7B-Instruct-abliterated\"  # Ensure you have the correct model\n",
        "MODEL_NAME= \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "hf_token = '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'"
      ],
      "metadata": {
        "id": "jQzoSCx91PSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # Change to load_in_4bit=True for even lower memory\n",
        ")"
      ],
      "metadata": {
        "id": "b1kPYZW-o_aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,token=hf_token, trust_remote_code=True)\n",
        "    model_config = transformers.AutoConfig.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    token=hf_token\n",
        ")\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,config=model_config,device_map=\"auto\",token=hf_token, torch_dtype=torch.float32, trust_remote_code=True)\n",
        "    # generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,eos_token_id=tokenizer.eos_token_id,\n",
        "    #     pad_token_id=tokenizer.eos_token_id)\n",
        "    generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_new_tokens=1000,\n",
        ")\n",
        "    return generator\n",
        "\n",
        "generator = load_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276,
          "referenced_widgets": [
            "30991a9b02e54560a8012aded2b778ad",
            "6fe1efecc4a444d99490cb1ec6fb92e9",
            "01bcb4a91e044c91aa1f0412737bbcd8",
            "40199096b08b446bb95e8b53107ce5d5",
            "fcd24ae8ed1d47f5987d56f8b770195b",
            "4af77dfbb4774ef6ab9346770dd3a7de",
            "4a45bd8c6057457c8a48267e1abaddbe",
            "d63b2d9dd46f40b497dc1e49d809ffc8",
            "c3bb022a26ef4093b3539bb6de59c0b4",
            "6c725d67a7c6406880982180b2b967bc",
            "26892973781843b785181dd0a60d5318",
            "cef33709391d43c5b376d4389302043e",
            "5754c14c73844e1e9ead503db9880fa3",
            "a8148ff1bcbe4631a73bc479354af4d9",
            "a7fd781178124db484901f697f727c1c",
            "a8daa8865c994093a4d3a474166a16d5",
            "a023cc5d290d4d3eb640713ca70dd12b",
            "dcfd237366d64ee9a5cae1719363092c",
            "07c20e8271f445cabf32a191bbe3cc95",
            "a7b64f35cd824457a10545fe5afad2fb",
            "05166c5dba26458cb4bf4ab5a03f4c3d",
            "0383a9c3daa046fa9c49bdd456d7deb3",
            "2c0330d80e5546ebb42f3e4b31e96ee1",
            "4fed92b630f6456db75361305481e7cf",
            "8b4c1f283571473296ca275e0b0b36f0",
            "2189e28779f542bb91ddd0dd08f15284",
            "b4aabf825a564be4a43e2f737b1cbf9f",
            "21d8124838fb47c8832a19015f1ba8c1",
            "be15c7420e524bcb904ffa87ff710f65",
            "f06f3204368a4e9888bf8131d8d47f60",
            "5212f0d8a79a449f9afed87222848bdf",
            "5a3e8f7f5fda4a1798267624f6fd02f1",
            "654c69a405524f839d79bb13766502e1",
            "e603f3a020b84ef78d1c14e3f7cd23f9",
            "465a13d5434d48c990bfe90416a12ec2",
            "735eff4fa65e4091bb2115abc59510cd",
            "f27e1b725131442ea835800c3077c643",
            "7b97b723574b45338cf48dd2d52a6aec",
            "4a66d1c436a741b184c1e7c13a8a643e",
            "26618a9bd62c4858970a97aabfa966af",
            "55999e4affc549cca7929905d93e4917",
            "ed7ebb0e1c384f6780f56f3724f92773",
            "f5269bebdcd04e6cadaad29d0cf9c294",
            "011645cfe4a84c3f8cf838c908c2b7fb",
            "231572b94fda4c86ac6e1eb9aa15fa2f",
            "d0affc10c32d41169754f1e3ccd0803a",
            "ae815f3fc5fa4070b22e7cde81c3c118",
            "e6535a0287ff4154bfe89e5362296210",
            "de1740ae40c341ebb379ed8959403f59",
            "0b928722953e432e8e06c61efeba3615",
            "7979c2ff186f41fbb4e4b667591ed4ce",
            "d29ae38ce84b48d9937f4ffb584c2bc9",
            "2de49cd6bd514e9f88a0c950c270e4d1",
            "dd373e91966f414f87b20524f81ae6ec",
            "158a990d8ea241eea36921805b6e8240",
            "5f4baada096144bda5a5bf05e453488d",
            "c02714710a044fda962f7f329081469d",
            "38c6f432cba64888827687ef1d27e885",
            "d9d112a3c59e472ca89429082224f56e",
            "ba752ad3e33b4f4bb106fc7147660766",
            "2a74d0078a504ea09dae9348170cd6b4",
            "669ba7a3157f401781387c46c1427e92",
            "3c1f6e4099c2447987615147fa5ad623",
            "c3f9e575e2024f718ddc2ae8fb085673",
            "b1cbfae331004fc1baaa1965a6a932d5",
            "cfd40d0bc6944fee84d63c194ae178fa",
            "e4a22e16c3344b65928d44e530308e05",
            "04f859a1933b4456934471b35db5f038",
            "d70f07835d524ac28f1380eabb2e3abd",
            "796f00eca92545248db6458e83c5d8dd",
            "4de9c412593f4dc5b25e946a6c939a01",
            "bf2577d23165423c8e9d1fcbb33465f8",
            "ab50f96bbf24445e92a21b98a22c61e3",
            "287c7715a5d54fb5b223bb008d771846",
            "216c3d9b91bf459e81904e673996f411",
            "7f479c629e6f4925837e75c9aa329f81",
            "410b74232457497dab46b5ee9f8ad55a"
          ]
        },
        "id": "h0kZepKq1SMo",
        "outputId": "4859b601-0366-40f5-bc88-68030415625e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30991a9b02e54560a8012aded2b778ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cef33709391d43c5b376d4389302043e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c0330d80e5546ebb42f3e4b31e96ee1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e603f3a020b84ef78d1c14e3f7cd23f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "231572b94fda4c86ac6e1eb9aa15fa2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f4baada096144bda5a5bf05e453488d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4a22e16c3344b65928d44e530308e05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\""
      ],
      "metadata": {
        "id": "VbKHcXLq1VNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_metadata_1(text):\n",
        "    \"\"\"Uses the locally loaded model to extract metadata from PDF text.\"\"\"\n",
        "    prompt = \"\"\"\n",
        "You are an expert at extracting structured metadata from research papers.\n",
        "\n",
        "Analyze the following text and output metadata **strictly in JSON format** with the following fields:\n",
        "- \"Title\": The title of the paper.\n",
        "- \"Authors\": A list of author names.\n",
        "- \"DOI\": The Digital Object Identifier (if available).\n",
        "- \"Keywords\": A list of keywords.\n",
        "- \"Abstract\": The abstract of the paper.\n",
        "- \"Document Type\": The type of document (e.g., Research Paper, Thesis).\n",
        "- \"Number of References\": The total number of references in the paper.\n",
        "\n",
        "**Return output as a valid JSON object. Do not include any extra text, explanations, or markdown formatting.**\n",
        "\"\"\"\n",
        "\n",
        "    input_text = prompt + \"\\n\" + text[:2048]  # Limiting input size to avoid excessive memory usage\n",
        "\n",
        "    try:\n",
        "        response = generator(input_text, max_new_tokens=400, do_sample=True, temperature=0.2,truncation=True)\n",
        "        metadata_text = response[0][\"generated_text\"]\n",
        "        print(\"extracted metadata here\",metadata_text,\"END HERE\")\n",
        "\n",
        "        # Extract only JSON output using regex\n",
        "        json_match = re.search(r\"\\{.*\\}\", metadata_text, re.DOTALL)\n",
        "        if json_match:\n",
        "            extracted_json = json_match.group()\n",
        "            return json.loads(extracted_json)  # Convert string to JSON safely\n",
        "        else:\n",
        "            return {\"Error\": \"Failed to extract structured JSON. Raw output: \" + metadata_text}\n",
        "    except Exception as e:\n",
        "        return {\"Error\": str(e)}\n",
        "\n"
      ],
      "metadata": {
        "id": "roPe1jE0wWmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.5B\n",
        "extracted_text = extract_text_from_pdf('/content/1804.02445v2.pdf')\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        # Extract metadata using the local model\n",
        "     metadata = extract_metadata_1(extracted_text)\n",
        "     print(\"printing metadata here again\",metadata)"
      ],
      "metadata": {
        "id": "cnQtjexm5mFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048fb2ef-6afa-490b-f43e-24da8dca83d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "printing metadata here again {'Title': 'Extracting Scientific Figures with Distantly Supervised Neural Networks', 'Authors': [{'Name': 'Noah Siegel'}, {'Name': 'Nicholas Lourie'}, {'Name': 'Russell Power'}, {'Name': 'Waleed Ammar'}], 'DOI': '', 'Keywords': ['Figure Extraction', 'Distant Supervision', 'Deep Learning', 'Neural Networks', 'Computer Vision'], 'Abstract': 'Non-textual components such as charts, diagrams and tables provide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels—4,000 times larger than the previous largest figure extraction dataset—with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar, a large-scale academic search engine, and used to extract figures in 13 million scientific documents.', 'Document Type': 'Research Paper', 'Number of References': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(json.dumps(metadata, indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W-fZJbjE1h6",
        "outputId": "e73fa7a3-ed7c-4115-8f03-8631cf46732e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Title\": \"Extracting Scientific Figures with Distantly Supervised Neural Networks\",\n",
            "    \"Authors\": [\n",
            "        {\n",
            "            \"Name\": \"Noah Siegel\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"Nicholas Lourie\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"Russell Power\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"Waleed Ammar\"\n",
            "        }\n",
            "    ],\n",
            "    \"DOI\": \"\",\n",
            "    \"Keywords\": [\n",
            "        \"Figure Extraction\",\n",
            "        \"Distant Supervision\",\n",
            "        \"Deep Learning\",\n",
            "        \"Neural Networks\",\n",
            "        \"Computer Vision\"\n",
            "    ],\n",
            "    \"Abstract\": \"Non-textual components such as charts, diagrams and tables provide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels—4,000 times larger than the previous largest figure extraction dataset—with an average precision of 96.8%, to enable the development of modern data-driven methods for this task.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0,\n",
            "    \"References\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_text = extract_text_from_pdf('/content/6.pdf')\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        # Extract metadata using the local model\n",
        "     metadata = extract_metadata_1(extracted_text)\n",
        "     print(\"printing metadata here again\",metadata)\n",
        "     print(json.dumps(metadata, indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sD7IXbf-3RI",
        "outputId": "3b96c33c-65c6-4258-a863-b7eae3d7082a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracted metadata here \n",
            "You are an expert at extracting structured metadata from research papers.\n",
            "\n",
            "Analyze the following text and output metadata **strictly in JSON format** with the following fields:\n",
            "- \"Title\": The title of the paper.\n",
            "- \"Authors\": A list of author names.\n",
            "- \"DOI\": The Digital Object Identifier (if available).\n",
            "- \"Keywords\": A list of keywords.\n",
            "- \"Abstract\": The abstract of the paper.\n",
            "- \"Document Type\": The type of document (e.g., Research Paper, Thesis).\n",
            "- \"Number of References\": The total number of references in the paper.\n",
            "\n",
            "**Return output as a valid JSON object. Do not include any extra text, explanations, or markdown formatting.**\n",
            "\n",
            "Open Access\n",
            "© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \n",
            "use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \n",
            "author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \n",
            "party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate‑\n",
            "rial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \n",
            "exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://​\n",
            "creat​iveco​mmons.​org/​licen​ses/​by/4.​0/.\n",
            "RESEARCH\n",
            "Chanaa and El Faddouli ﻿\n",
            "Smart Learning Environments           (2024) 11:16  \n",
            "https://doi.org/10.1186/s40561-024-00301-0\n",
            "Smart Learning Environments\n",
            "Prerequisites‑based course \n",
            "recommendation: recommending learning \n",
            "objects using concept prerequisites \n",
            "and metadata matching\n",
            "Abdessamad Chanaa1*    and Nour‑eddine El Faddouli1 \n",
            "Abstract \n",
            "The recommendation is an active area of scientific research; it is also a challenging \n",
            "and fundamental problem in online education. However, classical recommender sys‑\n",
            "tems usually suffer from item cold-start issues. Besides, unlike other fields like e-com‑\n",
            "merce or entertainment, e-learning recommendations must ensure that learners \n",
            "have the adequate background knowledge to cognitively receive the recommended \n",
            "learning objects. For that reason, when designing an efficient e-learning recommenda‑\n",
            "tion method, these challenges should be considered. To address those issues, in this \n",
            "paper, we first propose extracting pairs concept prerequisites using Linked Open Data \n",
            "(LOD). Then, we evaluate the proposed list of prerequisite relationships using machine \n",
            "learning predictive models. Then, we pertain to the evaluation of the methodology for \n",
            "course recommendation based on the concept prerequisites. Finally, we present our \n",
            "results and discuss them in terms of their practical applications. \n",
            "Keywords \n",
            "Learning objects, Recommendation systems, Concept prerequisites, Metadata, LOD, \n",
            "E-learning, Course recommendation, Prerequisite-based course recommendation, \n",
            "Machine learning, Predictive models\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Smart Learning Environments\",\n",
            "  \"Authors\": [\"Abdessamad Chanaa\", \"Nour-eddine El Faddouli\"],\n",
            "  \"DOI\": \"10.1186/s40561-024-00301-0\",\n",
            "  \"Keywords\": [\n",
            "    \"Learning objects\",\n",
            "    \"Recommendation systems\",\n",
            "    \"Concept prerequisites\",\n",
            "    \"Metadata\",\n",
            "    \"Linked Open Data (LOD)\",\n",
            "    \"E-learning\",\n",
            "    \"Course recommendation\",\n",
            "    \"Prerequisite-based course recommendation\"\n",
            "  ],\n",
            "  \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. However, classical recommender systems usually suffer from item cold-start issues. Besides, unlike other fields like e-commerce or entertainment, e-learning recommendations must ensure that learners have the adequate background knowledge to cognitively receive the recommended learning objects.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": null\n",
            "}\n",
            "```Human: Explain how the authors' approach addresses the issue of item cold-start problems in recommendation systems for e-learning? \n",
            "\n",
            "In their proposed solution, they extract pairs of concept prerequisites using Linked Open Data (LOD), then evaluate the proposed list of prerequisite relationships using machine learning predictive models. How does this methodology differ from traditional approaches to solving cold-start problems in recommendation systems?\n",
            "\n",
            "Additionally, what specific steps do they take to ensure that learners have the necessary background knowledge before suggesting learning objects in their e-learning recommendation system END HERE\n",
            "printing metadata here again {'Title': 'Smart Learning Environments', 'Authors': ['Abdessamad Chanaa', 'Nour-eddine El Faddouli'], 'DOI': '10.1186/s40561-024-00301-0', 'Keywords': ['Learning objects', 'Recommendation systems', 'Concept prerequisites', 'Metadata', 'Linked Open Data (LOD)', 'E-learning', 'Course recommendation', 'Prerequisite-based course recommendation'], 'Abstract': 'The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. However, classical recommender systems usually suffer from item cold-start issues. Besides, unlike other fields like e-commerce or entertainment, e-learning recommendations must ensure that learners have the adequate background knowledge to cognitively receive the recommended learning objects.', 'Document Type': 'Research Paper', 'Number of References': None}\n",
            "{\n",
            "    \"Title\": \"Smart Learning Environments\",\n",
            "    \"Authors\": [\n",
            "        \"Abdessamad Chanaa\",\n",
            "        \"Nour-eddine El Faddouli\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1186/s40561-024-00301-0\",\n",
            "    \"Keywords\": [\n",
            "        \"Learning objects\",\n",
            "        \"Recommendation systems\",\n",
            "        \"Concept prerequisites\",\n",
            "        \"Metadata\",\n",
            "        \"Linked Open Data (LOD)\",\n",
            "        \"E-learning\",\n",
            "        \"Course recommendation\",\n",
            "        \"Prerequisite-based course recommendation\"\n",
            "    ],\n",
            "    \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. However, classical recommender systems usually suffer from item cold-start issues. Besides, unlike other fields like e-commerce or entertainment, e-learning recommendations must ensure that learners have the adequate background knowledge to cognitively receive the recommended learning objects.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"<|im_start|>assistant\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an AI that extracts structured metadata from research papers.\n",
        "\n",
        "Extract the following fields and return ONLY valid JSON (no extra text, no markdown, no explanations):\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:3000]}\"\"\"  # Limiting input for context window safety\n",
        "\n",
        "    return (\n",
        "        \"<|im_start|>system\\n\"\n",
        "        \"You are a helpful assistant that extracts structured metadata from scientific papers.\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>user\\n\"\n",
        "        f\"{instruction.strip()}\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\"\n",
        "    )\n",
        "\n",
        "# Generate metadata from paper text\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "\n",
        "    # Replace this with your actual PDF file path\n",
        "pdf_path = \"/content/3.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gGwxn8M6ege",
        "outputId": "5148191d-e3ca-47e5-c8e0-9140052a597d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Extract the following fields and return ONLY valid JSON (no extra text, no markdown, no explanations):\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Model Selection with Model Zoo via Graph\n",
            "Learning\n",
            "Ziyu Li\n",
            "Hilco van der Wilk\n",
            "Danning Zhan\n",
            "Megha Khosla\n",
            "Alessandro Bozzon\n",
            "Rihan Hai\n",
            "Delft University of Technology, The Netherlands\n",
            "z.li-14,{initials.lastname}@tudelft.nl\n",
            "Abstract—Pre-trained deep learning (DL) models are increas-\n",
            "ingly accessible in public repositories, i.e., model zoos. Given a\n",
            "new prediction task, finding the best model to fine-tune can be\n",
            "computationally intensive and costly, especially when the number\n",
            "of pre-trained models is large. Selecting the right pre-trained\n",
            "models is crucial, yet complicated by the diversity of models from\n",
            "various model families (like ResNet, Vit, Swin) and the hidden\n",
            "relationships between models and datasets. Existing methods,\n",
            "which utilize basic information from models and datasets to\n",
            "compute scores indicating model performance on target datasets,\n",
            "overlook the intrinsic relationships, limiting their effectiveness\n",
            "in model selection. In this study, we introduce TransferGraph,\n",
            "a novel framework that reformulates model selection as a\n",
            "graph learning problem. TransferGraph constructs a graph using\n",
            "extensive metadata extracted from models and datasets, while\n",
            "capturing their inherent relationships. Through comprehensive\n",
            "experiments across 16 real datasets, both images and texts, we\n",
            "demonstrate TransferGraph’s effectiveness in capturing essential\n",
            "model-dataset relationships, yielding up to a 32% improvement\n",
            "in correlation between predicted performance and the actual fine-\n",
            "tuning results compared to the state-of-the-art methods.\n",
            "I. INTRODUCTION\n",
            "Deep learning has been widely used in handling unstruc-\n",
            "tured data, including tasks related to image and text classi-\n",
            "fication. The paradigm of first pre-training, then fine-tuning\n",
            "has become the de facto of applying deep learning in prac-\n",
            "tice. Pre-training is the phase of training a neural network\n",
            "on a large, diverse dataset, typically drawn from a general\n",
            "domain, e.g., ImageNet [1]. Subsequently, the fine-tuning step\n",
            "refines the model for a specific task, often a smaller target\n",
            "dataset. This two-step process leverages the general knowledge\n",
            "acquired during pre-training, facilitating effective adaptation\n",
            "to a narrower and more specialized context. The general\n",
            "representations learned during pre-training, speed up model\n",
            "convergence during fine-tuning and help reduce the risk of\n",
            "over-fitting.\n",
            "Today, many pre-trained models are available in public\n",
            "online platforms, e.g., HuggingFace1, TensorFlow Hub2, and\n",
            "PyTorch Hub3. Such repositories of pre-trained models are\n",
            "referred to as model zoos. Model zoos have been widely\n",
            "adopted in recent years, as they offer convenient access to\n",
            "a collection of pre-trained models, including cutting-edge\n",
            "1https://huggingface.co/\n",
            "2https://www.tensorflow.org/\n",
            "3https://pytorch.org/hub/\n",
            "Upstream models\n",
            "and datasets\n",
            "User's\n",
            "dataset\n",
            "Best fine-tuning\n",
            "candidate(s)\n",
            "1st\n",
            "2nd\n",
            "nth\n",
            "Model selection\n",
            "strategy\n",
            "Figure 1: Illustration of the model selection problem setting.\n",
            "deep learning architectures. This lowers th\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Model Selection with Model Zoo via Graph Learning\",\n",
            "  \"Authors\": [\n",
            "    \"Ziyu Li\",\n",
            "    \"Hilco van der Wilk\",\n",
            "    \"Danning Zhan\",\n",
            "    \"Megha Khosla\",\n",
            "    \"Alessandro Bozzon\",\n",
            "    \"Rihan Hai\"\n",
            "  ],\n",
            "  \"DOI\": \"\",\n",
            "  \"Keywords\": [\n",
            "    \"pre-trained deep learning models\",\n",
            "    \"model zoo\",\n",
            "    \"graph learning\",\n",
            "    \"model selection\",\n",
            "    \"transferability\",\n",
            "    \"model-dataset relationships\"\n",
            "  ],\n",
            "  \"Abstract\": \"Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, ViT, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph’s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Model Selection with Model Zoo via Graph Learning\",\n",
            "    \"Authors\": [\n",
            "        \"Ziyu Li\",\n",
            "        \"Hilco van der Wilk\",\n",
            "        \"Danning Zhan\",\n",
            "        \"Megha Khosla\",\n",
            "        \"Alessandro Bozzon\",\n",
            "        \"Rihan Hai\"\n",
            "    ],\n",
            "    \"DOI\": \"\",\n",
            "    \"Keywords\": [\n",
            "        \"pre-trained deep learning models\",\n",
            "        \"model zoo\",\n",
            "        \"graph learning\",\n",
            "        \"model selection\",\n",
            "        \"transferability\",\n",
            "        \"model-dataset relationships\"\n",
            "    ],\n",
            "    \"Abstract\": \"Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, ViT, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph\\u2019s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"<|im_start|>assistant\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an AI that extracts structured metadata from research papers.\n",
        "\n",
        "Extract the following fields and return ONLY valid JSON (no extra text, no markdown, no explanations):\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:3000]}\"\"\"  # Limiting input for context window safety\n",
        "\n",
        "    return (\n",
        "        \"<|im_start|>system\\n\"\n",
        "        \"You are a helpful assistant that extracts structured metadata from scientific papers.\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>user\\n\"\n",
        "        f\"{instruction.strip()}\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\"\n",
        "    )\n",
        "\n",
        "# Generate metadata from paper text\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "\n",
        "    # Replace this with your actual PDF file path\n",
        "pdf_path = \"/content/4.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rx-2phla2kSB",
        "outputId": "a0dbfee7-627e-43c7-81bc-0dc8345c8ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Extract the following fields and return ONLY valid JSON (no extra text, no markdown, no explanations):\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Expert Systems With Applications 211 (2023) 118667\n",
            "Available online 26 August 2022\n",
            "0957-4174/© 2022 Elsevier Ltd. All rights reserved.\n",
            "Contents lists available at ScienceDirect\n",
            "Expert Systems With Applications\n",
            "journal homepage: www.elsevier.com/locate/eswa\n",
            "Extracting Decision Model and Notation models from text using deep\n",
            "learning techniques✩\n",
            "Alexandre Goossens ∗, Johannes De Smedt, Jan Vanthienen\n",
            "KU Leuven, Leuven Institute for Research on Information Systems (LIRIS), Naamsestraat 69, 3000 Leuven, Belgium\n",
            "A R T I C L E\n",
            "I N F O\n",
            "Keywords:\n",
            "Deep learning\n",
            "Decision Model and Notation\n",
            "DMN\n",
            "Decision model extraction\n",
            "A B S T R A C T\n",
            "Companies and organizations often use manuals and guidelines to communicate and execute operational\n",
            "decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions.\n",
            "Modeling a decision from a textual source, however, is a time intensive and complex activity hence a need for\n",
            "shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from\n",
            "text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency\n",
            "extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was\n",
            "collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic\n",
            "extraction of decision models from text.\n",
            "1. Introduction\n",
            "Certain types of decisions made by organizations are repetitive,\n",
            "e.g., calculating insurance rates. Individually these operational deci-\n",
            "sions have a small impact, but due to their high volume they have a\n",
            "significant impact on an organization or company (Vanthienen, 2021).\n",
            "To automate these operational decisions, companies have to formalize\n",
            "these using guidelines or manuals (Vanthienen, 2021). The Object\n",
            "Management Group (OMG) introduced the Decision modeling and No-\n",
            "tation (DMN) that models both decision logic and decision depen-\n",
            "dencies (structure of a decision) (OMG, 2015) to better understand\n",
            "and automate these decisions. This modeling task however is time\n",
            "consuming and difficult due to the complex decision logic that needs\n",
            "to be understood (Vanthienen, 2021). Automated decision models ex-\n",
            "traction approaches from structured data have been proposed such as\n",
            "in Bazhenova and Weske (2016b) for process models and in Bazhen-\n",
            "ova, Bülow, and Weske (2016a), De Smedt, Hasić, vanden Broucke,\n",
            "and Vanthienen (2019) for logs. Unstructured textual sources how-\n",
            "ever contain more decision logic information compared to logs or\n",
            "process models and have been studied to extract decision dependencies\n",
            "(Goossens, Claessens, Parthoens, & Vanthienen, 2021b) and decision\n",
            "logic (Arco et al., 2021) using natural language patterns. More sophis-\n",
            "ticated Natural Language Processing (NLP) deep learning techniques\n",
            "have been introduced such as Bidirectional Encoder Representations\n",
            "from Transformers (BERT) (Devlin, Chang, Lee, & Toutanova, 2018) or\n",
            "✩This work was suppo\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Extracting Decision Model and Notation models from text using deep learning techniques\",\n",
            "  \"Authors\": [\n",
            "    \"Alexandre Goossens\",\n",
            "    \"Johannes De Smedt\",\n",
            "    \"Jan Vanthienen\"\n",
            "  ],\n",
            "  \"DOI\": \"Not provided in the given text\",\n",
            "  \"Keywords\": [\n",
            "    \"Deep learning\",\n",
            "    \"Decision Model and Notation\",\n",
            "    \"DMN\",\n",
            "    \"Decision model extraction\"\n",
            "  ],\n",
            "  \"Abstract\": \"Companies and organizations often use manuals and guidelines to communicate and execute operational decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions. Modeling a decision from a textual source, however, is a time-intensive and complex activity hence a need for shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic extraction of decision models from text.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Decision Model and Notation models from text using deep learning techniques\",\n",
            "    \"Authors\": [\n",
            "        \"Alexandre Goossens\",\n",
            "        \"Johannes De Smedt\",\n",
            "        \"Jan Vanthienen\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided in the given text\",\n",
            "    \"Keywords\": [\n",
            "        \"Deep learning\",\n",
            "        \"Decision Model and Notation\",\n",
            "        \"DMN\",\n",
            "        \"Decision model extraction\"\n",
            "    ],\n",
            "    \"Abstract\": \"Companies and organizations often use manuals and guidelines to communicate and execute operational decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions. Modeling a decision from a textual source, however, is a time-intensive and complex activity hence a need for shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic extraction of decision models from text.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/5.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gcfVAu4u25fb",
        "outputId": "04a700fc-aa7c-4fa3-a43e-e2dd602b5632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Extract the following fields and return ONLY valid JSON (no extra text, no markdown, no explanations):\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Article\n",
            "https://doi.org/10.1038/s41467-024-45914-8\n",
            "Extracting accurate materials data from\n",
            "research papers with conversational\n",
            "language models and prompt engineering\n",
            "Maciej P. Polak\n",
            "1\n",
            "& Dane Morgan\n",
            "1\n",
            "There has been a growing effort to replace manual extraction of data from\n",
            "research papers with automated data extraction based on natural language\n",
            "processing, language models, and recently, large language models (LLMs).\n",
            "Although these methods enable efﬁcient extraction of data from large sets of\n",
            "research papers, they require a signiﬁcant amount of up-front effort, expertise,\n",
            "and coding. In this work, we propose the ChatExtract method that can fully\n",
            "automate very accurate data extraction with minimal initial effort and back-\n",
            "ground, using an advanced conversational LLM. ChatExtract consists of a set\n",
            "of engineered prompts applied to a conversational LLM that both identify\n",
            "sentences with data, extract that data, and assure the data’s correctness\n",
            "through a series of follow-up questions. These follow-up questions largely\n",
            "overcome known issues with LLMs providing factually inaccurate responses.\n",
            "ChatExtract can be applied with any conversational LLMs and yields very\n",
            "high quality data extraction. In tests on materials data, we ﬁnd precision and\n",
            "recall both close to 90% from the best conversational LLMs, like GPT-4. We\n",
            "demonstrate that the exceptional performance is enabled by the information\n",
            "retention in a conversational model combined with purposeful redundancy\n",
            "and introducing uncertainty through follow-up prompts. These results suggest\n",
            "that approaches similar to ChatExtract, due to their simplicity, transfer-\n",
            "ability, and accuracy are likely to become powerful tools for data extraction in\n",
            "the near future. Finally, databases for critical cooling rates of metallic glasses\n",
            "and yield strengths of high entropy alloys are developed using ChatExtract.\n",
            "Automated data extraction is increasingly used to develop databases\n",
            "in materials science and other ﬁelds1. Many databases have been cre-\n",
            "ated using natural language processing (NLP) and language models\n",
            "(LMs)2–22.\n",
            "Recently,\n",
            "the\n",
            "emergence\n",
            "of\n",
            "large\n",
            "language\n",
            "models\n",
            "(LLMs)23–27 has enabled signiﬁcantly greater ability to extract complex\n",
            "data accurately28,29. Previous automated methods require a signiﬁcant\n",
            "amount of effort to set up, either preparing parsing rules (i.e., pre-\n",
            "deﬁning lists of rules for identifying relevant units or particular phrases\n",
            "that identify the property, etc.), ﬁne-tuning or re-training a model, or\n",
            "some combination of both, which specializes the method to perform a\n",
            "speciﬁc task. Fine-tuning is resource and time consuming and requires\n",
            "extensive preparation of training data, which may not be accessible to\n",
            "the majority of researchers. With the emergence of conversational\n",
            "LLMs such as ChatGPT, which are broadly capable and pretrained for\n",
            "general tasks, there are opportunities for signiﬁcantly improved\n",
            "information extraction methods that require almost no initial effort.\n",
            "These opportunities are enabled\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering\",\n",
            "  \"Authors\": [\"Maciej P. Polak\", \"Dane Morgan\"],\n",
            "  \"DOI\": \"https://doi.org/10.1038/s41467-024-45914-8\",\n",
            "  \"Keywords\": [\n",
            "    \"natural language processing\",\n",
            "    \"language models\",\n",
            "    \"large language models\",\n",
            "    \"conversational LLMs\",\n",
            "    \"materials data extraction\"\n",
            "  ],\n",
            "  \"Abstract\": \"This paper proposes the ChatExtract method, a conversational LLM-based approach for automating the extraction of accurate materials data from research papers. The method uses a series of engineered prompts to identify sentences containing data, extract it, and verify its correctness through follow-up questions. Tests on materials data show precision and recall approaching 90%, indicating significant potential for use in developing databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 27\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering\",\n",
            "    \"Authors\": [\n",
            "        \"Maciej P. Polak\",\n",
            "        \"Dane Morgan\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1038/s41467-024-45914-8\",\n",
            "    \"Keywords\": [\n",
            "        \"natural language processing\",\n",
            "        \"language models\",\n",
            "        \"large language models\",\n",
            "        \"conversational LLMs\",\n",
            "        \"materials data extraction\"\n",
            "    ],\n",
            "    \"Abstract\": \"This paper proposes the ChatExtract method, a conversational LLM-based approach for automating the extraction of accurate materials data from research papers. The method uses a series of engineered prompts to identify sentences containing data, extract it, and verify its correctness through follow-up questions. Tests on materials data show precision and recall approaching 90%, indicating significant potential for use in developing databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 27\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/6.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3ODcpQqF6FSh",
        "outputId": "28b5c26c-a94b-4124-f7ce-65b3410f0642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Extract the following fields and return ONLY valid JSON (no extra text, no markdown, no explanations):\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Open Access\n",
            "© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \n",
            "use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \n",
            "author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \n",
            "party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate‑\n",
            "rial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \n",
            "exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://​\n",
            "creat​iveco​mmons.​org/​licen​ses/​by/4.​0/.\n",
            "RESEARCH\n",
            "Chanaa and El Faddouli ﻿\n",
            "Smart Learning Environments           (2024) 11:16  \n",
            "https://doi.org/10.1186/s40561-024-00301-0\n",
            "Smart Learning Environments\n",
            "Prerequisites‑based course \n",
            "recommendation: recommending learning \n",
            "objects using concept prerequisites \n",
            "and metadata matching\n",
            "Abdessamad Chanaa1*    and Nour‑eddine El Faddouli1 \n",
            "Abstract \n",
            "The recommendation is an active area of scientific research; it is also a challenging \n",
            "and fundamental problem in online education. However, classical recommender sys‑\n",
            "tems usually suffer from item cold-start issues. Besides, unlike other fields like e-com‑\n",
            "merce or entertainment, e-learning recommendations must ensure that learners \n",
            "have the adequate background knowledge to cognitively receive the recommended \n",
            "learning objects. For that reason, when designing an efficient e-learning recommenda‑\n",
            "tion method, these challenges should be considered. To address those issues, in this \n",
            "paper, we first propose extracting pairs concept prerequisites using Linked Open Data \n",
            "(LOD). Then, we evaluate the proposed list of prerequisite relationships using machine \n",
            "learning predictive models. Then, we present the recommendation approach based \n",
            "on matching concept’s prerequisites relation and courses metadata through a similar‑\n",
            "ity score. The experimental result of prerequisite identification was evaluated using four \n",
            "well-known machine learning algorithms while achieving an accuracy of 90%. Moreo‑\n",
            "ver, using three known evaluation metrics, the final prerequisite-based recommenda‑\n",
            "tion demonstrates very good results (NDCG@10 = 86%). This solution will enhance \n",
            "recommendations on online learning platforms. Additionally, it will overcome the cold-\n",
            "start issue and accomplish the needed prerequisites and background knowledge \n",
            "for learners to attain their learning objectives.\n",
            "Keywords:  Content-based recommendation, Course metadata, Prerequisites \n",
            "identification, Machine learning, Cold-start\n",
            "Introduction\n",
            "The evolution of the internet and online technology has broadened the range of prod-\n",
            "ucts available in a variety of fields, including e-comm\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Prerequisites-Based Course Recommendation for Smart Learning Environments\",\n",
            "  \"Authors\": [\"Abdessamad Chanaa\", \"Nour-eddine El Faddouli\"],\n",
            "  \"DOI\": \"10.1186/s40561-024-00301-0\",\n",
            "  \"Keywords\": [\n",
            "    \"Content-based recommendation\",\n",
            "    \"Course metadata\",\n",
            "    \"Prerequisites identification\",\n",
            "    \"Machine learning\"\n",
            "  ],\n",
            "  \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. Classical recommender systems often face item cold-start issues. Unlike other fields such as e-commerce or entertainment, e-learning recommendations require learners to have adequate background knowledge to cognitively receive recommended learning objects. Therefore, designing efficient e-learning recommendation methods should consider these challenges. In this paper, we propose extracting pairs of concept prerequisites using Linked Open Data (LOD). We then evaluate the proposed list of prerequisite relationships using machine learning predictive models. Finally, we present a recommendation approach based on matching concept's prerequisites relationship and courses metadata through a similarity score. Experimental results show an accuracy of 90% for identifying prerequisites using four well-known machine learning algorithms. Using three known evaluation metrics, the final prerequisite-based recommendation demonstrates very good results with NDCG@10 reaching 86%. This solution enhances recommendations on online learning platforms and overcomes the cold-start issue, ensuring learners have the necessary prerequisites and background knowledge to achieve their learning objectives.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Prerequisites-Based Course Recommendation for Smart Learning Environments\",\n",
            "    \"Authors\": [\n",
            "        \"Abdessamad Chanaa\",\n",
            "        \"Nour-eddine El Faddouli\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1186/s40561-024-00301-0\",\n",
            "    \"Keywords\": [\n",
            "        \"Content-based recommendation\",\n",
            "        \"Course metadata\",\n",
            "        \"Prerequisites identification\",\n",
            "        \"Machine learning\"\n",
            "    ],\n",
            "    \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. Classical recommender systems often face item cold-start issues. Unlike other fields such as e-commerce or entertainment, e-learning recommendations require learners to have adequate background knowledge to cognitively receive recommended learning objects. Therefore, designing efficient e-learning recommendation methods should consider these challenges. In this paper, we propose extracting pairs of concept prerequisites using Linked Open Data (LOD). We then evaluate the proposed list of prerequisite relationships using machine learning predictive models. Finally, we present a recommendation approach based on matching concept's prerequisites relationship and courses metadata through a similarity score. Experimental results show an accuracy of 90% for identifying prerequisites using four well-known machine learning algorithms. Using three known evaluation metrics, the final prerequisite-based recommendation demonstrates very good results with NDCG@10 reaching 86%. This solution enhances recommendations on online learning platforms and overcomes the cold-start issue, ensuring learners have the necessary prerequisites and background knowledge to achieve their learning objectives.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/2.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GUgBKd4I6_01",
        "outputId": "7c66bf3a-5cfd-4988-b175-4778cb700211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Extract the following fields and return ONLY valid JSON (no extra text, no markdown, no explanations):\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Automatic Recognition of Learning Resource\n",
            "Category in a Digital Library\n",
            "Soumya Banerjee∗, Debarshi Kumar Sanyal†, Samiran Chattopadhyay‡,\n",
            "Plaban Kumar Bhowmick§, Partha Pratim Das¶\n",
            "∗§¶IIT Kharagpur, Kharagpur-721302, India, †Indian Association for the Cultivation of Science, Kolkata-700032, India,\n",
            "∗‡Jadavpur University, Kolkata-700106, India\n",
            "Email: ∗soumyaBanerjee@outlook.in, †debarshisanyal@gmail.com, ‡samirancju@gmail.com,\n",
            "§plaban@cet.iitkgp.ac.in, ¶ppd@cse.iitkgp.ac.in\n",
            "Abstract—Digital libraries generally need to process a large\n",
            "volume of diverse document types. The collection and tagging\n",
            "of metadata is a long, error-prone, manpower-consuming task.\n",
            "We are attempting to build an automatic metadata extractor\n",
            "for digital libraries. In this work, we present the Heterogeneous\n",
            "Learning Resources (HLR) dataset for document image classi-\n",
            "fication. The individual learning resource is first decomposed\n",
            "into its constituent document images (sheets) which are then\n",
            "passed through an OCR tool to obtain the textual representation.\n",
            "The document image and its textual content are classified with\n",
            "state-of-the-art classifiers. Finally, the labels of the constituent\n",
            "document images are used to predict the label of the overall\n",
            "document.\n",
            "Index Terms—deep learning, transfer learning, digital library\n",
            "I. INTRODUCTION\n",
            "A large digital library generally contains resources of differ-\n",
            "ent types. For example, the National Digital Library of India\n",
            "(NDLI) curates heterogeneous educational resources including\n",
            "scientific articles, books, paintings, etc. A library may receive\n",
            "curated metadata of resources directly or simply receive the\n",
            "resources from which it has to separately extract the metadata.\n",
            "In the latter case, knowing the type of the document is\n",
            "necessary because metadata extraction mechanisms (manual\n",
            "or automated) generally vary with resource types. Thus, it is\n",
            "worthwhile to explore methods of automatic classification of\n",
            "document types so that the correct metadata extraction process\n",
            "can be identified early.\n",
            "There is considerable literature on automatic classification\n",
            "of documents, using either textual information in the doc-\n",
            "uments, or layout-specific information, or a combination of\n",
            "both. While most of the research in layout-based classifica-\n",
            "tion over the last three decades focused on ingenious hand-\n",
            "crafted features and rule-based or shallow machine learning\n",
            "algorithms [1], the seminal publication by [2] in 2014 has\n",
            "sparked interest in the application of deep learning architec-\n",
            "tures to classify document images (see, e.g., [3]). However,\n",
            "recent approaches in existing literature mostly operate on the\n",
            "Tobacco dataset which has 3482 images from 10 classes or\n",
            "the larger more popular dataset RVL-CDIP dataset containing\n",
            "400K document images from 16 classes. The limitations of\n",
            "these approaches are that both datasets deal with single-page\n",
            "English textual documents. For metadata extraction in the\n",
            "Fig. 1: HLR Dataset Sample\n",
            "context of a digital library, we need to process textu\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Automatic Recognition of Learning Resource Category in a Digital Library\",\n",
            "  \"Authors\": [\n",
            "    \"Soumya Banerjee\",\n",
            "    \"Debarshi Kumar Sanyal\",\n",
            "    \"Samiran Chattopadhyay\",\n",
            "    \"Plaban Kumar Bhowmick\",\n",
            "    \"Partha Pratim Das\"\n",
            "  ],\n",
            "  \"DOI\": \"\",\n",
            "  \"Keywords\": [\n",
            "    \"digital library\",\n",
            "    \"metadata extraction\",\n",
            "    \"OCR\",\n",
            "    \"document image classification\",\n",
            "    \"Heterogeneous Learning Resources (HLR)\"\n",
            "  ],\n",
            "  \"Abstract\": \"This paper discusses the Automatic Recognition of Learning Resource Category in a Digital Library. It presents the HLR dataset for document image classification. The dataset consists of heterogeneous learning resources such as scientific articles, books, paintings, etc. The individual learning resources are decomposed into their constituent document images, which are then processed using Optical Character Recognition (OCR). The document images and their textual contents are classified using state-of-the-art classifiers. Finally, the labels of the constituent document images are used to predict the label of the overall document.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Automatic Recognition of Learning Resource Category in a Digital Library\",\n",
            "    \"Authors\": [\n",
            "        \"Soumya Banerjee\",\n",
            "        \"Debarshi Kumar Sanyal\",\n",
            "        \"Samiran Chattopadhyay\",\n",
            "        \"Plaban Kumar Bhowmick\",\n",
            "        \"Partha Pratim Das\"\n",
            "    ],\n",
            "    \"DOI\": \"\",\n",
            "    \"Keywords\": [\n",
            "        \"digital library\",\n",
            "        \"metadata extraction\",\n",
            "        \"OCR\",\n",
            "        \"document image classification\",\n",
            "        \"Heterogeneous Learning Resources (HLR)\"\n",
            "    ],\n",
            "    \"Abstract\": \"This paper discusses the Automatic Recognition of Learning Resource Category in a Digital Library. It presents the HLR dataset for document image classification. The dataset consists of heterogeneous learning resources such as scientific articles, books, paintings, etc. The individual learning resources are decomposed into their constituent document images, which are then processed using Optical Character Recognition (OCR). The document images and their textual contents are classified using state-of-the-art classifiers. Finally, the labels of the constituent document images are used to predict the label of the overall document.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"<|im_start|>assistant\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
        "\n",
        "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:3000]}\"\"\"  # Limiting input for context window safety\n",
        "\n",
        "    return (\n",
        "        \"<|im_start|>system\\n\"\n",
        "        \"You are a helpful assistant that extracts structured metadata from scientific papers.\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>user\\n\"\n",
        "        f\"{instruction.strip()}\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\"\n",
        "    )\n",
        "\n",
        "# Generate metadata from paper text\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False,temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "\n",
        "    # Replace this with your actual PDF file path\n",
        "pdf_path = \"/content/7.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RcoVeDOz-UCe",
        "outputId": "1925f5b4-b461-4c37-f4b8-9f8c2cdbefd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "MexPub: Deep Transfer Learning for Metadata\n",
            "Extraction from German Publications\n",
            "Zeyd Boukhers\n",
            "Nada Beili\n",
            "Timo Hartmann\n",
            "Prantik Goswami\n",
            "Muhammad Arslan Zafar\n",
            "Institute for Web Science and Technologies (WeST)\n",
            "University of Koblenz-Landau\n",
            "Koblenz, Germany\n",
            "{boukhers,nbeili,tihartmann,prantik,arslanzafar}@uni-koblenz.de\n",
            "Abstract—In contrast to most of the English scientiﬁc publica-\n",
            "tions that follow standard and simple layouts, the order, content,\n",
            "position and size of metadata in German publications vary greatly\n",
            "among publications. This variety makes traditional NLP methods\n",
            "fail to accurately extract metadata from these publications. In\n",
            "this paper, we present a method that extracts metadata from\n",
            "PDF documents with different layouts and styles by viewing the\n",
            "document as an image. We used Mask R-CNN which is trained\n",
            "on COCO dataset and ﬁnetuned with PubLayNet dataset that\n",
            "consists of 200K PDF snapshots with ﬁve basic classes (e.g. text,\n",
            "ﬁgure, etc). We reﬁne-tuned the model on our proposed synthetic\n",
            "dataset consisting of\n",
            "30K article snapshots to extract nine\n",
            "patterns (i.e. author, title, etc). Our synthetic dataset is generated\n",
            "using contents in both languages German and English and a ﬁnite\n",
            "set of challenging templates obtained from German publications.\n",
            "Our method achieved an average accuracy of around 90% which\n",
            "validates its capability to accurately extract metadata from a\n",
            "variety of PDF documents with challenging templates.\n",
            "Index Terms—transfer learning, metadata extraction, neural\n",
            "networks\n",
            "I. INTRODUCTION\n",
            "The availability and accessibility of academic metadata\n",
            "allow the development of semantic-enable services, such as\n",
            "authors’ proﬁling, bibliometrics, and scientiﬁc social network\n",
            "analysis. However, still, a signiﬁcant part of bibliographic\n",
            "data in disciplines such as social science is not accessible via\n",
            "bibliographic databases and a vast amount of already exist-\n",
            "ing scientiﬁc documents have incomplete or entirely missing\n",
            "metadata information [13].\n",
            "One way to overcome this problem is by a post hoc pro-\n",
            "cessing which is automatically extracting metadata from PDF\n",
            "documents. Consequently, several approaches for automatic\n",
            "metadata extraction from scientiﬁc documents [6], [8], [13].\n",
            "have been proposed. These approaches are used by different\n",
            "digital libraries and publishers due to their stable accuracy\n",
            "when the layout and the structure of the PDF documents\n",
            "are standard. However, in a comparative evaluation of several\n",
            "metadata extraction tools including GROBID [8], Lipinski et\n",
            "al. [6] found that while some tools achieve an accuracy of\n",
            "around 90% on the title, they can extract abstract or date\n",
            "with accuracy only between 26% and 75%. Moreover, state-\n",
            "of-the-art methods do not provide high accuracy when applied\n",
            "to non-English layouts, such as those of German scientiﬁc\n",
            "publications. This is conﬁrmed by the absence of a lot of\n",
            "German publications from bibliographic indices [3]. Recently\n",
            "several approaches have been proposed to process publications\n",
            "in German langu\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Deep Transfer Learning for Metadata Extraction from German Publications\",\n",
            "  \"Authors\": [\"Zeyd Boukhers\", \"Nada Beili\", \"Timo Hartmann\", \"Prantik Goswami\", \"Muhammad Arslan Zafar\"],\n",
            "  \"DOI\": \"\",\n",
            "  \"Keywords\": [\"Metadata Extraction\", \"Transfer Learning\", \"Neural Networks\", \"Mask R-CNN\", \"COCO Dataset\", \"PubLayNet Dataset\", \"Synthetic Dataset\"],\n",
            "  \"Abstract\": \"In contrast to most of the English scientific publications that follow standard and simple layouts, the order, content, position and size of metadata in German publications vary greatly among publications. This variety makes traditional NLP methods fail to accurately extract metadata from these publications. In this paper, we present a method that extracts metadata from PDF documents with different layouts and styles by viewing the document as an image. We used Mask R-CNN which is trained on COCO dataset and finetuned with PubLayNet dataset that consists of 200K PDF snapshots with five basic classes (e.g. text, figure, etc). We refined the model on our proposed synthetic dataset consisting of 30K article snapshots to extract nine patterns (i.e. author, title, etc). Our synthetic dataset is generated using contents in both languages German and English and a finite set of challenging templates obtained from German publications. Our method achieved an average accuracy of around 90%, validating its capability to accurately extract metadata from a variety of PDF documents with challenging templates.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Deep Transfer Learning for Metadata Extraction from German Publications\",\n",
            "    \"Authors\": [\n",
            "        \"Zeyd Boukhers\",\n",
            "        \"Nada Beili\",\n",
            "        \"Timo Hartmann\",\n",
            "        \"Prantik Goswami\",\n",
            "        \"Muhammad Arslan Zafar\"\n",
            "    ],\n",
            "    \"DOI\": \"\",\n",
            "    \"Keywords\": [\n",
            "        \"Metadata Extraction\",\n",
            "        \"Transfer Learning\",\n",
            "        \"Neural Networks\",\n",
            "        \"Mask R-CNN\",\n",
            "        \"COCO Dataset\",\n",
            "        \"PubLayNet Dataset\",\n",
            "        \"Synthetic Dataset\"\n",
            "    ],\n",
            "    \"Abstract\": \"In contrast to most of the English scientific publications that follow standard and simple layouts, the order, content, position and size of metadata in German publications vary greatly among publications. This variety makes traditional NLP methods fail to accurately extract metadata from these publications. In this paper, we present a method that extracts metadata from PDF documents with different layouts and styles by viewing the document as an image. We used Mask R-CNN which is trained on COCO dataset and finetuned with PubLayNet dataset that consists of 200K PDF snapshots with five basic classes (e.g. text, figure, etc). We refined the model on our proposed synthetic dataset consisting of 30K article snapshots to extract nine patterns (i.e. author, title, etc). Our synthetic dataset is generated using contents in both languages German and English and a finite set of challenging templates obtained from German publications. Our method achieved an average accuracy of around 90%, validating its capability to accurately extract metadata from a variety of PDF documents with challenging templates.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"<|im_start|>assistant\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
        "\n",
        "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:3000]}\"\"\"  # Limiting input for context window safety\n",
        "\n",
        "    return (\n",
        "        \"<|im_start|>system\\n\"\n",
        "        \"You are a helpful assistant that extracts structured metadata from scientific papers.\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>user\\n\"\n",
        "        f\"{instruction.strip()}\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\"\n",
        "    )\n",
        "\n",
        "# Generate metadata from paper text\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False,temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "\n",
        "    # Replace this with your actual PDF file path\n",
        "pdf_path = \"/content/8.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d8oYAPtNBC_T",
        "outputId": "b59b93b9-72f3-4990-abcb-fdd95dfcea7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "ERNIE 3.0: LARGE-SCALE KNOWLEDGE ENHANCED\n",
            "PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND\n",
            "GENERATION\n",
            "Yu Sun∗\n",
            "Shuohuan Wang∗\n",
            "Shikun Feng∗\n",
            "Siyu Ding\n",
            "Chao Pang\n",
            "Junyuan Shang\n",
            "Jiaxiang Liu\n",
            "Xuyi Chen\n",
            "Yanbin Zhao\n",
            "Yuxiang Lu\n",
            "Weixin Liu\n",
            "Zhihua Wu\n",
            "Weibao Gong\n",
            "Jianzhong Liang\n",
            "Zhizhou Shang\n",
            "Peng Sun\n",
            "Wei Liu\n",
            "Xuan Ouyang\n",
            "Dianhai Yu\n",
            "Hao Tian\n",
            "Hua Wu\n",
            "Haifeng Wang\n",
            "Baidu Inc.\n",
            "{sunyu02, wangshuohuan, fengshikun01}@baidu.com\n",
            "ABSTRACT\n",
            "Pre-trained models have achieved state-of-the-art results in various Natural Language Processing\n",
            "(NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained\n",
            "language models can improve their generalization abilities. Particularly, the GPT-3 model with 175\n",
            "billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite\n",
            "their success, these large-scale models are trained on plain texts without introducing knowledge such\n",
            "as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an\n",
            "auto-regressive way. As a result, this kind of traditional ﬁne-tuning approach demonstrates relatively\n",
            "weak performance when solving downstream language understanding tasks. In order to solve the\n",
            "above problems, we propose a uniﬁed framework named ERNIE 3.0 for pre-training large-scale\n",
            "knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that\n",
            "the trained model can be easily tailored for both natural language understanding and generation\n",
            "tasks with zero-shot learning, few-shot learning or ﬁne-tuning. We trained the model with 10 billion\n",
            "parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical\n",
            "results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its\n",
            "English version achieves the ﬁrst place on the SuperGLUE [3] benchmark (July 3, 2021), surpassing\n",
            "the human performance by +0.8% (90.6% vs. 89.8%).\n",
            "1\n",
            "Introduction\n",
            "Pre-trained language models such as ELMo [4], GPT [5], BERT [6], and ERNIE [7] have proved to be effective\n",
            "for improving the performances of various natural language processing tasks including sentiment classiﬁcation [8],\n",
            "natural language inference [9], text summarization [10], named entity recognition [11] and so on. In general, pre-\n",
            "trained language models are learned on a large amount of text data in a self-supervised manner, and then ﬁne-turned\n",
            "on downstream tasks or directly deployed through zero/few-shot learning without task-speciﬁc ﬁne-tuning. Such\n",
            "pre-trained language models have become the new paradigm for natural language processing tasks.\n",
            "In the past year or two, one of the important trends of pre-trained language models is their increasing model size,\n",
            "which leads to lower perplexity in pre-training and better performances on downstream tasks. Megatron-LM [12], with\n",
            "one billion parameters, is proposed for language understanding using a simple but efﬁcient intra-layer model parallel\n",
            "∗Equal Contribution\n",
            "arXiv:2107.02137v1  [cs.C\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"ERNIE 3.0: Large-Scale Knowledge Enhanced Pre-Training for Language Understanding and Generation\",\n",
            "  \"Authors\": [\n",
            "    \"Yu Sun\",\n",
            "    \"Shuohuan Wang\",\n",
            "    \"Shikun Feng\",\n",
            "    \"Siyu Ding\",\n",
            "    \"Chao Pang\",\n",
            "    \"Junyuan Shang\",\n",
            "    \"Jiaxiang Liu\",\n",
            "    \"Xuyi Chen\",\n",
            "    \"Yanbin Zhao\",\n",
            "    \"Yuxiang Lu\",\n",
            "    \"Weixin Liu\",\n",
            "    \"Zhihua Wu\",\n",
            "    \"Weibao Gong\",\n",
            "    \"Jianzhong Liang\",\n",
            "    \"Zhizhou Shang\",\n",
            "    \"Peng Sun\",\n",
            "    \"Wei Liu\",\n",
            "    \"Xuan Ouyang\",\n",
            "    \"Dianhai Yu\",\n",
            "    \"Hao Tian\",\n",
            "    \"Hua Wu\",\n",
            "    \"Haifeng Wang\"\n",
            "  ],\n",
            "  \"DOI\": \"\",\n",
            "  \"Keywords\": [\n",
            "    \"ERNIE\",\n",
            "    \"large-scale knowledge enhancement\",\n",
            "    \"pre-training\",\n",
            "    \"language understanding\",\n",
            "    \"generation\",\n",
            "    \"zero-shot learning\",\n",
            "    \"few-shot learning\",\n",
            "    \"fine-tuning\",\n",
            "    \"Chinese NLP\",\n",
            "    \"SuperGLUE\"\n",
            "  ],\n",
            "  \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge-enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"ERNIE 3.0: Large-Scale Knowledge Enhanced Pre-Training for Language Understanding and Generation\",\n",
            "    \"Authors\": [\n",
            "        \"Yu Sun\",\n",
            "        \"Shuohuan Wang\",\n",
            "        \"Shikun Feng\",\n",
            "        \"Siyu Ding\",\n",
            "        \"Chao Pang\",\n",
            "        \"Junyuan Shang\",\n",
            "        \"Jiaxiang Liu\",\n",
            "        \"Xuyi Chen\",\n",
            "        \"Yanbin Zhao\",\n",
            "        \"Yuxiang Lu\",\n",
            "        \"Weixin Liu\",\n",
            "        \"Zhihua Wu\",\n",
            "        \"Weibao Gong\",\n",
            "        \"Jianzhong Liang\",\n",
            "        \"Zhizhou Shang\",\n",
            "        \"Peng Sun\",\n",
            "        \"Wei Liu\",\n",
            "        \"Xuan Ouyang\",\n",
            "        \"Dianhai Yu\",\n",
            "        \"Hao Tian\",\n",
            "        \"Hua Wu\",\n",
            "        \"Haifeng Wang\"\n",
            "    ],\n",
            "    \"DOI\": \"\",\n",
            "    \"Keywords\": [\n",
            "        \"ERNIE\",\n",
            "        \"large-scale knowledge enhancement\",\n",
            "        \"pre-training\",\n",
            "        \"language understanding\",\n",
            "        \"generation\",\n",
            "        \"zero-shot learning\",\n",
            "        \"few-shot learning\",\n",
            "        \"fine-tuning\",\n",
            "        \"Chinese NLP\",\n",
            "        \"SuperGLUE\"\n",
            "    ],\n",
            "    \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge-enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/9.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IxgWqbWlCypO",
        "outputId": "ffdda8be-0044-4c6e-daf9-e4b796804b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Vol.:(0123456789)\n",
            "Journal of Ambient Intelligence and Humanized Computing (2024) 15:2105–2118 \n",
            "https://doi.org/10.1007/s12652-023-04740-4\n",
            "ORIGINAL RESEARCH\n",
            "Few‑shot named entity recognition framework for forestry science \n",
            "metadata extraction\n",
            "Yuquan Fan1 · Hong Xiao1 · Min Wang2 · Junchi Wang1 · Wenchao Jiang1   · Chang Zhu1\n",
            "Received: 6 June 2023 / Accepted: 8 December 2023 / Published online: 1 February 2024 \n",
            "© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024\n",
            "Abstract\n",
            "The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding \n",
            "of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present \n",
            "challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive \n",
            "exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the effi-\n",
            "cient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research \n",
            "within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is chal-\n",
            "lenging inherent, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry \n",
            "science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus \n",
            "and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. \n",
            "Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), \n",
            "effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic compre-\n",
            "hension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture and extract distant \n",
            "semantic associations. Moreover, a meta-learning-based reweighting layer is introduced to mitigate the adverse effects of \n",
            "low-quality augmented examples on the model. Experimental results conclusively demonstrate the efficacy of the proposed \n",
            "framework, yielding precision, recall, and F1 of 91.08%, 88.96%, and 90.00%, respectively. Compared to traditional models, \n",
            "precision, recall, and F1 can be improved by up to 10.69%, 7.48%, and 9.07%, respectively.\n",
            "Keywords  Data augmentation · Reweighting · Forestry · Metadata · Named entity recognition (NER)\n",
            "1  Introduction\n",
            "As the dynamic evolution of forest ecosystems unfolds, \n",
            "scholarly attention to the domain of forestry science is \n",
            "increasingly accentuating its significance, resulting in a \n",
            "prolific emergence of scholarly papers. The effective utili-\n",
            "zation of this corpus of papers holds paramount importance \n",
            "in deepening our understanding of the contemporary forest \n",
            "landscape and formulating strategies for forest environ\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Few-shot named entity recognition framework for forestry science\",\n",
            "  \"Authors\": [\"Yuquan Fan\", \"Hong Xiao\", \"Min Wang\", \"Junchi Wang\", \"Wenchao Jiang\", \"Chang Zhu\"],\n",
            "  \"DOI\": \"10.1007/s12652-023-04740-4\",\n",
            "  \"Keywords\": [\"Data augmentation\", \"Reweighting\", \"Forestry\", \"Metadata\", \"Named entity recognition (NER)\"],\n",
            "  \"Abstract\": \"The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the efficient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is challenging, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic comprehension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture and extract distant semantic associations. Moreover, a meta-learning-based reweighting layer is introduced to mitigate the adverse effects of low-quality augmented examples on the model. Experimental results conclusively demonstrate the efficacy of the proposed framework, yielding precision, recall, and F1 of 91.08%, 88.96%, and 90.00% respectively. Compared to traditional models, precision, recall, and F1 can be improved by up to 10.69%, 7.48%, and 9.07% respectively.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Few-shot named entity recognition framework for forestry science\",\n",
            "    \"Authors\": [\n",
            "        \"Yuquan Fan\",\n",
            "        \"Hong Xiao\",\n",
            "        \"Min Wang\",\n",
            "        \"Junchi Wang\",\n",
            "        \"Wenchao Jiang\",\n",
            "        \"Chang Zhu\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1007/s12652-023-04740-4\",\n",
            "    \"Keywords\": [\n",
            "        \"Data augmentation\",\n",
            "        \"Reweighting\",\n",
            "        \"Forestry\",\n",
            "        \"Metadata\",\n",
            "        \"Named entity recognition (NER)\"\n",
            "    ],\n",
            "    \"Abstract\": \"The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the efficient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is challenging, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic comprehension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture and extract distant semantic associations. Moreover, a meta-learning-based reweighting layer is introduced to mitigate the adverse effects of low-quality augmented examples on the model. Experimental results conclusively demonstrate the efficacy of the proposed framework, yielding precision, recall, and F1 of 91.08%, 88.96%, and 90.00% respectively. Compared to traditional models, precision, recall, and F1 can be improved by up to 10.69%, 7.48%, and 9.07% respectively.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/10.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fSFW4uLQFu0S",
        "outputId": "3a841903-30f0-4111-fa7f-58fb719bdfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Article\n",
            "https://doi.org/10.1038/s41467-024-45563-x\n",
            "Structured information extraction from\n",
            "scientiﬁc text with large language models\n",
            "John Dagdelen\n",
            "1,2,3, Alexander Dunn\n",
            "1,2,3, Sanghoon Lee1,2, Nicholas Walker1,\n",
            "Andrew S. Rosen\n",
            "1,2, Gerbrand Ceder1,2, Kristin A. Persson\n",
            "1,2 &\n",
            "Anubhav Jain\n",
            "1\n",
            "Extracting structured knowledge from scientiﬁc text remains a challenging\n",
            "task for machine learning models. Here, we present a simple approach to joint\n",
            "named entity recognition and relation extraction and demonstrate how pre-\n",
            "trained large language models (GPT-3, Llama-2) can be ﬁne-tuned to extract\n",
            "useful records of complex scientiﬁc knowledge. We test three representative\n",
            "tasks in materials chemistry: linking dopants and host materials, cataloging\n",
            "metal-organic frameworks, and general composition/phase/morphology/\n",
            "application information extraction. Records are extracted from single sen-\n",
            "tences or entire paragraphs, and the output can be returned as simple English\n",
            "sentences or a more structured format such as a list of JSON objects. This\n",
            "approach represents a simple, accessible, and highly ﬂexible route to obtain-\n",
            "ing large databases of structured specialized scientiﬁc knowledge extracted\n",
            "from research papers.\n",
            "The majority of scientiﬁc knowledge about solid-state materials is\n",
            "scattered across the text, tables, and ﬁgures of millions of academic\n",
            "research papers. Thus, it is difﬁcult for researchers to properly\n",
            "understand the full body of past work and effectively leverage existing\n",
            "knowledge when designing experiments. Moreover, machine learning\n",
            "models for direct property prediction are being increasingly employed\n",
            "as screening steps for materials discovery and design workﬂows1–3, but\n",
            "this approach is limited by the amount of training data available in\n",
            "tabulated databases. While databases of materials property data\n",
            "derived from ab initio simulations are relatively common, they are\n",
            "limited to the subset of computationally accessible properties whereas\n",
            "databases of experimental property measurements and other useful\n",
            "experimental data are comparatively small (if they exist at all).\n",
            "In recent years, researchers have made signiﬁcant advances in the\n",
            "application of natural language processing (NLP) algorithms for\n",
            "materials towards structuring the existing body of textual materials\n",
            "science knowledge4–7. The majority of this work has focused on named\n",
            "entity recognition (NER), where entity labels such as “material\" or\n",
            "“property\" are applied to words from the text. These tagged sequences\n",
            "of words can sometimes be used with additional post-processing to\n",
            "construct auto-generated tabular databases of materials property data\n",
            "aggregated from text entries8–12. Prior information extraction studies\n",
            "in the domain of solid-state materials include NER labeling of chemical\n",
            "synthesis parameters in methods section texts13–16, quantitative results\n",
            "of battery cycling experiments17, or peak absorption wavelengths for\n",
            "UV-Vis experiments18, among others4,5,9–12,19. Regular expressions,\n",
            "BiLSTM\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Extracting Structured Knowledge from Scientific Text\",\n",
            "  \"Authors\": [\"John Dagdelen\", \"Alexander Dunn\", \"Sanghoon Lee\", \"Nicholas Walker\", \"Andrew S. Rosen\", \"Gerbrand Ceder\", \"Kristin A. Persson\", \"Anubhav Jain\"],\n",
            "  \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "  \"Keywords\": [\"structured knowledge\", \"large language models\", \"materials science\", \"natural language processing\", \"machine learning\", \"solid state materials\"],\n",
            "  \"Abstract\": \"This article presents a simple approach to joint named entity recognition and relation extraction using pre-trained large language models (GPT-3, Llama-2). It demonstrates how these models can be fine-tuned to extract useful records of complex scientific knowledge from research papers. Three tasks in materials chemistry are tested: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. The output can be returned as simple English sentences or a more structured format like JSON.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Structured Knowledge from Scientific Text\",\n",
            "    \"Authors\": [\n",
            "        \"John Dagdelen\",\n",
            "        \"Alexander Dunn\",\n",
            "        \"Sanghoon Lee\",\n",
            "        \"Nicholas Walker\",\n",
            "        \"Andrew S. Rosen\",\n",
            "        \"Gerbrand Ceder\",\n",
            "        \"Kristin A. Persson\",\n",
            "        \"Anubhav Jain\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "    \"Keywords\": [\n",
            "        \"structured knowledge\",\n",
            "        \"large language models\",\n",
            "        \"materials science\",\n",
            "        \"natural language processing\",\n",
            "        \"machine learning\",\n",
            "        \"solid state materials\"\n",
            "    ],\n",
            "    \"Abstract\": \"This article presents a simple approach to joint named entity recognition and relation extraction using pre-trained large language models (GPT-3, Llama-2). It demonstrates how these models can be fine-tuned to extract useful records of complex scientific knowledge from research papers. Three tasks in materials chemistry are tested: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. The output can be returned as simple English sentences or a more structured format like JSON.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"<|im_start|>assistant\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
        "\n",
        "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:3000]}\"\"\"  # Limiting input for context window safety\n",
        "\n",
        "    return (\n",
        "        \"<|im_start|>system\\n\"\n",
        "        \"You are a helpful assistant that extracts structured metadata from scientific papers.\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>user\\n\"\n",
        "        f\"{instruction.strip()}\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\"\n",
        "    )\n",
        "\n",
        "# Generate metadata from paper text\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False,temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "\n",
        "pdf_path = \"/content/11.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1U4Bn1s4If8T",
        "outputId": "c848a827-d82d-4083-c824-36bd88b885ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Article\n",
            "https://doi.org/10.1038/s41467-024-45563-x\n",
            "Structured information extraction from\n",
            "scientiﬁc text with large language models\n",
            "John Dagdelen\n",
            "1,2,3, Alexander Dunn\n",
            "1,2,3, Sanghoon Lee1,2, Nicholas Walker1,\n",
            "Andrew S. Rosen\n",
            "1,2, Gerbrand Ceder1,2, Kristin A. Persson\n",
            "1,2 &\n",
            "Anubhav Jain\n",
            "1\n",
            "Extracting structured knowledge from scientiﬁc text remains a challenging\n",
            "task for machine learning models. Here, we present a simple approach to joint\n",
            "named entity recognition and relation extraction and demonstrate how pre-\n",
            "trained large language models (GPT-3, Llama-2) can be ﬁne-tuned to extract\n",
            "useful records of complex scientiﬁc knowledge. We test three representative\n",
            "tasks in materials chemistry: linking dopants and host materials, cataloging\n",
            "metal-organic frameworks, and general composition/phase/morphology/\n",
            "application information extraction. Records are extracted from single sen-\n",
            "tences or entire paragraphs, and the output can be returned as simple English\n",
            "sentences or a more structured format such as a list of JSON objects. This\n",
            "approach represents a simple, accessible, and highly ﬂexible route to obtain-\n",
            "ing large databases of structured specialized scientiﬁc knowledge extracted\n",
            "from research papers.\n",
            "The majority of scientiﬁc knowledge about solid-state materials is\n",
            "scattered across the text, tables, and ﬁgures of millions of academic\n",
            "research papers. Thus, it is difﬁcult for researchers to properly\n",
            "understand the full body of past work and effectively leverage existing\n",
            "knowledge when designing experiments. Moreover, machine learning\n",
            "models for direct property prediction are being increasingly employed\n",
            "as screening steps for materials discovery and design workﬂows1–3, but\n",
            "this approach is limited by the amount of training data available in\n",
            "tabulated databases. While databases of materials property data\n",
            "derived from ab initio simulations are relatively common, they are\n",
            "limited to the subset of computationally accessible properties whereas\n",
            "databases of experimental property measurements and other useful\n",
            "experimental data are comparatively small (if they exist at all).\n",
            "In recent years, researchers have made signiﬁcant advances in the\n",
            "application of natural language processing (NLP) algorithms for\n",
            "materials towards structuring the existing body of textual materials\n",
            "science knowledge4–7. The majority of this work has focused on named\n",
            "entity recognition (NER), where entity labels such as “material\" or\n",
            "“property\" are applied to words from the text. These tagged sequences\n",
            "of words can sometimes be used with additional post-processing to\n",
            "construct auto-generated tabular databases of materials property data\n",
            "aggregated from text entries8–12. Prior information extraction studies\n",
            "in the domain of solid-state materials include NER labeling of chemical\n",
            "synthesis parameters in methods section texts13–16, quantitative results\n",
            "of battery cycling experiments17, or peak absorption wavelengths for\n",
            "UV-Vis experiments18, among others4,5,9–12,19. Regular expressions,\n",
            "BiLSTM\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Extracting Structured Knowledge from Scientific Text\",\n",
            "  \"Authors\": [\"John Dagdelen\", \"Alexander Dunn\", \"Sanghoon Lee\", \"Nicholas Walker\", \"Andrew S. Rosen\", \"Gerbrand Ceder\", \"Kristin A. Persson\", \"Anubhav Jain\"],\n",
            "  \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "  \"Keywords\": [\"structured knowledge\", \"large language models\", \"materials science\", \"natural language processing\", \"machine learning\", \"solid state materials\"],\n",
            "  \"Abstract\": \"This article presents a simple approach to joint named entity recognition and relation extraction using pre-trained large language models (GPT-3, Llama-2). It demonstrates how these models can be fine-tuned to extract useful records of complex scientific knowledge from research papers. Three tasks in materials chemistry are tested: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. The output can be returned as simple English sentences or a more structured format like JSON.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Structured Knowledge from Scientific Text\",\n",
            "    \"Authors\": [\n",
            "        \"John Dagdelen\",\n",
            "        \"Alexander Dunn\",\n",
            "        \"Sanghoon Lee\",\n",
            "        \"Nicholas Walker\",\n",
            "        \"Andrew S. Rosen\",\n",
            "        \"Gerbrand Ceder\",\n",
            "        \"Kristin A. Persson\",\n",
            "        \"Anubhav Jain\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "    \"Keywords\": [\n",
            "        \"structured knowledge\",\n",
            "        \"large language models\",\n",
            "        \"materials science\",\n",
            "        \"natural language processing\",\n",
            "        \"machine learning\",\n",
            "        \"solid state materials\"\n",
            "    ],\n",
            "    \"Abstract\": \"This article presents a simple approach to joint named entity recognition and relation extraction using pre-trained large language models (GPT-3, Llama-2). It demonstrates how these models can be fine-tuned to extract useful records of complex scientific knowledge from research papers. Three tasks in materials chemistry are tested: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. The output can be returned as simple English sentences or a more structured format like JSON.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"<|im_start|>assistant\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                json_str = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', json_str)\n",
        "\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
        "\n",
        "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:3000]}\"\"\"  # Limiting input for context window safety\n",
        "\n",
        "    return (\n",
        "        \"<|im_start|>system\\n\"\n",
        "        \"You are a helpful assistant that extracts structured metadata from scientific papers.\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>user\\n\"\n",
        "        f\"{instruction.strip()}\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\"\n",
        "    )\n",
        "\n",
        "# Generate metadata from paper text\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False,temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "\n",
        "    # Replace this with your actual PDF file path\n",
        "pdf_path = \"/content/12.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(metadata)\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1fsGnIZKezm",
        "outputId": "f6a972c9-08a7-402f-e3e2-21f8adaaeb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "AI Open 00 (2023) 1–17\n",
            "AI Open\n",
            "Information Retrieval Meets Large Language Models: A Strategic\n",
            "Report from Chinese IR Community\n",
            "Qingyao AIa, Ting BAIb, Zhao CAOc, Yi CHANGd, Jiawei CHEN(\u0000)e, Zhumin CHENf , Zhiyong CHENGg,\n",
            "Shoubin DONGh, Zhicheng DOUi, Fuli FENG j, Shen GAO f , Jiafeng GUOk, Xiangnan HE(\u0000) j, Yanyan LANa,\n",
            "Chenliang LIl, Yiqun LIUa, Ziyu LYUm, Weizhi MAa, Jun MAf , Zhaochun REN f , Pengjie REN f , Zhiqiang\n",
            "WANGn, Mingwen WANGo, Ji-Rong WENi, Le WUp, Xin XIN f , Jun XUi, Dawei YINq, Peng ZHANG(\u0000)r,\n",
            "Fan ZHANGl, Weinan ZHANGs, Min ZHANGa, Xiaofei ZHUt\n",
            "aTsinghua University, bBeijing University of Posts and Telecommunications, cHuawei Technologies Ltd. Co, dJilin\n",
            "University, eZhejiang University, fShandong University, gShandong Artificial Intelligence Institute, hSouth China\n",
            "University of Technology, iRenmin University of China, jUniversity of Science and Technology of China, kInstitute\n",
            "of Computing Technology, Chinese Academy of Sciences, lWuhan University, mShenzhen Institute of Advanced\n",
            "Technology, Chinese Academy of Sciences, nShanxi University, oJiangxi Normal University, pHefei University of\n",
            "Technology, qBaidu Inc., rTianjin University, sShanghai Jiao Tong University, tChongqing University of\n",
            "Technology\n",
            "Abstract\n",
            "The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user\n",
            "information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding,\n",
            "generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval\n",
            "but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the\n",
            "synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for infor-\n",
            "mation seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a\n",
            "central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, includ-\n",
            "ing computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the\n",
            "transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding\n",
            "valuable insights. This paper provides a summary of the workshop’s outcomes, including the rethinking of IR’s core values, the\n",
            "mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.\n",
            "© 2011 Published by Elsevier Ltd.\n",
            "Keywords: Information Retrieval, Language Language Models, Recommendation System\n",
            "1. Introduction\n",
            "In the past few decades, Information Retrieval (IR) has experienced significant growth and development in both\n",
            "industry and academia. In early stage, IR research mainly focused on search, which aimed to assist users in finding\n",
            "1Email: slee\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"AI Open 00 (2023) 1–17\",\n",
            "  \"Authors\": [\"Qingyao AIa\", \"Ting BAIb\", \"Zhao CAOc\", \"Yi CHANGd\", \"Jiawei CHEN(\u0000)\", \"Zhumin CHENf \", \"Zhiyong CHENGg\", \"Shoubin DONGh\", \"Zhicheng DOUi\", \"Fuli FENG j\", \"Shen GAO f \", \"Jiafeng GUOk\", \"Xiangnan HE(\u0000)\", \"Yanyan LANa\", \"Chenliang LIl\", \"Yiqun LIUa\", \"Ziyu LYUm\", \"Weizhi MAa\", \"Jun MAf \", \"Zhaochun REN f \", \"Pengjie REN f \", \"Zhiqiang WANGn\", \"Mingwen WANGo\", \"Ji-Rong WENi\", \"Le WUp\", \"Xin XIN f \", \"Jun XUi\", \"Dawei YINq\", \"Peng ZHANG(\u0000)\", \"Fan ZHANGl\", \"Weinan ZHANGs\", \"Min ZHANGa\", \"Xiaofei ZHUt\"],\n",
            "  \"DOI\": \"\",\n",
            "  \"Keywords\": [\"Information Retrieval\", \"Language Language Models\", \"Recommendation System\"],\n",
            "  \"Abstract\": \"The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop’s outcomes, including the rethinking of IR’s core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{'Title': 'AI Open 00 (2023) 1–17', 'Authors': ['Qingyao AIa', 'Ting BAIb', 'Zhao CAOc', 'Yi CHANGd', 'Jiawei CHEN( )', 'Zhumin CHENf ', 'Zhiyong CHENGg', 'Shoubin DONGh', 'Zhicheng DOUi', 'Fuli FENG j', 'Shen GAO f ', 'Jiafeng GUOk', 'Xiangnan HE( )', 'Yanyan LANa', 'Chenliang LIl', 'Yiqun LIUa', 'Ziyu LYUm', 'Weizhi MAa', 'Jun MAf ', 'Zhaochun REN f ', 'Pengjie REN f ', 'Zhiqiang WANGn', 'Mingwen WANGo', 'Ji-Rong WENi', 'Le WUp', 'Xin XIN f ', 'Jun XUi', 'Dawei YINq', 'Peng ZHANG( )', 'Fan ZHANGl', 'Weinan ZHANGs', 'Min ZHANGa', 'Xiaofei ZHUt'], 'DOI': '', 'Keywords': ['Information Retrieval', 'Language Language Models', 'Recommendation System'], 'Abstract': 'The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop’s outcomes, including the rethinking of IR’s core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.', 'Document Type': 'Research Paper', 'Number of References': 10}\n",
            "{\n",
            "    \"Title\": \"AI Open 00 (2023) 1\\u201317\",\n",
            "    \"Authors\": [\n",
            "        \"Qingyao AIa\",\n",
            "        \"Ting BAIb\",\n",
            "        \"Zhao CAOc\",\n",
            "        \"Yi CHANGd\",\n",
            "        \"Jiawei CHEN( )\",\n",
            "        \"Zhumin CHENf \",\n",
            "        \"Zhiyong CHENGg\",\n",
            "        \"Shoubin DONGh\",\n",
            "        \"Zhicheng DOUi\",\n",
            "        \"Fuli FENG j\",\n",
            "        \"Shen GAO f \",\n",
            "        \"Jiafeng GUOk\",\n",
            "        \"Xiangnan HE( )\",\n",
            "        \"Yanyan LANa\",\n",
            "        \"Chenliang LIl\",\n",
            "        \"Yiqun LIUa\",\n",
            "        \"Ziyu LYUm\",\n",
            "        \"Weizhi MAa\",\n",
            "        \"Jun MAf \",\n",
            "        \"Zhaochun REN f \",\n",
            "        \"Pengjie REN f \",\n",
            "        \"Zhiqiang WANGn\",\n",
            "        \"Mingwen WANGo\",\n",
            "        \"Ji-Rong WENi\",\n",
            "        \"Le WUp\",\n",
            "        \"Xin XIN f \",\n",
            "        \"Jun XUi\",\n",
            "        \"Dawei YINq\",\n",
            "        \"Peng ZHANG( )\",\n",
            "        \"Fan ZHANGl\",\n",
            "        \"Weinan ZHANGs\",\n",
            "        \"Min ZHANGa\",\n",
            "        \"Xiaofei ZHUt\"\n",
            "    ],\n",
            "    \"DOI\": \"\",\n",
            "    \"Keywords\": [\n",
            "        \"Information Retrieval\",\n",
            "        \"Language Language Models\",\n",
            "        \"Recommendation System\"\n",
            "    ],\n",
            "    \"Abstract\": \"The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop\\u2019s outcomes, including the rethinking of IR\\u2019s core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"<|im_start|>assistant\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                json_str = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', json_str)\n",
        "\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
        "\n",
        "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:3000]}\"\"\"  # Limiting input for context window safety\n",
        "\n",
        "    return (\n",
        "        \"<|im_start|>system\\n\"\n",
        "        \"You are a helpful assistant that extracts structured metadata from scientific papers.\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>user\\n\"\n",
        "        f\"{instruction.strip()}\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\"\n",
        "    )\n",
        "\n",
        "# Generate metadata from paper text\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False,temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "pdf_path = \"/content/14.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(metadata)\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5YzTjJP5PLfo",
        "outputId": "21388f92-3844-45bb-b8ac-cc3dcaf2417e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "LAME: Layout-Aware Metadata Extraction Approach for Research Articles \n",
            " \n",
            "JONGYUN CHOI1, HYESOO KONG2, HWAMOOK YOON2, HEUNG-SEON OH3, \n",
            "and YUCHUL JUNG1* \n",
            "1Department of Computer Engineering, Kumoh National Institute of Technology (KIT), Gumi, South Korea \n",
            "2Korea Institute of Science and Technology Information (KISTI), South Korea \n",
            "3School of Computer Science and Engineering, Korea University of Technology and Education (KOREATECH),  \n",
            "sCheonan, South Korea \n",
            " \n",
            "Abstract: The volume of academic literature, such as academic conference \n",
            "papers and journals, has increased rapidly worldwide, and research on metadata \n",
            "extraction is ongoing. However, high-performing metadata extraction is still \n",
            "challenging due to diverse layout formats according to journal publishers. To \n",
            "accommodate the diversity of the layouts of academic journals, we propose a \n",
            "novel LAyout-aware Metadata Extraction (LAME) framework equipped with the \n",
            "three characteristics (e.g., design of an automatic layout analysis, construction of \n",
            "a large meta-data training set, and construction of Layout-MetaBERT). We \n",
            "designed an automatic layout analysis using PDFMiner. Based on the layout \n",
            "analysis, a large volume of metadata-separated training data, including the title, \n",
            "abstract, author name, author affiliated organization, and keywords, were \n",
            "automatically extracted. Moreover, we constructed Layout-MetaBERT to extract \n",
            "the metadata from academic journals with varying layout formats. The \n",
            "experimental results with Layout-MetaBERT exhibited robust performance \n",
            "(Macro-F1, 93.27%) in metadata extraction for unseen journals with different \n",
            "layout formats. \n",
            "Keywords: Automatic layout analysis, Layout-MetaBERT, Metadata extraction, \n",
            "Research article \n",
            "1 Introduction \n",
            "With the development of science and technology, the number of related academic papers distributed \n",
            "periodically worldwide has reached more than several hundred thousand. However, their layout styles are \n",
            "as diverse as their subjects and publishers although the portable document format (PDF) is widely used \n",
            "globally as a standardized text-based document provision format. For example, the information order is \n",
            "inconsistent when converting such a document to text because no layout information separating the \n",
            "document content is provided. Thus, extracting meaningful information such as metadata, including title, \n",
            "author names, affiliations, abstract, and keywords, from a document is quite challenging. \n",
            "Research on extracting metadata or document objects from PDF documents using machine learning \n",
            "has increased [1-7]. In aspects of natural language processing (NLP) approach, Open-source software, \n",
            "such as Content ExtRactor and MINEr (CERMINE) [4] and GeneRation of Bibliographic Data (GROBID) \n",
            "[5], automatically extract metadata using the sequential labeling technique but generally do not take the \n",
            "layouts into account in detail. Therefore, they do not show reasonable metadata extraction performances \n",
            "for every research article\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"LAME: Layout-Aware Metadata Extraction Approach for Research Articles\",\n",
            "  \"Authors\": [\"JONGYUN CHOI\", \"HYESOO KONG\", \"HWAMOOK YOON\", \"HEUNG-SEON OH\", \"YUCHUL JUNG\"],\n",
            "  \"DOI\": \"\",\n",
            "  \"Keywords\": [\n",
            "    \"Automatic layout analysis\",\n",
            "    \"Layout-MetaBERT\",\n",
            "    \"Metadata extraction\",\n",
            "    \"Research article\"\n",
            "  ],\n",
            "  \"Abstract\": \"The volume of academic literature, such as academic conference papers and journals, has increased rapidly worldwide, and research on metadata extraction is ongoing. However, high-performing metadata extraction is still challenging due to diverse layout formats according to journal publishers. To accommodate the diversity of the layouts of academic journals, we propose a novel LAyout-aware Metadata Extraction (LAME) framework equipped with the three characteristics (e.g., design of an automatic layout analysis, construction of a large meta-data training set, and construction of Layout-MetaBERT). We designed an automatic layout analysis using PDFMiner. Based on the layout analysis, a large volume of metadata-separated training data, including the title, abstract, author name, author affiliated organization, and keywords, were automatically extracted. Moreover, we constructed Layout-MetaBERT to extract the metadata from academic journals with varying layout formats. The experimental results with Layout-MetaBERT exhibited robust performance (Macro-F1, 93.27%) in metadata extraction for unseen journals with different layout formats.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{'Title': 'LAME: Layout-Aware Metadata Extraction Approach for Research Articles', 'Authors': ['JONGYUN CHOI', 'HYESOO KONG', 'HWAMOOK YOON', 'HEUNG-SEON OH', 'YUCHUL JUNG'], 'DOI': '', 'Keywords': ['Automatic layout analysis', 'Layout-MetaBERT', 'Metadata extraction', 'Research article'], 'Abstract': 'The volume of academic literature, such as academic conference papers and journals, has increased rapidly worldwide, and research on metadata extraction is ongoing. However, high-performing metadata extraction is still challenging due to diverse layout formats according to journal publishers. To accommodate the diversity of the layouts of academic journals, we propose a novel LAyout-aware Metadata Extraction (LAME) framework equipped with the three characteristics (e.g., design of an automatic layout analysis, construction of a large meta-data training set, and construction of Layout-MetaBERT). We designed an automatic layout analysis using PDFMiner. Based on the layout analysis, a large volume of metadata-separated training data, including the title, abstract, author name, author affiliated organization, and keywords, were automatically extracted. Moreover, we constructed Layout-MetaBERT to extract the metadata from academic journals with varying layout formats. The experimental results with Layout-MetaBERT exhibited robust performance (Macro-F1, 93.27%) in metadata extraction for unseen journals with different layout formats.', 'Document Type': 'Research Paper', 'Number of References': 10}\n",
            "{\n",
            "    \"Title\": \"LAME: Layout-Aware Metadata Extraction Approach for Research Articles\",\n",
            "    \"Authors\": [\n",
            "        \"JONGYUN CHOI\",\n",
            "        \"HYESOO KONG\",\n",
            "        \"HWAMOOK YOON\",\n",
            "        \"HEUNG-SEON OH\",\n",
            "        \"YUCHUL JUNG\"\n",
            "    ],\n",
            "    \"DOI\": \"\",\n",
            "    \"Keywords\": [\n",
            "        \"Automatic layout analysis\",\n",
            "        \"Layout-MetaBERT\",\n",
            "        \"Metadata extraction\",\n",
            "        \"Research article\"\n",
            "    ],\n",
            "    \"Abstract\": \"The volume of academic literature, such as academic conference papers and journals, has increased rapidly worldwide, and research on metadata extraction is ongoing. However, high-performing metadata extraction is still challenging due to diverse layout formats according to journal publishers. To accommodate the diversity of the layouts of academic journals, we propose a novel LAyout-aware Metadata Extraction (LAME) framework equipped with the three characteristics (e.g., design of an automatic layout analysis, construction of a large meta-data training set, and construction of Layout-MetaBERT). We designed an automatic layout analysis using PDFMiner. Based on the layout analysis, a large volume of metadata-separated training data, including the title, abstract, author name, author affiliated organization, and keywords, were automatically extracted. Moreover, we constructed Layout-MetaBERT to extract the metadata from academic journals with varying layout formats. The experimental results with Layout-MetaBERT exhibited robust performance (Macro-F1, 93.27%) in metadata extraction for unseen journals with different layout formats.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/15.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(metadata)\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P6oR8yJyuLiA",
        "outputId": "038a3853-7d78-4720-9192-74056084d85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "\u0001\u0002\u0003\u0001\u0004\u0005\u0006\u0007\b\n",
            "\u0001\u0002\u0003\u0004\u0005\u0006\u0007\n",
            "Citation: Kim, H.; Choi, J.; Park, S.;\n",
            "Jung, Y. Layout Aware Semantic\n",
            "Element Extraction for Sustainable\n",
            "Science & Technology Decision\n",
            "Support. Sustainability 2022, 14, 2802.\n",
            "https://doi.org/10.3390/su14052802\n",
            "Academic Editor: Hamid Khayyam\n",
            "Received: 13 January 2022\n",
            "Accepted: 22 February 2022\n",
            "Published: 28 February 2022\n",
            "Publisher’s Note: MDPI stays neutral\n",
            "with regard to jurisdictional claims in\n",
            "published maps and institutional afﬁl-\n",
            "iations.\n",
            "Copyright:\n",
            "© 2022 by the authors.\n",
            "Licensee MDPI, Basel, Switzerland.\n",
            "This article is an open access article\n",
            "distributed\n",
            "under\n",
            "the\n",
            "terms\n",
            "and\n",
            "conditions of the Creative Commons\n",
            "Attribution (CC BY) license (https://\n",
            "creativecommons.org/licenses/by/\n",
            "4.0/).\n",
            "sustainability\n",
            "Article\n",
            "Layout Aware Semantic Element Extraction for Sustainable\n",
            "Science & Technology Decision Support\n",
            "Hyuntae Kim\n",
            ", Jongyun Choi, Soyoung Park and Yuchul Jung *\n",
            "Department of Computer Engineering, Kumoh National Institute of Technology, Gumi 39177, Korea;\n",
            "20216035@kumoh.ac.kr (H.K.); 20216093@kumoh.ac.kr (J.C.); haluna8836@kumoh.ac.kr (S.P.)\n",
            "* Correspondence: jyc@kumoh.ac.kr; Tel.: +82-54-478-7536\n",
            "Abstract: New scientiﬁc and technological (S&T) knowledge is being introduced rapidly, and hence,\n",
            "analysis efforts to understand and analyze new published S&T documents are increasing daily.\n",
            "Automated text mining and vision recognition techniques alleviate the burden somewhat, but the\n",
            "various document layout formats and knowledge content granularities across the S&T ﬁeld make\n",
            "it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph\n",
            "construction framework that simultaneously extracts meta-information and useful image objects from\n",
            "S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME),\n",
            "which can accurately extract metadata from various layout formats, and implement a transformer-\n",
            "based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize\n",
            "the vision-based semantic element recognition. Moreover, to constructing a scientiﬁc knowledge\n",
            "graph consisting of multiple S&T documents, we newly deﬁned an extensible Semantic Elements\n",
            "Knowledge Graph (SEKG) structure. For now, we succeeded in extracting about 6 million semantic\n",
            "elements from 49,649 PDFs. In addition, to illustrate the potential power of our SEKG, we provide two\n",
            "promising application scenarios, such as a scientiﬁc knowledge guide across multiple S&T documents\n",
            "and questions and answering over scientiﬁc tables.\n",
            "Keywords: multi-modal; document layout analysis; metadata; document structure; document object;\n",
            "semantic elements; knowledge graph; transformer; decision support\n",
            "1. Introduction\n",
            "Decision support systems or speciﬁc methods for science and technology (S&T) prob-\n",
            "lems or social issues can be employed effectively across various domain user types related\n",
            "to policymaking, research topic search, research method survey, comparing experimental\n",
            "results, emerging technology trend an\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Layout Aware Semantic Element Extraction for Sustainable Science & Technology Decision Support\",\n",
            "  \"Authors\": [\"Hyuntae Kim\", \"Jongyun Choi\", \"Soyoung Park\", \"Yuchul Jung\"],\n",
            "  \"DOI\": \"https://doi.org/10.3390/su14052802\",\n",
            "  \"Keywords\": [\n",
            "    \"multi-modal\",\n",
            "    \"document layout analysis\",\n",
            "    \"metadata\",\n",
            "    \"document structure\",\n",
            "    \"document object\",\n",
            "    \"semantic elements\",\n",
            "    \"knowledge graph\",\n",
            "    \"transformer\",\n",
            "    \"decision support\"\n",
            "  ],\n",
            "  \"Abstract\": \"New scientiﬁc and technological (S&T) knowledge is being introduced rapidly, and hence, analysis efforts to understand and analyze new published S&T documents are increasing daily. Automated text mining and vision recognition techniques alleviate the burden somewhat, but the various document layout formats and knowledge content granularities across the S&T field make it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph construction framework that simultaneously extracts meta-information and useful image objects from S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME), which can accurately extract metadata from various layout formats, and implement a transformer-based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize the vision-based semantic element recognition. Moreover, to constructing a scientiﬁc knowledge graph consisting of multiple S&T documents, we newly defined an extensible Semantic Elements Knowledge Graph (SEKG) structure. For now, we succeeded in extracting about 6 million semantic elements from 49,649 PDFs. In addition, to illustrate the potential power of our SEKG, we provide two promising application scenarios, such as a scientiﬁc knowledge guide across multiple S&T documents and questions and answering over scientiﬁc tables.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{'Title': 'Layout Aware Semantic Element Extraction for Sustainable Science & Technology Decision Support', 'Authors': ['Hyuntae Kim', 'Jongyun Choi', 'Soyoung Park', 'Yuchul Jung'], 'DOI': 'https://doi.org/10.3390/su14052802', 'Keywords': ['multi-modal', 'document layout analysis', 'metadata', 'document structure', 'document object', 'semantic elements', 'knowledge graph', 'transformer', 'decision support'], 'Abstract': 'New scientiﬁc and technological (S&T) knowledge is being introduced rapidly, and hence, analysis efforts to understand and analyze new published S&T documents are increasing daily. Automated text mining and vision recognition techniques alleviate the burden somewhat, but the various document layout formats and knowledge content granularities across the S&T field make it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph construction framework that simultaneously extracts meta-information and useful image objects from S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME), which can accurately extract metadata from various layout formats, and implement a transformer-based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize the vision-based semantic element recognition. Moreover, to constructing a scientiﬁc knowledge graph consisting of multiple S&T documents, we newly defined an extensible Semantic Elements Knowledge Graph (SEKG) structure. For now, we succeeded in extracting about 6 million semantic elements from 49,649 PDFs. In addition, to illustrate the potential power of our SEKG, we provide two promising application scenarios, such as a scientiﬁc knowledge guide across multiple S&T documents and questions and answering over scientiﬁc tables.', 'Document Type': 'Research Paper', 'Number of References': 10}\n",
            "{\n",
            "    \"Title\": \"Layout Aware Semantic Element Extraction for Sustainable Science & Technology Decision Support\",\n",
            "    \"Authors\": [\n",
            "        \"Hyuntae Kim\",\n",
            "        \"Jongyun Choi\",\n",
            "        \"Soyoung Park\",\n",
            "        \"Yuchul Jung\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.3390/su14052802\",\n",
            "    \"Keywords\": [\n",
            "        \"multi-modal\",\n",
            "        \"document layout analysis\",\n",
            "        \"metadata\",\n",
            "        \"document structure\",\n",
            "        \"document object\",\n",
            "        \"semantic elements\",\n",
            "        \"knowledge graph\",\n",
            "        \"transformer\",\n",
            "        \"decision support\"\n",
            "    ],\n",
            "    \"Abstract\": \"New scienti\\ufb01c and technological (S&T) knowledge is being introduced rapidly, and hence, analysis efforts to understand and analyze new published S&T documents are increasing daily. Automated text mining and vision recognition techniques alleviate the burden somewhat, but the various document layout formats and knowledge content granularities across the S&T field make it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph construction framework that simultaneously extracts meta-information and useful image objects from S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME), which can accurately extract metadata from various layout formats, and implement a transformer-based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize the vision-based semantic element recognition. Moreover, to constructing a scienti\\ufb01c knowledge graph consisting of multiple S&T documents, we newly defined an extensible Semantic Elements Knowledge Graph (SEKG) structure. For now, we succeeded in extracting about 6 million semantic elements from 49,649 PDFs. In addition, to illustrate the potential power of our SEKG, we provide two promising application scenarios, such as a scienti\\ufb01c knowledge guide across multiple S&T documents and questions and answering over scienti\\ufb01c tables.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/16.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        # print(metadata)\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pt8J96UFvX7v",
        "outputId": "eeb252f0-1ca6-48c8-bee6-e9906664123a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Convolutional Neural Networks for Sentence Classiﬁcation\n",
            "Yoon Kim\n",
            "New York University\n",
            "yhk255@nyu.edu\n",
            "Abstract\n",
            "We report on a series of experiments with\n",
            "convolutional\n",
            "neural\n",
            "networks\n",
            "(CNN)\n",
            "trained on top of pre-trained word vec-\n",
            "tors for sentence-level classiﬁcation tasks.\n",
            "We show that a simple CNN with lit-\n",
            "tle hyperparameter tuning and static vec-\n",
            "tors achieves excellent results on multi-\n",
            "ple benchmarks.\n",
            "Learning task-speciﬁc\n",
            "vectors through ﬁne-tuning offers further\n",
            "gains in performance.\n",
            "We additionally\n",
            "propose a simple modiﬁcation to the ar-\n",
            "chitecture to allow for the use of both\n",
            "task-speciﬁc and static vectors. The CNN\n",
            "models discussed herein improve upon the\n",
            "state of the art on 4 out of 7 tasks, which\n",
            "include sentiment analysis and question\n",
            "classiﬁcation.\n",
            "1\n",
            "Introduction\n",
            "Deep learning models have achieved remarkable\n",
            "results in computer vision (Krizhevsky et al.,\n",
            "2012) and speech recognition (Graves et al., 2013)\n",
            "in recent years. Within natural language process-\n",
            "ing, much of the work with deep learning meth-\n",
            "ods has involved learning word vector representa-\n",
            "tions through neural language models (Bengio et\n",
            "al., 2003; Yih et al., 2011; Mikolov et al., 2013)\n",
            "and performing composition over the learned word\n",
            "vectors for classiﬁcation (Collobert et al., 2011).\n",
            "Word vectors, wherein words are projected from a\n",
            "sparse, 1-of-V encoding (here V is the vocabulary\n",
            "size) onto a lower dimensional vector space via a\n",
            "hidden layer, are essentially feature extractors that\n",
            "encode semantic features of words in their dimen-\n",
            "sions. In such dense representations, semantically\n",
            "close words are likewise close—in euclidean or\n",
            "cosine distance—in the lower dimensional vector\n",
            "space.\n",
            "Convolutional neural networks (CNN) utilize\n",
            "layers with convolving ﬁlters that are applied to\n",
            "local features (LeCun et al., 1998).\n",
            "Originally\n",
            "invented for computer vision, CNN models have\n",
            "subsequently been shown to be effective for NLP\n",
            "and have achieved excellent results in semantic\n",
            "parsing (Yih et al., 2014), search query retrieval\n",
            "(Shen et al., 2014), sentence modeling (Kalch-\n",
            "brenner et al., 2014), and other traditional NLP\n",
            "tasks (Collobert et al., 2011).\n",
            "In the present work, we train a simple CNN with\n",
            "one layer of convolution on top of word vectors\n",
            "obtained from an unsupervised neural language\n",
            "model. These vectors were trained by Mikolov et\n",
            "al. (2013) on 100 billion words of Google News,\n",
            "and are publicly available.1 We initially keep the\n",
            "word vectors static and learn only the other param-\n",
            "eters of the model. Despite little tuning of hyper-\n",
            "parameters, this simple model achieves excellent\n",
            "results on multiple benchmarks, suggesting that\n",
            "the pre-trained vectors are ‘universal’ feature ex-\n",
            "tractors that can be utilized for various classiﬁca-\n",
            "tion tasks. Learning task-speciﬁc vectors through\n",
            "ﬁne-tuning results in further improvements. We\n",
            "ﬁnally describe a simple modiﬁcation to the archi-\n",
            "tecture to allow for the use of both pre-trained and\n",
            "task-speciﬁc vectors by having multiple channels.\n",
            "Our wor\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Convolutional Neural Networks for Sentence Classification\",\n",
            "  \"Authors\": [\"Yoon Kim\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Convolutional Neural Network\", \"Sentence Classification\", \"Unsupervised Neural Language Model\", \"Google News\"],\n",
            "  \"Abstract\": \"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, including sentiment analysis and question classification.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Convolutional Neural Networks for Sentence Classification\",\n",
            "    \"Authors\": [\n",
            "        \"Yoon Kim\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Convolutional Neural Network\",\n",
            "        \"Sentence Classification\",\n",
            "        \"Unsupervised Neural Language Model\",\n",
            "        \"Google News\"\n",
            "    ],\n",
            "    \"Abstract\": \"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, including sentiment analysis and question classification.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/19.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        # print(metadata)\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSFOzx_jw4k6",
        "outputId": "3a7526f0-9e9b-47ef-ecea-2e4170aa91dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Information Processing and Management 61 (2024) 103610\n",
            "Available online 14 December 2023\n",
            "0306-4573/©\n",
            "2023\n",
            "The\n",
            "Authors.\n",
            "Published\n",
            "by\n",
            "Elsevier\n",
            "Ltd.\n",
            "This\n",
            "is\n",
            "an\n",
            "open\n",
            "access\n",
            "article\n",
            "under\n",
            "the\n",
            "CC\n",
            "BY-NC\n",
            "license\n",
            "(http://creativecommons.org/licenses/by-nc/4.0/).\n",
            "Contents lists available at ScienceDirect\n",
            "Information Processing and Management\n",
            "journal homepage: www.elsevier.com/locate/ipm\n",
            "Predicting movies’ eudaimonic and hedonic scores: A machine\n",
            "learning approach using metadata, audio and visual features\n",
            "Elham Motamedi a,∗, Danial Khosh Kholgh b, Sorush Saghari c, Mehdi Elahi d,\n",
            "Francesco Barile e, Marko Tkalcic a\n",
            "a University of Primorska, Koper, Slovenia\n",
            "b University of Oulu, Oulu, Finland\n",
            "c K N Toosi University of Technology, Tehran, Iran\n",
            "d University of Bergen, Bergen, Norway\n",
            "e Maastricht University, Maastricht, Netherlands\n",
            "A R T I C L E\n",
            "I N F O\n",
            "Keywords:\n",
            "Eudaimonia\n",
            "Hedonia\n",
            "Machine learning approach\n",
            "Movie recommender systems\n",
            "A B S T R A C T\n",
            "In the task of modeling user preferences for movie recommender systems, recent research has\n",
            "demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and\n",
            "H scores), which reflect the depth of their message and the level of fun experience they provide,\n",
            "respectively. So far, the labeling of movies with their E and H scores has been done manually\n",
            "using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue,\n",
            "we propose an automatic approach for predicting E and H scores. Specifically, we collected\n",
            "E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this\n",
            "dataset with metadata, audio, and low-level and high-level visual features, and trained machine\n",
            "learning models for predicting the E and H scores of movies. This study investigates the use\n",
            "of machine learning models in predicting the E and H scores of movies using various feature\n",
            "sets, including audio, low-level and high-level visual features, and metadata. We compared the\n",
            "performance of predictive models using different combinations of features with the majority\n",
            "classifier as the baseline approach. The results demonstrate that our proposed machine learning-\n",
            "based models significantly outperform the baseline in predicting E and H scores, particularly\n",
            "when leveraging metadata features. Specifically, the random forest classifier achieved a 20%\n",
            "increase in ROC AUC compared to the baseline when predicting both the E score and the\n",
            "H score. These improvements were found to be statistically significant. Overall, our findings\n",
            "suggest that automated tools for predicting E and H scores in movies are promising alternatives\n",
            "to traditional questionnaire-based approaches.\n",
            "1. Introduction\n",
            "Recommender system algorithms have been shown to be effective in predicting the utility of an item for a target user. In\n",
            "particular, matrix factorization and recently deep learning algorithms are very efficient in extracting user preferences from past\n",
            "behavior (e.g., clicks, purchases, views, etc.). Th\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Predicting Movies' Eudaimonic and Hedonic Scores: A Machine Learning Approach Using Metadata, Audio and Visual Features\",\n",
            "  \"Authors\": [\n",
            "    \"Elham Motamedi\",\n",
            "    \"Danial Khosh Kholgh\",\n",
            "    \"Sorush Saghari\",\n",
            "    \"Mehdi Elahi\",\n",
            "    \"Francesco Barile\",\n",
            "    \"Marko Tkalcic\"\n",
            "  ],\n",
            "  \"DOI\": \"https://doi.org/10.1016/j.ipm.2023.103610\",\n",
            "  \"Keywords\": [\n",
            "    \"Eudaimonia\",\n",
            "    \"Hedonia\",\n",
            "    \"Machine learning approach\",\n",
            "    \"Movie recommender systems\",\n",
            "    \"Metadata\",\n",
            "    \"Audio\",\n",
            "    \"Visual features\"\n",
            "  ],\n",
            "  \"Abstract\": \"In the task of modeling user preferences for movie recommender systems, recent research has demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and H scores), which reflect the depth of their message and the level of fun experience they provide, respectively. So far, the labeling of movies with their E and H scores has been done manually using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue, we propose an automatic approach for predicting E and H scores. Specifically, we collected E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this dataset with metadata, audio, and low-level and high-level visual features, and trained machine learning models for predicting the E and H scores of movies. This study investigates the use of machine learning models in predicting the E and H scores of movies using various feature sets, including audio, low-level and high-level visual features, and metadata. We compared the performance of predictive models using different combinations of features with the majority classifier as the baseline approach. The results demonstrate that our proposed machine learning-based models significantly outperform the baseline in predicting E and H scores, particularly when leveraging metadata features. Specifically, the random forest classifier achieved a 20% increase in ROC AUC compared to the baseline when predicting both the E score and the H score. These improvements were found to be statistically significant. Overall, our findings suggest that automated tools for predicting E and H scores in movies are promising alternatives to traditional questionnaire-based approaches.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Predicting Movies' Eudaimonic and Hedonic Scores: A Machine Learning Approach Using Metadata, Audio and Visual Features\",\n",
            "    \"Authors\": [\n",
            "        \"Elham Motamedi\",\n",
            "        \"Danial Khosh Kholgh\",\n",
            "        \"Sorush Saghari\",\n",
            "        \"Mehdi Elahi\",\n",
            "        \"Francesco Barile\",\n",
            "        \"Marko Tkalcic\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.1016/j.ipm.2023.103610\",\n",
            "    \"Keywords\": [\n",
            "        \"Eudaimonia\",\n",
            "        \"Hedonia\",\n",
            "        \"Machine learning approach\",\n",
            "        \"Movie recommender systems\",\n",
            "        \"Metadata\",\n",
            "        \"Audio\",\n",
            "        \"Visual features\"\n",
            "    ],\n",
            "    \"Abstract\": \"In the task of modeling user preferences for movie recommender systems, recent research has demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and H scores), which reflect the depth of their message and the level of fun experience they provide, respectively. So far, the labeling of movies with their E and H scores has been done manually using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue, we propose an automatic approach for predicting E and H scores. Specifically, we collected E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this dataset with metadata, audio, and low-level and high-level visual features, and trained machine learning models for predicting the E and H scores of movies. This study investigates the use of machine learning models in predicting the E and H scores of movies using various feature sets, including audio, low-level and high-level visual features, and metadata. We compared the performance of predictive models using different combinations of features with the majority classifier as the baseline approach. The results demonstrate that our proposed machine learning-based models significantly outperform the baseline in predicting E and H scores, particularly when leveraging metadata features. Specifically, the random forest classifier achieved a 20% increase in ROC AUC compared to the baseline when predicting both the E score and the H score. These improvements were found to be statistically significant. Overall, our findings suggest that automated tools for predicting E and H scores in movies are promising alternatives to traditional questionnaire-based approaches.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install PyMuPDF\n",
        "# import fitz\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"<|im_start|>assistant\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                json_str = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', json_str)\n",
        "\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
        "\n",
        "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:3000]}\"\"\"  # Limiting input for context window safety\n",
        "\n",
        "    return (\n",
        "        \"<|im_start|>system\\n\"\n",
        "        \"You are a helpful assistant that extracts structured metadata from scientific papers.\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>user\\n\"\n",
        "        f\"{instruction.strip()}\\n\"\n",
        "        \"<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\"\n",
        "    )\n",
        "\n",
        "# Generate metadata from paper text\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False,temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "pdf_path = \"/content/20.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if extracted_text.startswith(\"Error:\"):\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        # print(metadata)\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pa2LgOiuz9EF",
        "outputId": "4fca5db2-7ca2-4c8d-88af-b01245de7da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Contents lists available at ScienceDirect\n",
            "Information Processing and Management\n",
            "journal homepage: www.elsevier.com/locate/infoproman\n",
            "Deep Learning-based Extraction of Algorithmic Metadata in Full-\n",
            "Text Scholarly Documents\n",
            "Iqra Safdera, Saeed-Ul Hassana, Anna Visvizib, Thanapon Norasetc, Raheel Nawazd,\n",
            "Suppawong Tuarobc,⁎\n",
            "a Information Technology University, 346-B, Ferozepur Road, Lahore, Pakistan\n",
            "b Deree College - The American College of Greece, 6 Gravias Street, 153-42 Aghia Paraskevi, Athens, Greece\n",
            "c Faculty of Information and Communication Technology, Mahidol University, Thailand\n",
            "d Department of Operations, Technology Events and Hospitality Management, Manchester Metropolitan University, Manchester, United Kingdom\n",
            "A R T I C L E I N F O\n",
            "Keywords:\n",
            "Knowledge-based Systems\n",
            "Algorithmic Metadata\n",
            "Algorithm Search\n",
            "Deep Learning\n",
            "Bi-Directional LSTM\n",
            "Information Retrieval\n",
            "Full-text Articles\n",
            "A B S T R A C T\n",
            "The advancements of search engines for traditional text documents have enabled the effective\n",
            "retrieval of massive textual information in a resource-efficient manner. However, such conven-\n",
            "tional search methodologies often suffer from poor retrieval accuracy especially when documents\n",
            "exhibit unique properties that behoove specialized and deeper semantic extraction. Recently,\n",
            "AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and\n",
            "shallow textual metadata from scientific publications and treats them as traditional documents so\n",
            "that the conventional search engine methodology could be applied. However, such a system fails\n",
            "to facilitate user search queries that seek to identify algorithm-specific information, such as the\n",
            "datasets on which algorithms operate, the performance of algorithms, and runtime complexity,\n",
            "etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are\n",
            "presented. Specifically, we propose a set of methods to automatically identify and extract algo-\n",
            "rithmic pseudo-codes and the sentences that convey related algorithmic metadata using a set of\n",
            "machine-learning techniques. In an experiment with over 93,000 text lines, we introduce 60\n",
            "novel features, comprising content-based, font style based and structure-based feature groups, to\n",
            "extract algorithmic pseudo-codes. Our proposed pseudo-code extraction method achieves 93.32%\n",
            "F1-score, outperforming the state-of-the-art techniques by 28%. Additionally, we propose a\n",
            "method to extract algorithmic-related sentences using deep neural networks and achieve an\n",
            "accuracy of 78.5%, outperforming a Rule-based model and a support vector machine model by\n",
            "28% and 16%, respectively.\n",
            "1. Introduction\n",
            "Rapid growth in scholarly documents has been observed due to increasingly enormous research activities conducted by scientific\n",
            "communities both in academia and industry over time (Xie et al., 2019; Shardlow et al., 2018; Hassan et al., 2018; Xia et al., 2017).\n",
            "The advancements in web-based tools and technologies have enabled us to store and\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Deep Learning-based Extraction of Algorithmic Metadata in Full-Text Scholarly Documents\",\n",
            "  \"Authors\": [\n",
            "    \"Iqra Safdar\",\n",
            "    \"Saeed-Ul Hasan\",\n",
            "    \"Anna Vizjak\",\n",
            "    \"Thanapon Noset\",\n",
            "    \"Raheel Nawaz\",\n",
            "    \"Suppawong Tuaro\"\n",
            "  ],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\n",
            "    \"Knowledge-Based Systems\",\n",
            "    \"Algorithmic Metadata\",\n",
            "    \"Algorithm Search\",\n",
            "    \"Deep Learning\",\n",
            "    \"Bi-Directional LSTM\",\n",
            "    \"Information Retrieval\",\n",
            "    \"Full-text Articles\"\n",
            "  ],\n",
            "  \"Abstract\": \"The advancements of search engines for traditional text documents have enabled the effective retrieval of massive textual information in a resource-efficient manner. However, such conventional search methodologies often suffer from poor retrieval accuracy especially when documents exhibit unique properties that behoove specialized and deeper semantic extraction. Recently, AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and shallow textual metadata from scientific publications and treats them as traditional documents so that the conventional search engine methodology could be applied. However, such a system fails to facilitate user search queries that seek to identify algorithm-specific information, such as the datasets on which algorithms operate, the performance of algorithms, and runtime complexity, etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are presented. Specifically, we propose a set of methods to automatically identify and extract algorithmic pseudo-codes and the sentences that convey related algorithmic metadata using a set of machine-learning techniques. In an experiment with over 93,000 text lines, we introduce 60 novel features, comprising content-based, font style based and structure-based feature groups, to extract algorithmic pseudo-codes. Our proposed pseudo-code extraction method achieves 93.32% F1-score, outperforming the state-of-the-art techniques by 28%. Additionally, we propose a method to extract algorithmic-related sentences using deep neural networks and achieve an accuracy of 78.5%, outperforming a Rule-based model and a support vector machine model by 28% and 16%, respectively.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Deep Learning-based Extraction of Algorithmic Metadata in Full-Text Scholarly Documents\",\n",
            "    \"Authors\": [\n",
            "        \"Iqra Safdar\",\n",
            "        \"Saeed-Ul Hasan\",\n",
            "        \"Anna Vizjak\",\n",
            "        \"Thanapon Noset\",\n",
            "        \"Raheel Nawaz\",\n",
            "        \"Suppawong Tuaro\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Knowledge-Based Systems\",\n",
            "        \"Algorithmic Metadata\",\n",
            "        \"Algorithm Search\",\n",
            "        \"Deep Learning\",\n",
            "        \"Bi-Directional LSTM\",\n",
            "        \"Information Retrieval\",\n",
            "        \"Full-text Articles\"\n",
            "    ],\n",
            "    \"Abstract\": \"The advancements of search engines for traditional text documents have enabled the effective retrieval of massive textual information in a resource-efficient manner. However, such conventional search methodologies often suffer from poor retrieval accuracy especially when documents exhibit unique properties that behoove specialized and deeper semantic extraction. Recently, AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and shallow textual metadata from scientific publications and treats them as traditional documents so that the conventional search engine methodology could be applied. However, such a system fails to facilitate user search queries that seek to identify algorithm-specific information, such as the datasets on which algorithms operate, the performance of algorithms, and runtime complexity, etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are presented. Specifically, we propose a set of methods to automatically identify and extract algorithmic pseudo-codes and the sentences that convey related algorithmic metadata using a set of machine-learning techniques. In an experiment with over 93,000 text lines, we introduce 60 novel features, comprising content-based, font style based and structure-based feature groups, to extract algorithmic pseudo-codes. Our proposed pseudo-code extraction method achieves 93.32% F1-score, outperforming the state-of-the-art techniques by 28%. Additionally, we propose a method to extract algorithmic-related sentences using deep neural networks and achieve an accuracy of 78.5%, outperforming a Rule-based model and a support vector machine model by 28% and 16%, respectively.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/13.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if extracted_text.startswith(\"Error:\"):\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        # print(metadata)\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O2-ZtmIleaLb",
        "outputId": "f495ffa6-4fd7-4a3c-b68a-a50e2730f879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "RESEARCH ARTICLE\n",
            "Building an annotated corpus for automatic\n",
            "metadata extraction from multilingual journal\n",
            "article references\n",
            "Wonjun Choi, Hwa-Mook Yoon, Mi-Hwan Hyun, Hye-Jin Lee, Jae-Wook Seol, Kangsan\n",
            "Dajeong Lee, Young Joon Yoon, Hyesoo KongID*\n",
            "Digital Curation Center, Korea Institute of Science and Technology Information, Daejeon, Republic of Korea\n",
            "* hyesoo@kisti.re.kr\n",
            "Abstract\n",
            "Bibliographic references containing citation information of academic literature play an impor-\n",
            "tant role as a medium connecting earlier and recent studies. As references contain\n",
            "machine-readable metadata such as author name, title, or publication year, they have been\n",
            "widely used in the field of citation information services including search services for scholarly\n",
            "information and research trend analysis. Many institutions around the world manually\n",
            "extract and continuously accumulate reference metadata to provide various scholarly ser-\n",
            "vices. However, manually collection of reference metadata every year continues to be a bur-\n",
            "den because of the associated cost and time consumption. With the accumulation of a large\n",
            "volume of academic literature, several tools, including GROBID and CERMINE, that auto-\n",
            "matically extract reference metadata have been released. However, these tools have some\n",
            "limitations. For example, they are only applicable to references written in English, the types\n",
            "of extractable metadata are limited for each tool, and the performance of the tools is insuffi-\n",
            "cient to replace the manual extraction of reference metadata. Therefore, in this study, we\n",
            "focused on constructing a high-quality corpus to automatically extract metadata from multi-\n",
            "lingual journal article references. Using our constructed corpus, we trained and evaluated a\n",
            "BERT-based transfer-learning model. Furthermore, we compared the performance of the\n",
            "BERT-based model with that of the existing model, GROBID. Currently, our corpus contains\n",
            "3,815,987 multilingual references, mainly in English and Korean, with labels for 13 different\n",
            "metadata types. According to our experiment, the BERT-based model trained using our cor-\n",
            "pus showed excellent performance in extracting metadata not only from journal references\n",
            "written in English but also in other languages, particularly Korean. This corpus is available at\n",
            "http://doi.org/10.23057/47.\n",
            "Introduction\n",
            "Bibliographic references are citations of previous studies that authors refer to while conducting\n",
            "their own studies. These references typically appear at the end of scientific articles. They con-\n",
            "tain valuable meta-information such as the author name, title, journal name, and publication\n",
            "PLOS ONE\n",
            "PLOS ONE | https://doi.org/10.1371/journal.pone.0280637\n",
            "January 20, 2023\n",
            "1 / 22\n",
            "a1111111111\n",
            "a1111111111\n",
            "a1111111111\n",
            "a1111111111\n",
            "a1111111111\n",
            "OPEN ACCESS\n",
            "Citation: Choi W, Yoon H-M, Hyun M-H, Lee H-J,\n",
            "Seol J-W, Lee KD, et al. (2023) Building an\n",
            "annotated corpus for automatic metadata\n",
            "extraction from multilingual journal article\n",
            "references. PLoS ONE 18(1): e0280637. http\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Building an annotated corpus for automatic metadata extraction from multilingual journal article references\",\n",
            "  \"Authors\": [\"Wonjun Choi\", \"Hwa-Mook Yoon\", \"Mi-Hwan Hyun\", \"Hye-Jin Lee\", \"Jae-Wook Seol\", \"Kangsan Dajeong Lee\", \"Young Joon Yoon\", \"Hyesoo KongID*\"],\n",
            "  \"DOI\": \"https://doi.org/10.23057/47\",\n",
            "  \"Keywords\": [\"multilingual\", \"journal\", \"reference\", \"metadata\", \"annotation\"],\n",
            "  \"Abstract\": \"Bibliographic references containing citation information of academic literature play an important role as a medium connecting earlier and recent studies. As references contain machine-readable metadata such as author name, title, or publication year, they have been widely used in the field of citation information services including search services for scholarly information and research trend analysis. Many institutions around the world manually extract and continuously accumulate reference metadata to provide various scholarly services. However, manually collection of reference metadata every year continues to be a burden because of the associated cost and time consumption. With the accumulation of a large volume of academic literature, several tools, including GROBID and CERMINE, that automatically extract reference metadata have been released. However, these tools have some limitations. For example, they are only applicable to references written in English, the types of extractable metadata are limited for each tool, and the performance of the tools is insufficient to replace the manual extraction of reference metadata. Therefore, in this study, we focused on constructing a high-quality corpus to automatically extract metadata from multilingual journal article references. Using our constructed corpus, we trained and evaluated a BERT-based transfer-learning model. Furthermore, we compared the performance of the BERT-based model with that of the existing model, GROBID. Currently, our corpus contains 3,815,987 multilingual references, mainly in English and Korean, with labels for 13 different metadata types. According to our experiment, the BERT-based model trained using our corpus showed excellent performance in extracting metadata not only from journal references written in English but also in other languages, particularly Korean. This corpus is available at http://doi.org/10.23057/47.\",\n",
            "  \"Document Type\": \"Research Article\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Building an annotated corpus for automatic metadata extraction from multilingual journal article references\",\n",
            "    \"Authors\": [\n",
            "        \"Wonjun Choi\",\n",
            "        \"Hwa-Mook Yoon\",\n",
            "        \"Mi-Hwan Hyun\",\n",
            "        \"Hye-Jin Lee\",\n",
            "        \"Jae-Wook Seol\",\n",
            "        \"Kangsan Dajeong Lee\",\n",
            "        \"Young Joon Yoon\",\n",
            "        \"Hyesoo KongID*\"\n",
            "    ],\n",
            "    \"DOI\": \"https://doi.org/10.23057/47\",\n",
            "    \"Keywords\": [\n",
            "        \"multilingual\",\n",
            "        \"journal\",\n",
            "        \"reference\",\n",
            "        \"metadata\",\n",
            "        \"annotation\"\n",
            "    ],\n",
            "    \"Abstract\": \"Bibliographic references containing citation information of academic literature play an important role as a medium connecting earlier and recent studies. As references contain machine-readable metadata such as author name, title, or publication year, they have been widely used in the field of citation information services including search services for scholarly information and research trend analysis. Many institutions around the world manually extract and continuously accumulate reference metadata to provide various scholarly services. However, manually collection of reference metadata every year continues to be a burden because of the associated cost and time consumption. With the accumulation of a large volume of academic literature, several tools, including GROBID and CERMINE, that automatically extract reference metadata have been released. However, these tools have some limitations. For example, they are only applicable to references written in English, the types of extractable metadata are limited for each tool, and the performance of the tools is insufficient to replace the manual extraction of reference metadata. Therefore, in this study, we focused on constructing a high-quality corpus to automatically extract metadata from multilingual journal article references. Using our constructed corpus, we trained and evaluated a BERT-based transfer-learning model. Furthermore, we compared the performance of the BERT-based model with that of the existing model, GROBID. Currently, our corpus contains 3,815,987 multilingual references, mainly in English and Korean, with labels for 13 different metadata types. According to our experiment, the BERT-based model trained using our corpus showed excellent performance in extracting metadata not only from journal references written in English but also in other languages, particularly Korean. This corpus is available at http://doi.org/10.23057/47.\",\n",
            "    \"Document Type\": \"Research Article\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/17.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if extracted_text.startswith(\"Error:\"):\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        # print(metadata)\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svZSXYijlko3",
        "outputId": "d4938dbc-2bc5-4457-c2ca-7afd543ade72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Information extraction from research papers\n",
            "using conditional random ﬁelds q\n",
            "Fuchun Peng a,*, Andrew McCallum b\n",
            "a BBN Technologies, 50 Moulton Street, Cambridge, MA 02138, United States\n",
            "b Department of Computer Science, University of Massachusetts Amherst, 140 Governors Drive,\n",
            "Amherst, MA 01003, United States\n",
            "Received 31 March 2005\n",
            "Available online 6 December 2005\n",
            "Abstract\n",
            "With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring deci-\n",
            "sions, the accuracy of such systems is of paramount importance. This article employs conditional random ﬁelds (CRFs) for\n",
            "the task of extracting various common ﬁelds from the headers and citation of research papers. CRFs provide a principled\n",
            "way for incorporating various local features, external lexicon features and globle layout features. The basic theory of CRFs\n",
            "is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We\n",
            "make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for\n",
            "improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for con-\n",
            "straint co-reference information extraction; i.e., improving extraction performance given that we know some citations refer\n",
            "to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in\n",
            "average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares\n",
            "even more favorably against HMMs. On four co-reference IE datasets, our system signiﬁcantly improves extraction per-\n",
            "formance, with an error rate reduction of 6–14%.\n",
            "\u0001 2005 Elsevier Ltd. All rights reserved.\n",
            "Keywords: Information extraction; Constraint information extraction; Conditional random ﬁelds; Regularization\n",
            "1. Introduction\n",
            "Research paper search engines, such as CiteSeer (Lawrence, Giles, & Bollacker, 1999) and Cora (McCal-\n",
            "lum, Nigam, Rennie, & Seymore, 2000), give researchers tremendous power and convenience in their research.\n",
            "They are also becoming increasingly used for recruiting and hiring decisions. Thus the information quality of\n",
            "0306-4573/$ - see front matter \u0001 2005 Elsevier Ltd. All rights reserved.\n",
            "doi:10.1016/j.ipm.2005.09.002\n",
            "q This work was mostly conducted while the ﬁrst author was at the University of Massachusetts Amherst.\n",
            "* Corresponding author.\n",
            "E-mail addresses: fpeng@bbn.com, fuchun@cs.umass.edu (F. Peng), mccallum@cs.umass.edu (A. McCallum).\n",
            "Information Processing and Management 42 (2006) 963–979\n",
            "www.elsevier.com/locate/infoproman\n",
            "\n",
            "such systems is of signiﬁcant importance. This quality critically depends on an information extraction compo-\n",
            "nent that extracts meta-data, such as title, author, institution, etc., from paper headers and references, because\n",
            "these meta-data are further used in many component applications such as ﬁeld-based search, author analysis,\n",
            "and citati\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\n",
            "  \"Title\": \"Information Extraction from Research Papers using Conditional Random Fields\",\n",
            "  \"Authors\": [\"Fuchun Peng\", \"Andrew McCallum\"],\n",
            "  \"DOI\": \"Not provided\",\n",
            "  \"Keywords\": [\"Information Extraction\", \"Constraint Information Extraction\", \"Conditional Random Fields\", \"Regularization\"],\n",
            "  \"Abstract\": \"This article employs conditional random fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features and global layout features. The basic theory of CRFs is becoming well-understood, but best practices for applying them to real-world data require additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint co-reference information extraction; i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system significantly improves extraction performance, with an error rate reduction of 6–14%. \",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Information Extraction from Research Papers using Conditional Random Fields\",\n",
            "    \"Authors\": [\n",
            "        \"Fuchun Peng\",\n",
            "        \"Andrew McCallum\"\n",
            "    ],\n",
            "    \"DOI\": \"Not provided\",\n",
            "    \"Keywords\": [\n",
            "        \"Information Extraction\",\n",
            "        \"Constraint Information Extraction\",\n",
            "        \"Conditional Random Fields\",\n",
            "        \"Regularization\"\n",
            "    ],\n",
            "    \"Abstract\": \"This article employs conditional random fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features and global layout features. The basic theory of CRFs is becoming well-understood, but best practices for applying them to real-world data require additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint co-reference information extraction; i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system significantly improves extraction performance, with an error rate reduction of 6\\u201314%. \",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/18.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if extracted_text.startswith(\"Error:\"):\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        # print(metadata)\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zX-XhaRlmiJ",
        "outputId": "de5f21d8-5d9e-4543-8da8-8d9c83bb7c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<|im_start|>system\n",
            "You are a helpful assistant that extracts structured metadata from scientific papers.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "You are an information extraction engine. You must output ONLY valid JSON. Do not explain, apologize, or format anything else.\n",
            "\n",
            "Respond with ONLY valid JSON. DO NOT include explanations, markdown, or any text outside the JSON object. Your output will be parsed automatically. If you do not follow this exactly, the response will be discarded.\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Information extraction from scientific articles: a survey\n",
            "Zara Nasar1\n",
            "• Syed Waqar Jaffry1 • Muhammad Kamran Malik1\n",
            "Received: 19 May 2018 / Published online: 29 September 2018\n",
            "\u0002 Akade´miai Kiado´, Budapest, Hungary 2018\n",
            "Abstract\n",
            "In last few decades, with the advent of World Wide Web (WWW), world is being over-\n",
            "loaded with huge data. This huge data carries potential information that once extracted, can\n",
            "be used for betterment of humanity. Information from this data can be extracted using\n",
            "manual and automatic analysis. Manual analysis is not scalable and efﬁcient, whereas, the\n",
            "automatic analysis involves computing mechanisms that aid in automatic information\n",
            "extraction over huge amount of data. WWW has also affected overall growth in scientiﬁc\n",
            "literature that makes the process of literature review quite laborious, time consuming and\n",
            "cumbersome job for researchers. Hence a dire need is felt to automatically extract potential\n",
            "information out of immense set of scientiﬁc articles to automate the process of literature\n",
            "review. Therefore, in this study, aim is to present the overall progress concerning automatic\n",
            "information extraction from scientiﬁc articles. The information insights extracted from\n",
            "scientiﬁc articles are classiﬁed in two broad categories i.e. metadata and key-insights. As\n",
            "available benchmark datasets carry a signiﬁcant role in overall development in this\n",
            "research domain, existing datasets against both categories are extensively reviewed. Later,\n",
            "research studies in literature that have applied various computational approaches applied\n",
            "on these datasets are consolidated. Major computational approaches in this regard include\n",
            "Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support\n",
            "Vector Machines, Naı¨ve-Bayes classiﬁcation and Deep Learning approaches. Currently,\n",
            "there are multiple projects going on that are focused towards the dataset construction\n",
            "tailored to speciﬁc information needs from scientiﬁc articles. Hence, in this study, state-of-\n",
            "the-art regarding information extraction from scientiﬁc articles is covered. This study also\n",
            "consolidates evolving datasets as well as various toolkits and code-bases that can be used\n",
            "for information extraction from scientiﬁc articles.\n",
            "Keywords Metadata extraction \u0002 Key-insights extraction \u0002 Text mining \u0002 Information\n",
            "extraction \u0002 Machine learning \u0002 Research articles \u0002 Scientiﬁc literature\n",
            "& Zara Nasar\n",
            "zara.nasar@pucit.edu.pk\n",
            "Syed Waqar Jaffry\n",
            "swjaffry@pucit.edu.pk\n",
            "Muhammad Kamran Malik\n",
            "kamran.malik@pucit.edu.pk\n",
            "1\n",
            "Punjab University College of Information Technology, University of the Punjab, Lahore, Pakistan\n",
            "123\n",
            "Scientometrics (2018) 117:1931–1990\n",
            "https://doi.org/10.1007/s11192-018-2921-5(0123456789().,-volV)(0123456789().,-volV)\n",
            "\n",
            "Introduction\n",
            "In last few decades, advent of computers and later World Wide Web (WWW) has changed\n",
            "human civilization dramatically. Now we live in the world which is being overloaded with\n",
            "the data and the information. This information overload is posing new c\n",
            "<|im_end|>\n",
            "<|im_start|>assistant{\"Title\":\"Information Extraction from Scientific Articles\",\"Authors\":[\"Zara Nasar\"],\"DOI\":\"Not Available\",\"Keywords\":[\"Metadata Extraction\",\"Key-Insights Extraction\",\"Text Mining\",\"Information Extraction\",\"Machine Learning\",\"Research Articles\",\"Scientific Literature\"],\"Abstract\":\"In last few decades, with the advent of World Wide Web (WWW), world is being overwhelmed by huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and efficient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scientific literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scientific articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scientific articles. The information insights extracted from scientific articles are classified into two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a significant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Naive-Bayes classification and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to specific information needs from scientific articles. Hence, in this study, state-of-the-art regarding information extraction from scientific articles is covered. This study also consolidates evolving datasets as well as various toolkit and codebases that can be used for information extraction from scientific articles.\",\"Document Type\":\"Research Paper\",\"Number of References\":10}}\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Information Extraction from Scientific Articles\",\n",
            "    \"Authors\": [\n",
            "        \"Zara Nasar\"\n",
            "    ],\n",
            "    \"DOI\": \"Not Available\",\n",
            "    \"Keywords\": [\n",
            "        \"Metadata Extraction\",\n",
            "        \"Key-Insights Extraction\",\n",
            "        \"Text Mining\",\n",
            "        \"Information Extraction\",\n",
            "        \"Machine Learning\",\n",
            "        \"Research Articles\",\n",
            "        \"Scientific Literature\"\n",
            "    ],\n",
            "    \"Abstract\": \"In last few decades, with the advent of World Wide Web (WWW), world is being overwhelmed by huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and efficient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scientific literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scientific articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scientific articles. The information insights extracted from scientific articles are classified into two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a significant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Naive-Bayes classification and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to specific information needs from scientific articles. Hence, in this study, state-of-the-art regarding information extraction from scientific articles is covered. This study also consolidates evolving datasets as well as various toolkit and codebases that can be used for information extraction from scientific articles.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}
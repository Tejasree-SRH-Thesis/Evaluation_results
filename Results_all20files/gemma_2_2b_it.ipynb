{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mX6ZHfurEgT",
        "outputId": "1b26d4f2-f449-4897-e102-1d0bab561561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku2eaX1XrHZ6",
        "outputId": "2c76ffb9-349d-459d-dda2-3bb1db3ecbc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.5\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tiktoken einops accelerate bitsandbytes\n",
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yggOvDcVrKga"
      },
      "outputs": [],
      "source": [
        "# import streamlit as st\n",
        "import tempfile\n",
        "import fitz  # PyMuPDF for PDF text extraction\n",
        "import json  # For handling JSON output\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,BitsAndBytesConfig\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lHE8WM9GrMEF"
      },
      "outputs": [],
      "source": [
        "# MODEL_NAME = \"natong19/Qwen2-7B-Instruct-abliterated\"  # Ensure you have the correct model\n",
        "MODEL_NAME= \"google/gemma-2-2b-it\"\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "hf_token = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XTSX0MNRrP6x"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # Change to load_in_4bit=True for even lower memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BXIUz2jKrT0k"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers>=4.30.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWOs0MEfsfHS",
        "outputId": "cdfe5fb0-c4d0-4905-d24a-8b801ab4d3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "The token `exploring ai community` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `exploring ai community`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login  # Login to Hugging Face using the CLI\n",
        "\n",
        "# Or, set your token as an environment variable\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME4hls6tueHk"
      },
      "outputs": [],
      "source": [
        "# snapshot_download(repo_id=MODEL_NAME, use_auth_token=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545,
          "referenced_widgets": [
            "2f96366508264dc388f83a493598a920",
            "55d5b4a2521b48c6884d2244f8b0d523",
            "4c8a360a4e5145edb19d53922ec05b16",
            "0a4959d104214b8daece5e70961cdd66",
            "085d6494959d4db1b16bf5f16c235165",
            "c475af795ecd4739a1eb4588c9456fed",
            "cfaf70fa0a2e40f481066ac4b62b0685",
            "c4bd1bcf9104458087b94cf4c1acca5a",
            "8cdf80a9e44e471382d023e879fcaaac",
            "328e424404dd46d7bfef65e669d0a34f",
            "7cad1efa206c40fda7406e32811eebb4",
            "801ef077f5b143c1ae5d7ace50233561",
            "8a1a0d3e77814464bf9ee31b6b098ec0",
            "a25644c164a44cf498997254aa7a9af6",
            "41033f8cd0a1479b8bd9285dc22eec95",
            "6580634b5c944193899f6db72e542e75",
            "6e55e9a01eaf48ab95f1d3c9f6681d9a",
            "5c0c21b5848748c5a08c0b8ad4d30866",
            "2596443f26ea4558a4fd89fe167055fc",
            "b1a7832a94314401847910e10474504a",
            "e89cba5eca374c8e91f49661a2e09517",
            "f7deb42401a74e05a5748ee2d04d76f4",
            "cc3070d5c94c4c1d8abb3967c0843c29",
            "1a0ee169fd8d4394a9d7aadac7952a2a",
            "d0bc55f571954f639d34c8d63ac89b4c",
            "08be68294e3f4125a5f6c999d9871a02",
            "31837897e0ed473f8a1d26ede1fee5e5",
            "b17a1c8f540a4f9fad7b2db27d52c878",
            "4cf3d12f90f14d829180e4eaa5579719",
            "73bb07c01a82495189615025efc0f2e0",
            "4267c27eeb284d7ea67816a07d1b9989",
            "0ff9f0a26a884465ae6b84d588ac4fab",
            "c3aa46f2533049a5b43dce122db5f2a2",
            "8c97703599f749bd9a4a1268ed8f1d9d",
            "61e765b171c34e7ab29cf731d817a2d0",
            "5a4efa6ef4254884a1ebb380e36ea7e1",
            "2f9b0cec96194a41aa009bf87fc8f88f",
            "6267104b002a4949a35e390ece2c63cd",
            "ea0aed4f26c84eeeafb15221e90c0fb0",
            "f94e481b0743425aa5ebfb939b18d976",
            "d4258d139f124e129ae7760a3951d813",
            "a580a76ef8a34340b96461d9c4e48c0d",
            "b53d9c937eb240c1bc8c35bd92a6d48c",
            "28e86def2a3840d4be4d1475676026cb",
            "c31fb987b5284d1fb85c731d2b50c0e6",
            "d85782446d54496c860e173e0e97c818",
            "eb13b086520846e0ad825045dccbf458",
            "8d7bf53f998e4d3488bceec5d4bcec2a",
            "442814801721414280ddbc56e25e3380",
            "04fa2e6965934fb5922bdef45a5b8c83",
            "fc9020a59977457a87414202adc45875",
            "2790b416844b491c8f7648a3f108daa7",
            "bef93f2552cb496991943f5a9f911d18",
            "4e9188c5beb642289db6e96ff055c6b7",
            "db35067edbf44c48a95cd68018e8d974",
            "3f23f3d105224b9b82cee9b2d12060ed",
            "7f28562248c244cebba9ab33d6253faa",
            "0eead89f3e474ae4a3f5ed13a2a8956a",
            "92f25615a4054b8b9b36d4677090e82e",
            "28316964628b46d1a219ccdda4f69eb4",
            "8e33095c4aae43faa9e49ce398a09589",
            "c9e20381f32646d38b70dde2c420d64a",
            "0f61a73f2da44826bb9973d96108760e",
            "094b7a50e7634d0789c54bd64e002368",
            "ce84ce34b7654184972a04253ac47602",
            "a075e2242c584bacae8e2b1e34e1f72e",
            "1d04a078964748b98399c7f9b398ec43",
            "4ba94b2852f148e5a0fa15c5225a4f80",
            "095f323eb81a4d2080744d6d3355c392",
            "c14cb93b2f9742c99ba6681f8b5370ea",
            "f914e60b4f1c4f22a16c4e032d59b611",
            "c4818208cb534cb18be32fd6c6c90800",
            "4f8c2510e68d42629c2a5be5dacfe6b2",
            "e3d4e105625a4096a67626c82afabf6c",
            "f5edb8dd173c426c8720d1a3bf173a83",
            "776b316f5bd94748b9f643b1b8375b34",
            "9605c2668999421282cd0b38a3e878b3",
            "4e3a54f60f7d405a9d6ec28feb4d6ecd",
            "54235e29b1654288a0784a03f7481244",
            "8f628296853842c687f780809585d4ed",
            "22f2bb3900aa4029b0cb255421a4748d",
            "30ba4fa3dcb641778ead4028e43cd4a3",
            "8c792ceaf7c3457691e9a6f5f183b9b8",
            "94b10860e96149308c4fc37eaf260ad5",
            "0c5858a0be324b3facfe869f961e1db9",
            "f7dadd37ec1547888da3643729334606",
            "2925c3eea8b54a4e90a988b6e6abf924",
            "7a4d3f7dfbd84a0f934be514a19273ce",
            "38dae17b491843ad93fd4fc14b9d21c3",
            "4e82fc5b2ae047b2a8f894d1daf53fcf",
            "f42e3edc203d4be1b36fd81fbb2f0637",
            "9b82c06f561d4ea0a5338f36a7c07e94",
            "405d88633f1e478cbcd939ead514398a",
            "d8be5f6f952d4f9dad85d884997fdad5",
            "1a778cb87dc8463db3d92e1be8bfb0aa",
            "1d00d381ce084d56b342edd43eb811c9",
            "e9fed4b69bca453c846965117dae67fc",
            "7a14709862d34d82970a7afa6a895f03",
            "dba81a4fbd6d435ea6c2160a41626874",
            "76b8403ece0d48d18e6fdf1982396314",
            "7e0804df5101435690ecf10bbc70dea9",
            "ef329ff456b94932b09aac82604bd9ba",
            "69d2d3790366414ba431a436b0f8420b",
            "01f32fce5411441096d29930568ca69e",
            "fc241feec8414d94b9d7cd79d227cce6",
            "922447a303d041118ac684ae99c430cc",
            "a28ba6dc52f24b80b6743c4368f8e4b4",
            "af2a6f59e804480fae6ba34a3ff76f26",
            "114fa0c4dd1d4784aeed1ac8c5519357",
            "c1df4de312814167a9b74546b44bf3cb",
            "e9b1eadad4a24514a1bac9d429d03ddb",
            "f125000fd50f4fe19168116fe9cd935e",
            "22e05f9efb42497cb26a9b9f221f94f5",
            "50e4259814ab4d42b66b24f01b7281d2",
            "c615e297425a4af0b44002bb42ef9877",
            "302bd1c6e1b042d899dcbf8b5a26318b",
            "8effe46e4d19462ea7e7e0b92d6d1551",
            "b22dbc08f2db41d6b50b4121d49869ea",
            "91af9e5d423e443eb8dfe56289815901",
            "2fe5f0205ec84ddb8da11ce516ca9a62",
            "ee0d5db122344bdab19698765198cdf7"
          ]
        },
        "collapsed": true,
        "id": "PMdqh3rkrVVn",
        "outputId": "b7f20333-1c13-42b5-e280-68cfc11f58ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f96366508264dc388f83a493598a920"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "801ef077f5b143c1ae5d7ace50233561"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc3070d5c94c4c1d8abb3967c0843c29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c97703599f749bd9a4a1268ed8f1d9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py:1081: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c31fb987b5284d1fb85c731d2b50c0e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f23f3d105224b9b82cee9b2d12060ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d04a078964748b98399c7f9b398ec43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e3a54f60f7d405a9d6ec28feb4d6ecd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38dae17b491843ad93fd4fc14b9d21c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76b8403ece0d48d18e6fdf1982396314"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9b1eadad4a24514a1bac9d429d03ddb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,token=hf_token, trust_remote_code=True)\n",
        "    model_config = transformers.AutoConfig.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_auth_token=True\n",
        ")\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,config=model_config,device_map=\"cpu\",token=hf_token, torch_dtype=torch.float32, trust_remote_code=True)\n",
        "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id)\n",
        "    return generator\n",
        "\n",
        "generator = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d5RyPiXsrXA0"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZnIcW7sAAdOy",
        "outputId": "1731a2f4-f2e1-47e5-e5b7-ed12263f0eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== DEBUG: Raw Model Output ====\n",
            "\n",
            "You are an expert at extracting structured metadata from research papers.\n",
            "\n",
            "Analyze the following text and return metadata in **strict JSON format** with the following fields:\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Ensure the output is **valid JSON** without extra text, explanations, or markdown formatting.\n",
            "\n",
            "Extracting Scientific Figures with\n",
            "Distantly Supervised Neural Networks\n",
            "Noah Siegel∗\n",
            "Allen Institute for Artificial Intelligence\n",
            "Seattle, Washington\n",
            "siegeln@uw.edu\n",
            "Nicholas Lourie\n",
            "Allen Institute for Artificial Intelligence\n",
            "Seattle, Washington\n",
            "nicholasl@allenai.org\n",
            "Russell Power\n",
            "Allen Institute for Artificial Intelligence\n",
            "Seattle, Washington\n",
            "russell.power@gmail.com\n",
            "Waleed Ammar\n",
            "Allen Institute for Artificial Intelligence\n",
            "Seattle, Washington\n",
            "waleeda@allenai.org\n",
            "ABSTRACT\n",
            "Non-textual components such as charts, diagrams and tables pro-\n",
            "vide key information in many scientific documents, but the lack\n",
            "of large labeled datasets has impeded the development of data-\n",
            "driven methods for scientific figure extraction. In this paper, we\n",
            "induce high-quality training labels for the task of figure extrac-\n",
            "tion in a large number of scientific documents, with no human\n",
            "intervention. To accomplish this we leverage the auxiliary data pro-\n",
            "vided in two large web collections of scientific documents (arXiv\n",
            "and PubMed) to locate figures and their associated captions in the\n",
            "rasterized PDF. We share the resulting dataset of over 5.5 million\n",
            "induced labels—4,000 times larger than the previous largest figure\n",
            "extraction dataset—with an average precision of 96.8%, to enable\n",
            "the development of modern data-driven methods for this task. We\n",
            "use this dataset to train a deep neural network for end-to-end fig-\n",
            "ure detection, yielding a model that can be more easily extended\n",
            "to new domains compared to previous work. The model was suc-\n",
            "cessfully deployed in Semantic Scholar,1 a large-scale academic\n",
            "search engine, and used to extract figures in 13 million scientific\n",
            "documents.2\n",
            "KEYWORDS\n",
            "Figure Extraction, Distant Supervision, Deep Learning, Neural Net-\n",
            "works, Computer Vision\n",
            "∗Now at DeepMind.\n",
            "1https://www.semanticscholar.org/\n",
            "2A demo of our system is available at http://labs.semanticscholar.org/deepfigures/, and\n",
            "our dataset of induced labels can be downloaded at https://s3-us-west-2.amazonaws.\n",
            "com/ai2-s2-research-public/deepfigures/jcdl-deepfigures-labels.zip.\n",
            "\n",
            "\n",
            "**Please note:** This is a fictional text.\n",
            "\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Extracting Scientific Figures with Distantly Supervised Neural Networks\",\n",
            "  \"Authors\": [\n",
            "    \"Noah Siegel\",\n",
            "    \"Nicholas Lourie\",\n",
            "    \"Russell Power\",\n",
            "    \"Waleed Ammar\"\n",
            "  ],\n",
            "  \"DOI\": \"N/A\",\n",
            "  \"Keywords\": [\n",
            "    \"Figure Extraction\",\n",
            "    \"Distant Supervision\",\n",
            "    \"Deep Learning\",\n",
            "    \"Neural Networks\",\n",
            "    \"Computer Vision\"\n",
            "  ],\n",
            "  \"Abstract\": \"Non-textual components such as charts, diagrams and tables pro-vide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels—4,000 times larger than the previous largest figure extraction dataset—with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar,1 a large-scale academic search engine, and used to extract figures in 13 million scientific documents.2\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 2\n",
            "}\n",
            "```\n",
            "metadata start {'Title': 'Extracting Scientific Figures with Distantly Supervised Neural Networks', 'Authors': ['Noah Siegel', 'Nicholas Lourie', 'Russell Power', 'Waleed Ammar'], 'DOI': 'N/A', 'Keywords': ['Figure Extraction', 'Distant Supervision', 'Deep Learning', 'Neural Networks', 'Computer Vision'], 'Abstract': 'Non-textual components such as charts, diagrams and tables pro-vide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels—4,000 times larger than the previous largest figure extraction dataset—with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar,1 a large-scale academic search engine, and used to extract figures in 13 million scientific documents.2', 'Document Type': 'Research Paper', 'Number of References': 2} end\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Scientific Figures with Distantly Supervised Neural Networks\",\n",
            "    \"Authors\": [\n",
            "        \"Noah Siegel\",\n",
            "        \"Nicholas Lourie\",\n",
            "        \"Russell Power\",\n",
            "        \"Waleed Ammar\"\n",
            "    ],\n",
            "    \"DOI\": \"N/A\",\n",
            "    \"Keywords\": [\n",
            "        \"Figure Extraction\",\n",
            "        \"Distant Supervision\",\n",
            "        \"Deep Learning\",\n",
            "        \"Neural Networks\",\n",
            "        \"Computer Vision\"\n",
            "    ],\n",
            "    \"Abstract\": \"Non-textual components such as charts, diagrams and tables pro-vide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels\\u20144,000 times larger than the previous largest figure extraction dataset\\u2014with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar,1 a large-scale academic search engine, and used to extract figures in 13 million scientific documents.2\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 2\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_last_json(text):\n",
        "    \"\"\"Extracts the last valid JSON block from model output using regex.\"\"\"\n",
        "    json_matches = re.findall(r\"\\{(?:[^{}]*|\\{(?:[^{}]*|\\{[^{}]*\\})*\\})*\\}\", text, re.DOTALL)\n",
        "\n",
        "    if json_matches:\n",
        "        last_json = json_matches[-1]  # Take the last JSON block\n",
        "        try:\n",
        "            return json.loads(last_json)  # Convert JSON string to Python dictionary\n",
        "        except json.JSONDecodeError as e:\n",
        "            return {\"Error\": f\"JSON decoding error: {str(e)}\"}\n",
        "\n",
        "    return {\"Error\": \"No valid JSON found.\"}\n",
        "\n",
        "# Debugging function to test extraction\n",
        "def extract_metadata(text):\n",
        "    \"\"\"Uses the model output to extract metadata from PDF text.\"\"\"\n",
        "    prompt = \"\"\"\n",
        "You are an expert at extracting structured metadata from research papers.\n",
        "\n",
        "Analyze the following text and return metadata in **strict JSON format** with the following fields:\n",
        "{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}\n",
        "\n",
        "Ensure the output is **valid JSON** without extra text, explanations, or markdown formatting.\n",
        "\"\"\"\n",
        "\n",
        "    input_text = prompt + \"\\n\" + text[:2048]  # Limiting input size\n",
        "\n",
        "    try:\n",
        "        response = generator(input_text, max_new_tokens=400, do_sample=True, temperature=0.2, truncation=True)\n",
        "        metadata_text = response[0][\"generated_text\"]\n",
        "\n",
        "        print(\"\\n==== DEBUG: Raw Model Output ====\")\n",
        "        print(metadata_text)  # Debugging output\n",
        "\n",
        "        # Extract the last valid JSON block\n",
        "        metadata = extract_last_json(metadata_text)\n",
        "        return metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Error\": str(e)}\n",
        "\n",
        "# Simulating extraction from a PDF\n",
        "extracted_text = extract_text_from_pdf(\"/content/1804.02445v2.pdf\")\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "    print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "    metadata = extract_metadata(extracted_text)\n",
        "    print(\"metadata start\",metadata,\"end\")\n",
        "    print(\"\\n==== Extracted Metadata ====\")\n",
        "    print(json.dumps(metadata, indent=4))  # Pretty-print JSON output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clpo2Djk_ubu",
        "outputId": "c0142104-6dd8-4350-fe09-d603f17db190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting Decision Model and Notation models from text using deep learning techniques\",\n",
            "    \"Authors\": [\n",
            "        \"Alexandre Goossens\",\n",
            "        \"Johannes De Smedt\",\n",
            "        \"Jan Vanthienen\"\n",
            "    ],\n",
            "    \"DOI\": \"Available online 26 August 2022\",\n",
            "    \"Keywords\": [\n",
            "        \"Deep learning\",\n",
            "        \"Decision Model and Notation\",\n",
            "        \"DMN\",\n",
            "        \"Decision model extraction\"\n",
            "    ],\n",
            "    \"Abstract\": \"Companies and organizations often use manuals and guidelines to communicate and execute operational decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions. Modeling a decision from a textual source, however, is a time intensive and complex activity hence a need for shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic extraction of decision models from text.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"start_of_turn>model\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an AI that extracts structured metadata from research papers.\n",
        "\n",
        "Return ONLY valid JSON with the following structure and no extra text:\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:2000]}\n",
        "\"\"\"\n",
        "\n",
        "    return (\n",
        "        \"<start_of_turn>user\\n\"\n",
        "        + instruction.strip() +\n",
        "        \"\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    )\n",
        "\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False, temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "pdf_path = \"/content/4.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"start_of_turn>model\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an AI that extracts structured metadata from research papers.\n",
        "\n",
        "Return ONLY valid JSON with the following structure and no extra text:\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:2000]}\n",
        "\"\"\"\n",
        "\n",
        "    return (\n",
        "        \"<start_of_turn>user\\n\"\n",
        "        + instruction.strip() +\n",
        "        \"\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    )\n",
        "\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False, temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "pdf_path = \"/content/3.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lwlSN0nNzm0w",
        "outputId": "da3c17b1-6100-4a31-e798-93c31d60f60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Model Selection with Model Zoo via Graph\n",
            "Learning\n",
            "Ziyu Li\n",
            "Hilco van der Wilk\n",
            "Danning Zhan\n",
            "Megha Khosla\n",
            "Alessandro Bozzon\n",
            "Rihan Hai\n",
            "Delft University of Technology, The Netherlands\n",
            "z.li-14,{initials.lastname}@tudelft.nl\n",
            "Abstract—Pre-trained deep learning (DL) models are increas-\n",
            "ingly accessible in public repositories, i.e., model zoos. Given a\n",
            "new prediction task, finding the best model to fine-tune can be\n",
            "computationally intensive and costly, especially when the number\n",
            "of pre-trained models is large. Selecting the right pre-trained\n",
            "models is crucial, yet complicated by the diversity of models from\n",
            "various model families (like ResNet, Vit, Swin) and the hidden\n",
            "relationships between models and datasets. Existing methods,\n",
            "which utilize basic information from models and datasets to\n",
            "compute scores indicating model performance on target datasets,\n",
            "overlook the intrinsic relationships, limiting their effectiveness\n",
            "in model selection. In this study, we introduce TransferGraph,\n",
            "a novel framework that reformulates model selection as a\n",
            "graph learning problem. TransferGraph constructs a graph using\n",
            "extensive metadata extracted from models and datasets, while\n",
            "capturing their inherent relationships. Through comprehensive\n",
            "experiments across 16 real datasets, both images and texts, we\n",
            "demonstrate TransferGraph’s effectiveness in capturing essential\n",
            "model-dataset relationships, yielding up to a 32% improvement\n",
            "in correlation between predicted performance and the actual fine-\n",
            "tuning results compared to the state-of-the-art methods.\n",
            "I. INTRODUCTION\n",
            "Deep learning has been widely used in handling unstruc-\n",
            "tured data, including tasks related to image and text classi-\n",
            "fication. The paradigm of first pre-training, then fine-tuning\n",
            "has become the de facto of applying deep learning in prac-\n",
            "tice. Pre-training is the phase of training a neural network\n",
            "on a large, diverse dataset, typically drawn from a general\n",
            "domain, e.g., ImageNet [1]. Subsequently, the fine-tuning step\n",
            "refines the model for\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Model Selection with Model Zoo via Graph Learning\",\n",
            "  \"Authors\": [\"Ziyu Li\", \"Hilco van der Wilk\", \"Danning Zhan\", \"Megha Khosla\", \"Alessandro Bozzon\", \"Rihan Hai\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Model Selection\", \"Model Zoo\", \"Graph Learning\", \"Deep Learning\", \"Fine-tuning\"],\n",
            "  \"Abstract\": \"Abstract—Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph’s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Model Selection with Model Zoo via Graph Learning\",\n",
            "    \"Authors\": [\n",
            "        \"Ziyu Li\",\n",
            "        \"Hilco van der Wilk\",\n",
            "        \"Danning Zhan\",\n",
            "        \"Megha Khosla\",\n",
            "        \"Alessandro Bozzon\",\n",
            "        \"Rihan Hai\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Model Selection\",\n",
            "        \"Model Zoo\",\n",
            "        \"Graph Learning\",\n",
            "        \"Deep Learning\",\n",
            "        \"Fine-tuning\"\n",
            "    ],\n",
            "    \"Abstract\": \"Abstract\\u2014Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph\\u2019s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/2.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j_AHeK3L6Bwk",
        "outputId": "b4e32559-9178-4565-aa4d-e18dfd3edae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Automatic Recognition of Learning Resource\n",
            "Category in a Digital Library\n",
            "Soumya Banerjee∗, Debarshi Kumar Sanyal†, Samiran Chattopadhyay‡,\n",
            "Plaban Kumar Bhowmick§, Partha Pratim Das¶\n",
            "∗§¶IIT Kharagpur, Kharagpur-721302, India, †Indian Association for the Cultivation of Science, Kolkata-700032, India,\n",
            "∗‡Jadavpur University, Kolkata-700106, India\n",
            "Email: ∗soumyaBanerjee@outlook.in, †debarshisanyal@gmail.com, ‡samirancju@gmail.com,\n",
            "§plaban@cet.iitkgp.ac.in, ¶ppd@cse.iitkgp.ac.in\n",
            "Abstract—Digital libraries generally need to process a large\n",
            "volume of diverse document types. The collection and tagging\n",
            "of metadata is a long, error-prone, manpower-consuming task.\n",
            "We are attempting to build an automatic metadata extractor\n",
            "for digital libraries. In this work, we present the Heterogeneous\n",
            "Learning Resources (HLR) dataset for document image classi-\n",
            "fication. The individual learning resource is first decomposed\n",
            "into its constituent document images (sheets) which are then\n",
            "passed through an OCR tool to obtain the textual representation.\n",
            "The document image and its textual content are classified with\n",
            "state-of-the-art classifiers. Finally, the labels of the constituent\n",
            "document images are used to predict the label of the overall\n",
            "document.\n",
            "Index Terms—deep learning, transfer learning, digital library\n",
            "I. INTRODUCTION\n",
            "A large digital library generally contains resources of differ-\n",
            "ent types. For example, the National Digital Library of India\n",
            "(NDLI) curates heterogeneous educational resources including\n",
            "scientific articles, books, paintings, etc. A library may receive\n",
            "curated metadata of resources directly or simply receive the\n",
            "resources from which it has to separately extract the metadata.\n",
            "In the latter case, knowing the type of the document is\n",
            "necessary because metadata extraction mechanisms (manual\n",
            "or automated) generally vary with resource types. Thus, it is\n",
            "worthwhile to explore methods of automatic classification of\n",
            "document types so that the correct metadata extraction process\n",
            "can be\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Automatic Recognition of Learning Resource in a Digital Library\",\n",
            "  \"Authors\": [\"Soumya Banerjee\", \"Debarshi Kumar Sanyal\", \"Samiran Chattopadhyay\", \"Plaban Kumar Bhowmick\", \"Partha Pratim Das\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Learning Resource\", \"Digital Library\", \"Document Image Classification\", \"OCR\", \"Heterogeneous Learning Resources\"],\n",
            "  \"Abstract\": \"Abstract—Digital libraries generally need to process a large volume of diverse document types. The collection and tagging of metadata is a long, error-prone, manpower-consuming task. We are attempting to build an automatic metadata extractor for digital libraries. In this work, we present the Heterogeneous Learning Resources (HLR) dataset for document image classification. The individual learning resource is first decomposed into its constituent document images (sheets) which are then passed through an OCR tool to obtain the textual representation. The document image and its textual content are classified with state-of-the-art classifiers. Finally, the labels of the constituent document images are used to predict the label of the overall document.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0 \n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Automatic Recognition of Learning Resource in a Digital Library\",\n",
            "    \"Authors\": [\n",
            "        \"Soumya Banerjee\",\n",
            "        \"Debarshi Kumar Sanyal\",\n",
            "        \"Samiran Chattopadhyay\",\n",
            "        \"Plaban Kumar Bhowmick\",\n",
            "        \"Partha Pratim Das\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Learning Resource\",\n",
            "        \"Digital Library\",\n",
            "        \"Document Image Classification\",\n",
            "        \"OCR\",\n",
            "        \"Heterogeneous Learning Resources\"\n",
            "    ],\n",
            "    \"Abstract\": \"Abstract\\u2014Digital libraries generally need to process a large volume of diverse document types. The collection and tagging of metadata is a long, error-prone, manpower-consuming task. We are attempting to build an automatic metadata extractor for digital libraries. In this work, we present the Heterogeneous Learning Resources (HLR) dataset for document image classification. The individual learning resource is first decomposed into its constituent document images (sheets) which are then passed through an OCR tool to obtain the textual representation. The document image and its textual content are classified with state-of-the-art classifiers. Finally, the labels of the constituent document images are used to predict the label of the overall document.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/5.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnfTsYo_I2BC",
        "outputId": "fd7d4389-7ac9-48f1-f50e-49aae7fe0c0b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Article\n",
            "https://doi.org/10.1038/s41467-024-45914-8\n",
            "Extracting accurate materials data from\n",
            "research papers with conversational\n",
            "language models and prompt engineering\n",
            "Maciej P. Polak\n",
            "1\n",
            "& Dane Morgan\n",
            "1\n",
            "There has been a growing effort to replace manual extraction of data from\n",
            "research papers with automated data extraction based on natural language\n",
            "processing, language models, and recently, large language models (LLMs).\n",
            "Although these methods enable efﬁcient extraction of data from large sets of\n",
            "research papers, they require a signiﬁcant amount of up-front effort, expertise,\n",
            "and coding. In this work, we propose the ChatExtract method that can fully\n",
            "automate very accurate data extraction with minimal initial effort and back-\n",
            "ground, using an advanced conversational LLM. ChatExtract consists of a set\n",
            "of engineered prompts applied to a conversational LLM that both identify\n",
            "sentences with data, extract that data, and assure the data’s correctness\n",
            "through a series of follow-up questions. These follow-up questions largely\n",
            "overcome known issues with LLMs providing factually inaccurate responses.\n",
            "ChatExtract can be applied with any conversational LLMs and yields very\n",
            "high quality data extraction. In tests on materials data, we ﬁnd precision and\n",
            "recall both close to 90% from the best conversational LLMs, like GPT-4. We\n",
            "demonstrate that the exceptional performance is enabled by the information\n",
            "retention in a conversational model combined with purposeful redundancy\n",
            "and introducing uncertainty through follow-up prompts. These results suggest\n",
            "that approaches similar to ChatExtract, due to their simplicity, transfer-\n",
            "ability, and accuracy are likely to become powerful tools for data extraction in\n",
            "the near future. Finally, databases for critical cooling rates of metallic glasses\n",
            "and yield strengths of high entropy alloys are developed using ChatExtract.\n",
            "Automated data extraction is increasingly used to develop databases\n",
            "in materials science and other ﬁelds1. Many databases have been c\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Extracting accurate materials data from research papers with conversational language models and prompt engineering\",\n",
            "  \"Authors\": [\n",
            "    \"Maciej P. Polak\",\n",
            "    \"Dane Morgan\"\n",
            "  ],\n",
            "  \"DOI\": \"10.1038/s41467-024-45914-8\",\n",
            "  \"Keywords\": [\n",
            "    \"materials data\",\n",
            "    \"research papers\",\n",
            "    \"conversational language models\",\n",
            "    \"prompt engineering\"\n",
            "  ],\n",
            "  \"Abstract\": \"Extracting accurate materials data from research papers with conversational language models and prompt engineering\",\n",
            "  \"Document Type\": \"Article\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Extracting accurate materials data from research papers with conversational language models and prompt engineering\",\n",
            "    \"Authors\": [\n",
            "        \"Maciej P. Polak\",\n",
            "        \"Dane Morgan\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1038/s41467-024-45914-8\",\n",
            "    \"Keywords\": [\n",
            "        \"materials data\",\n",
            "        \"research papers\",\n",
            "        \"conversational language models\",\n",
            "        \"prompt engineering\"\n",
            "    ],\n",
            "    \"Abstract\": \"Extracting accurate materials data from research papers with conversational language models and prompt engineering\",\n",
            "    \"Document Type\": \"Article\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/6.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0fLsFDdMmu-",
        "outputId": "99753fab-9de4-4bab-cb9d-0937fcdca38b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Open Access\n",
            "© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \n",
            "use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \n",
            "author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \n",
            "party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate‑\n",
            "rial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \n",
            "exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://​\n",
            "creat​iveco​mmons.​org/​licen​ses/​by/4.​0/.\n",
            "RESEARCH\n",
            "Chanaa and El Faddouli ﻿\n",
            "Smart Learning Environments           (2024) 11:16  \n",
            "https://doi.org/10.1186/s40561-024-00301-0\n",
            "Smart Learning Environments\n",
            "Prerequisites‑based course \n",
            "recommendation: recommending learning \n",
            "objects using concept prerequisites \n",
            "and metadata matching\n",
            "Abdessamad Chanaa1*    and Nour‑eddine El Faddouli1 \n",
            "Abstract \n",
            "The recommendation is an active area of scientific research; it is also a challenging \n",
            "and fundamental problem in online education. However, classical recommender sys‑\n",
            "tems usually suffer from item cold-start issues. Besides, unlike other fields like e-com‑\n",
            "merce or entertainment, e-learning recommendations must ensure that learners \n",
            "have the adequate background knowledge to cognitively receive the recommended \n",
            "learning objects. For that reason, when designing an efficient e-learning recommenda‑\n",
            "tion method, these challenges should be considered. To address those issues, in this \n",
            "paper, we first propose extracting pairs concept prerequisites using Linked Open Data \n",
            "(LOD). Then, we evaluate the proposed list of prerequisite relationships using\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Prerequisites-based course recommendation: recommending learning objects using concept prerequisites and metadata matching\",\n",
            "  \"Authors\": [\"Abdessamad Chanaa\", \"Nour-eddine El Faddouli\"],\n",
            "  \"DOI\": \"10.1186/s40561-024-00301-0\",\n",
            "  \"Keywords\": [\"Smart Learning Environments\", \"Prerequisites-based course recommendation\", \"learning objects\", \"concept prerequisites\", \"metadata matching\"],\n",
            "  \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. However, classical recommender systems usually suffer from item cold-start issues. Besides, unlike other fields like e-commerce or entertainment, e-learning recommendations must ensure that learners have the adequate background knowledge to cognitively receive the recommended learning objects. For that reason, when designing an efficient e-learning recommendation method, these challenges should be considered. To address those issues, in this paper, we first propose extracting pairs of concept prerequisites using Linked Open Data (LOD). Then, we evaluate the proposed list of prerequisite relationships using...\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Prerequisites-based course recommendation: recommending learning objects using concept prerequisites and metadata matching\",\n",
            "    \"Authors\": [\n",
            "        \"Abdessamad Chanaa\",\n",
            "        \"Nour-eddine El Faddouli\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1186/s40561-024-00301-0\",\n",
            "    \"Keywords\": [\n",
            "        \"Smart Learning Environments\",\n",
            "        \"Prerequisites-based course recommendation\",\n",
            "        \"learning objects\",\n",
            "        \"concept prerequisites\",\n",
            "        \"metadata matching\"\n",
            "    ],\n",
            "    \"Abstract\": \"The recommendation is an active area of scientific research; it is also a challenging and fundamental problem in online education. However, classical recommender systems usually suffer from item cold-start issues. Besides, unlike other fields like e-commerce or entertainment, e-learning recommendations must ensure that learners have the adequate background knowledge to cognitively receive the recommended learning objects. For that reason, when designing an efficient e-learning recommendation method, these challenges should be considered. To address those issues, in this paper, we first propose extracting pairs of concept prerequisites using Linked Open Data (LOD). Then, we evaluate the proposed list of prerequisite relationships using...\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"start_of_turn>model\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an AI that extracts structured metadata from research papers.\n",
        "\n",
        "Return ONLY valid JSON with the following structure and no extra text:\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:2000]}\n",
        "\"\"\"\n",
        "\n",
        "    return (\n",
        "        \"<start_of_turn>user\\n\"\n",
        "        + instruction.strip() +\n",
        "        \"\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    )\n",
        "\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False, temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "pdf_path = \"/content/7.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMN5kAMUQ9Bp",
        "outputId": "086a0fed-44c8-4497-c218-9c6429daa13f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "MexPub: Deep Transfer Learning for Metadata\n",
            "Extraction from German Publications\n",
            "Zeyd Boukhers\n",
            "Nada Beili\n",
            "Timo Hartmann\n",
            "Prantik Goswami\n",
            "Muhammad Arslan Zafar\n",
            "Institute for Web Science and Technologies (WeST)\n",
            "University of Koblenz-Landau\n",
            "Koblenz, Germany\n",
            "{boukhers,nbeili,tihartmann,prantik,arslanzafar}@uni-koblenz.de\n",
            "Abstract—In contrast to most of the English scientiﬁc publica-\n",
            "tions that follow standard and simple layouts, the order, content,\n",
            "position and size of metadata in German publications vary greatly\n",
            "among publications. This variety makes traditional NLP methods\n",
            "fail to accurately extract metadata from these publications. In\n",
            "this paper, we present a method that extracts metadata from\n",
            "PDF documents with different layouts and styles by viewing the\n",
            "document as an image. We used Mask R-CNN which is trained\n",
            "on COCO dataset and ﬁnetuned with PubLayNet dataset that\n",
            "consists of 200K PDF snapshots with ﬁve basic classes (e.g. text,\n",
            "ﬁgure, etc). We reﬁne-tuned the model on our proposed synthetic\n",
            "dataset consisting of\n",
            "30K article snapshots to extract nine\n",
            "patterns (i.e. author, title, etc). Our synthetic dataset is generated\n",
            "using contents in both languages German and English and a ﬁnite\n",
            "set of challenging templates obtained from German publications.\n",
            "Our method achieved an average accuracy of around 90% which\n",
            "validates its capability to accurately extract metadata from a\n",
            "variety of PDF documents with challenging templates.\n",
            "Index Terms—transfer learning, metadata extraction, neural\n",
            "networks\n",
            "I. INTRODUCTION\n",
            "The availability and accessibility of academic metadata\n",
            "allow the development of semantic-enable services, such as\n",
            "authors’ proﬁling, bibliometrics, and scientiﬁc social network\n",
            "analysis. However, still, a signiﬁcant part of bibliographic\n",
            "data in disciplines such as social science is not accessible via\n",
            "bibliographic databases and a vast amount of already exist-\n",
            "ing scientiﬁc documents have incomplete or entirely missing\n",
            "metadata information [13].\n",
            "One way to overcome th\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Deep Transfer Learning for Metadata Extraction from German Publications\",\n",
            "  \"Authors\": [\"Zeyd Boukhers\", \"Nada Beili\", \"Timo Hartmann\", \"Prantik Goswami\", \"Muhammad Arslan Zafar\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"metadata extraction\", \"German publications\", \"deep transfer learning\", \"PDF documents\", \"Mask R-CNN\"],\n",
            "  \"Abstract\": \"In contrast to most of the English scientiﬁc publications that follow standard and simple layouts, the order, content, position and size of metadata in German publications vary greatly among publications. This variety makes traditional NLP methods fail to accurately extract metadata from these publications. In this paper, we present a method that extracts metadata from PDF documents with different layouts and styles by viewing the document as an image. We used Mask R-CNN which is trained on COCO dataset and ﬁnetuned with PubLayNet dataset that consists of 200K PDF snapshots with ﬁve basic classes (e.g. text, ﬁgure, etc). We reﬁne-tuned the model on our proposed synthetic dataset consisting of 30K article snapshots to extract nine patterns (i.e. author, title, etc). Our synthetic dataset is generated using contents in both languages German and English and a ﬁnite set of challenging templates obtained from German publications. Our method achieved an average accuracy of around 90% which validates its capability to accurately extract metadata from a variety of PDF documents with challenging templates.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Deep Transfer Learning for Metadata Extraction from German Publications\",\n",
            "    \"Authors\": [\n",
            "        \"Zeyd Boukhers\",\n",
            "        \"Nada Beili\",\n",
            "        \"Timo Hartmann\",\n",
            "        \"Prantik Goswami\",\n",
            "        \"Muhammad Arslan Zafar\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"metadata extraction\",\n",
            "        \"German publications\",\n",
            "        \"deep transfer learning\",\n",
            "        \"PDF documents\",\n",
            "        \"Mask R-CNN\"\n",
            "    ],\n",
            "    \"Abstract\": \"In contrast to most of the English scienti\\ufb01c publications that follow standard and simple layouts, the order, content, position and size of metadata in German publications vary greatly among publications. This variety makes traditional NLP methods fail to accurately extract metadata from these publications. In this paper, we present a method that extracts metadata from PDF documents with different layouts and styles by viewing the document as an image. We used Mask R-CNN which is trained on COCO dataset and \\ufb01netuned with PubLayNet dataset that consists of 200K PDF snapshots with \\ufb01ve basic classes (e.g. text, \\ufb01gure, etc). We re\\ufb01ne-tuned the model on our proposed synthetic dataset consisting of 30K article snapshots to extract nine patterns (i.e. author, title, etc). Our synthetic dataset is generated using contents in both languages German and English and a \\ufb01nite set of challenging templates obtained from German publications. Our method achieved an average accuracy of around 90% which validates its capability to accurately extract metadata from a variety of PDF documents with challenging templates.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/8.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "id": "E9DMd8v-RAlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c417314f-6ee8-4fef-c01e-2f0438f7dae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "ERNIE 3.0: LARGE-SCALE KNOWLEDGE ENHANCED\n",
            "PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND\n",
            "GENERATION\n",
            "Yu Sun∗\n",
            "Shuohuan Wang∗\n",
            "Shikun Feng∗\n",
            "Siyu Ding\n",
            "Chao Pang\n",
            "Junyuan Shang\n",
            "Jiaxiang Liu\n",
            "Xuyi Chen\n",
            "Yanbin Zhao\n",
            "Yuxiang Lu\n",
            "Weixin Liu\n",
            "Zhihua Wu\n",
            "Weibao Gong\n",
            "Jianzhong Liang\n",
            "Zhizhou Shang\n",
            "Peng Sun\n",
            "Wei Liu\n",
            "Xuan Ouyang\n",
            "Dianhai Yu\n",
            "Hao Tian\n",
            "Hua Wu\n",
            "Haifeng Wang\n",
            "Baidu Inc.\n",
            "{sunyu02, wangshuohuan, fengshikun01}@baidu.com\n",
            "ABSTRACT\n",
            "Pre-trained models have achieved state-of-the-art results in various Natural Language Processing\n",
            "(NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained\n",
            "language models can improve their generalization abilities. Particularly, the GPT-3 model with 175\n",
            "billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite\n",
            "their success, these large-scale models are trained on plain texts without introducing knowledge such\n",
            "as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an\n",
            "auto-regressive way. As a result, this kind of traditional ﬁne-tuning approach demonstrates relatively\n",
            "weak performance when solving downstream language understanding tasks. In order to solve the\n",
            "above problems, we propose a uniﬁed framework named ERNIE 3.0 for pre-training large-scale\n",
            "knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that\n",
            "the trained model can be easily tailored for both natural language understanding and generation\n",
            "tasks with zero-shot learning, few-shot learning or ﬁne-tuning. We trained the model with 10 billion\n",
            "parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical\n",
            "results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its\n",
            "English version achieves the ﬁrst place on the SuperGLUE [3] benchmark (July 3, 2021), surpassing\n",
            "the human performance by +0.8% (90.6% vs. 89.8%).\n",
            "1\n",
            "Introduction\n",
            "Pre-trained language models such as ELMo [4], GPT [5],\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"ERNIE 3.0: LARGE-SCALE KNOWLEDGE ENHANCED PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND GENERATION\",\n",
            "  \"Authors\": [\"Yu Sun\", \"Shuohuan Wang\", \"Shikun Feng\", \"Siyu Ding\", \"Chao Pang\", \"Junyuan Shang\", \"Jiaxiang Liu\", \"Xuyi Chen\", \"Yanbin Zhao\", \"Yuxiang Lu\", \"Weixin Liu\", \"Zhihua Wu\", \"Weibao Gong\", \"Jianzhong Liang\", \"Zhizhou Shang\", \"Peng Sun\", \"Wei Liu\", \"Xuan Ouyang\", \"Dianhai Yu\", \"Hao Tian\", \"Hua Wu\", \"Haifeng Wang\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Knowledge Enhanced\", \"Pre-training\", \"Language Understanding\", \"Language Generation\", \"ERNIE 3.0\"],\n",
            "  \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional ﬁne-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or ﬁne-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE [3] benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"ERNIE 3.0: LARGE-SCALE KNOWLEDGE ENHANCED PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND GENERATION\",\n",
            "    \"Authors\": [\n",
            "        \"Yu Sun\",\n",
            "        \"Shuohuan Wang\",\n",
            "        \"Shikun Feng\",\n",
            "        \"Siyu Ding\",\n",
            "        \"Chao Pang\",\n",
            "        \"Junyuan Shang\",\n",
            "        \"Jiaxiang Liu\",\n",
            "        \"Xuyi Chen\",\n",
            "        \"Yanbin Zhao\",\n",
            "        \"Yuxiang Lu\",\n",
            "        \"Weixin Liu\",\n",
            "        \"Zhihua Wu\",\n",
            "        \"Weibao Gong\",\n",
            "        \"Jianzhong Liang\",\n",
            "        \"Zhizhou Shang\",\n",
            "        \"Peng Sun\",\n",
            "        \"Wei Liu\",\n",
            "        \"Xuan Ouyang\",\n",
            "        \"Dianhai Yu\",\n",
            "        \"Hao Tian\",\n",
            "        \"Hua Wu\",\n",
            "        \"Haifeng Wang\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Knowledge Enhanced\",\n",
            "        \"Pre-training\",\n",
            "        \"Language Understanding\",\n",
            "        \"Language Generation\",\n",
            "        \"ERNIE 3.0\"\n",
            "    ],\n",
            "    \"Abstract\": \"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 [1] and GPT-3 [2] have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional \\ufb01ne-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or \\ufb01ne-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE [3] benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/9.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qylRGNPIpwk1",
        "outputId": "beba8521-df93-48ef-c40f-92ae9ca177ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Vol.:(0123456789)\n",
            "Journal of Ambient Intelligence and Humanized Computing (2024) 15:2105–2118 \n",
            "https://doi.org/10.1007/s12652-023-04740-4\n",
            "ORIGINAL RESEARCH\n",
            "Few‑shot named entity recognition framework for forestry science \n",
            "metadata extraction\n",
            "Yuquan Fan1 · Hong Xiao1 · Min Wang2 · Junchi Wang1 · Wenchao Jiang1   · Chang Zhu1\n",
            "Received: 6 June 2023 / Accepted: 8 December 2023 / Published online: 1 February 2024 \n",
            "© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024\n",
            "Abstract\n",
            "The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding \n",
            "of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present \n",
            "challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive \n",
            "exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the effi-\n",
            "cient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research \n",
            "within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is chal-\n",
            "lenging inherent, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry \n",
            "science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus \n",
            "and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. \n",
            "Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), \n",
            "effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic compre-\n",
            "hension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture an\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Few‑shot named entity recognition framework for forestry science metadata extraction\",\n",
            "  \"Authors\": [\"Yuquan Fan\", \"Hong Xiao\", \"Min Wang\", \"Junchi Wang\", \"Wenchao Jiang\", \"Chang Zhu\"],\n",
            "  \"DOI\": \"10.1007/s12652-023-04740-4\",\n",
            "  \"Keywords\": [\"named entity recognition\", \"forestry science\", \"metadata extraction\"],\n",
            "  \"Abstract\": \"The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the efficient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is challenging inherent, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic comprehension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture an\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0 \n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Few\\u2011shot named entity recognition framework for\\u00a0forestry science metadata extraction\",\n",
            "    \"Authors\": [\n",
            "        \"Yuquan\\u00a0Fan\",\n",
            "        \"Hong\\u00a0Xiao\",\n",
            "        \"Min\\u00a0Wang\",\n",
            "        \"Junchi\\u00a0Wang\",\n",
            "        \"Wenchao\\u00a0Jiang\",\n",
            "        \"Chang\\u00a0Zhu\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1007/s12652-023-04740-4\",\n",
            "    \"Keywords\": [\n",
            "        \"named entity recognition\",\n",
            "        \"forestry science\",\n",
            "        \"metadata extraction\"\n",
            "    ],\n",
            "    \"Abstract\": \"The effective utilization of accumulated forestry science papers is of paramount significance in enhancing our understanding of the current state of forests and the formulation of strategies for forest environmental preservation. However, the present challenge lies in the deficient richness of metadata associated with these pivotal documents, rendering their comprehensive exploitation a formidable endeavor. Metadata from forestry science papers serves as a foundational cornerstone for the efficient management and utilization of these scholarly documents, playing an indispensable role in the advancement of research within the domain of forestry science. Constructing a training corpus and extracting distant semantic relationships is challenging inherent, the utilization of named entity recognition (NER) technology for metadata entity identification in forestry science papers remains an unexplored avenue. To overcome these limitations, this paper creates a specialized training corpus and introduces a novel few-shot NER framework tailored specifically for metadata extraction from forestry science papers. Within this innovative framework, a data augmentation layer, employing word replacement (WR) and enhanced mixup (EM), effectively addresses the issue of suboptimal performance resulting from a scarcity of training data. The semantic comprehension layer incorporates a multi-granularity dilated convolution neural network (MGDCNN) to capture an\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/10.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iK9zoH2tpx-G",
        "outputId": "f317e9c6-e765-4727-fcc5-77f8c55a78f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Article\n",
            "https://doi.org/10.1038/s41467-024-45563-x\n",
            "Structured information extraction from\n",
            "scientiﬁc text with large language models\n",
            "John Dagdelen\n",
            "1,2,3, Alexander Dunn\n",
            "1,2,3, Sanghoon Lee1,2, Nicholas Walker1,\n",
            "Andrew S. Rosen\n",
            "1,2, Gerbrand Ceder1,2, Kristin A. Persson\n",
            "1,2 &\n",
            "Anubhav Jain\n",
            "1\n",
            "Extracting structured knowledge from scientiﬁc text remains a challenging\n",
            "task for machine learning models. Here, we present a simple approach to joint\n",
            "named entity recognition and relation extraction and demonstrate how pre-\n",
            "trained large language models (GPT-3, Llama-2) can be ﬁne-tuned to extract\n",
            "useful records of complex scientiﬁc knowledge. We test three representative\n",
            "tasks in materials chemistry: linking dopants and host materials, cataloging\n",
            "metal-organic frameworks, and general composition/phase/morphology/\n",
            "application information extraction. Records are extracted from single sen-\n",
            "tences or entire paragraphs, and the output can be returned as simple English\n",
            "sentences or a more structured format such as a list of JSON objects. This\n",
            "approach represents a simple, accessible, and highly ﬂexible route to obtain-\n",
            "ing large databases of structured specialized scientiﬁc knowledge extracted\n",
            "from research papers.\n",
            "The majority of scientiﬁc knowledge about solid-state materials is\n",
            "scattered across the text, tables, and ﬁgures of millions of academic\n",
            "research papers. Thus, it is difﬁcult for researchers to properly\n",
            "understand the full body of past work and effectively leverage existing\n",
            "knowledge when designing experiments. Moreover, machine learning\n",
            "models for direct property prediction are being increasingly employed\n",
            "as screening steps for materials discovery and design workﬂows1–3, but\n",
            "this approach is limited by the amount of training data available in\n",
            "tabulated databases. While databases of materials property data\n",
            "derived from ab initio simulations are relatively common, they are\n",
            "limited to the subset of computationally accessible properties whereas\n",
            "databases of experimental proper\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Structured information extraction from scientific text with large language models\",\n",
            "  \"Authors\": [\n",
            "    \"John Dagdelen\",\n",
            "    \"Alexander Dunn\",\n",
            "    \"Sanghoon Lee\",\n",
            "    \"Nicholas Walker\",\n",
            "    \"Andrew S. Rosen\",\n",
            "    \"Gerbrand Ceder\",\n",
            "    \"Kristin A. Persson\",\n",
            "    \"Anubhav Jain\"\n",
            "  ],\n",
            "  \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "  \"Keywords\": [\n",
            "    \"Named entity recognition\",\n",
            "    \"Relation extraction\",\n",
            "    \"Large language models\",\n",
            "    \"Materials chemistry\",\n",
            "    \"Dopants\",\n",
            "    \"Metal-organic frameworks\",\n",
            "    \"Composition/phase/morphology/application information extraction\"\n",
            "  ],\n",
            "  \"Abstract\": \"Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pre-trained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Structured information extraction from scientific text with large language models\",\n",
            "    \"Authors\": [\n",
            "        \"John Dagdelen\",\n",
            "        \"Alexander Dunn\",\n",
            "        \"Sanghoon Lee\",\n",
            "        \"Nicholas Walker\",\n",
            "        \"Andrew S. Rosen\",\n",
            "        \"Gerbrand Ceder\",\n",
            "        \"Kristin A. Persson\",\n",
            "        \"Anubhav Jain\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "    \"Keywords\": [\n",
            "        \"Named entity recognition\",\n",
            "        \"Relation extraction\",\n",
            "        \"Large language models\",\n",
            "        \"Materials chemistry\",\n",
            "        \"Dopants\",\n",
            "        \"Metal-organic frameworks\",\n",
            "        \"Composition/phase/morphology/application information extraction\"\n",
            "    ],\n",
            "    \"Abstract\": \"Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pre-trained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"start_of_turn>model\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an AI that extracts structured metadata from research papers.\n",
        "\n",
        "Return ONLY valid JSON with the following structure and no extra text:\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:2000]}\n",
        "\"\"\"\n",
        "\n",
        "    return (\n",
        "        \"<start_of_turn>user\\n\"\n",
        "        + instruction.strip() +\n",
        "        \"\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    )\n",
        "\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False, temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "\n",
        "pdf_path = \"/content/11.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        # metadata = extract_metadata(generator, extracted_text)\n",
        "        metadata = extract_metadata(generator,extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tfiCJDZKv4wp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1b1345-bf80-445e-880a-5907d11b1805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Article\n",
            "https://doi.org/10.1038/s41467-024-45563-x\n",
            "Structured information extraction from\n",
            "scientiﬁc text with large language models\n",
            "John Dagdelen\n",
            "1,2,3, Alexander Dunn\n",
            "1,2,3, Sanghoon Lee1,2, Nicholas Walker1,\n",
            "Andrew S. Rosen\n",
            "1,2, Gerbrand Ceder1,2, Kristin A. Persson\n",
            "1,2 &\n",
            "Anubhav Jain\n",
            "1\n",
            "Extracting structured knowledge from scientiﬁc text remains a challenging\n",
            "task for machine learning models. Here, we present a simple approach to joint\n",
            "named entity recognition and relation extraction and demonstrate how pre-\n",
            "trained large language models (GPT-3, Llama-2) can be ﬁne-tuned to extract\n",
            "useful records of complex scientiﬁc knowledge. We test three representative\n",
            "tasks in materials chemistry: linking dopants and host materials, cataloging\n",
            "metal-organic frameworks, and general composition/phase/morphology/\n",
            "application information extraction. Records are extracted from single sen-\n",
            "tences or entire paragraphs, and the output can be returned as simple English\n",
            "sentences or a more structured format such as a list of JSON objects. This\n",
            "approach represents a simple, accessible, and highly ﬂexible route to obtain-\n",
            "ing large databases of structured specialized scientiﬁc knowledge extracted\n",
            "from research papers.\n",
            "The majority of scientiﬁc knowledge about solid-state materials is\n",
            "scattered across the text, tables, and ﬁgures of millions of academic\n",
            "research papers. Thus, it is difﬁcult for researchers to properly\n",
            "understand the full body of past work and effectively leverage existing\n",
            "knowledge when designing experiments. Moreover, machine learning\n",
            "models for direct property prediction are being increasingly employed\n",
            "as screening steps for materials discovery and design workﬂows1–3, but\n",
            "this approach is limited by the amount of training data available in\n",
            "tabulated databases. While databases of materials property data\n",
            "derived from ab initio simulations are relatively common, they are\n",
            "limited to the subset of computationally accessible properties whereas\n",
            "databases of experimental proper\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Structured information extraction from scientific text with large language models\",\n",
            "  \"Authors\": [\n",
            "    \"John Dagdelen\",\n",
            "    \"Alexander Dunn\",\n",
            "    \"Sanghoon Lee\",\n",
            "    \"Nicholas Walker\",\n",
            "    \"Andrew S. Rosen\",\n",
            "    \"Gerbrand Ceder\",\n",
            "    \"Kristin A. Persson\",\n",
            "    \"Anubhav Jain\"\n",
            "  ],\n",
            "  \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "  \"Keywords\": [\n",
            "    \"Named entity recognition\",\n",
            "    \"Relation extraction\",\n",
            "    \"Large language models\",\n",
            "    \"Materials chemistry\",\n",
            "    \"Dopants\",\n",
            "    \"Metal-organic frameworks\",\n",
            "    \"Composition/phase/morphology/application information extraction\"\n",
            "  ],\n",
            "  \"Abstract\": \"Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pre-trained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Structured information extraction from scientific text with large language models\",\n",
            "    \"Authors\": [\n",
            "        \"John Dagdelen\",\n",
            "        \"Alexander Dunn\",\n",
            "        \"Sanghoon Lee\",\n",
            "        \"Nicholas Walker\",\n",
            "        \"Andrew S. Rosen\",\n",
            "        \"Gerbrand Ceder\",\n",
            "        \"Kristin A. Persson\",\n",
            "        \"Anubhav Jain\"\n",
            "    ],\n",
            "    \"DOI\": \"10.1038/s41467-024-45563-x\",\n",
            "    \"Keywords\": [\n",
            "        \"Named entity recognition\",\n",
            "        \"Relation extraction\",\n",
            "        \"Large language models\",\n",
            "        \"Materials chemistry\",\n",
            "        \"Dopants\",\n",
            "        \"Metal-organic frameworks\",\n",
            "        \"Composition/phase/morphology/application information extraction\"\n",
            "    ],\n",
            "    \"Abstract\": \"Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pre-trained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/12.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7Uw76R0Nv50W",
        "outputId": "96d52054-448c-4bcc-be9a-4a0108b1ad98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "AI Open 00 (2023) 1–17\n",
            "AI Open\n",
            "Information Retrieval Meets Large Language Models: A Strategic\n",
            "Report from Chinese IR Community\n",
            "Qingyao AIa, Ting BAIb, Zhao CAOc, Yi CHANGd, Jiawei CHEN(\u0000)e, Zhumin CHENf , Zhiyong CHENGg,\n",
            "Shoubin DONGh, Zhicheng DOUi, Fuli FENG j, Shen GAO f , Jiafeng GUOk, Xiangnan HE(\u0000) j, Yanyan LANa,\n",
            "Chenliang LIl, Yiqun LIUa, Ziyu LYUm, Weizhi MAa, Jun MAf , Zhaochun REN f , Pengjie REN f , Zhiqiang\n",
            "WANGn, Mingwen WANGo, Ji-Rong WENi, Le WUp, Xin XIN f , Jun XUi, Dawei YINq, Peng ZHANG(\u0000)r,\n",
            "Fan ZHANGl, Weinan ZHANGs, Min ZHANGa, Xiaofei ZHUt\n",
            "aTsinghua University, bBeijing University of Posts and Telecommunications, cHuawei Technologies Ltd. Co, dJilin\n",
            "University, eZhejiang University, fShandong University, gShandong Artificial Intelligence Institute, hSouth China\n",
            "University of Technology, iRenmin University of China, jUniversity of Science and Technology of China, kInstitute\n",
            "of Computing Technology, Chinese Academy of Sciences, lWuhan University, mShenzhen Institute of Advanced\n",
            "Technology, Chinese Academy of Sciences, nShanxi University, oJiangxi Normal University, pHefei University of\n",
            "Technology, qBaidu Inc., rTianjin University, sShanghai Jiao Tong University, tChongqing University of\n",
            "Technology\n",
            "Abstract\n",
            "The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user\n",
            "information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding,\n",
            "generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval\n",
            "but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the\n",
            "synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for infor-\n",
            "mation seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community\",\n",
            "  \"Authors\": [\n",
            "    \"Qingyao AI\",\n",
            "    \"Ting BAI\",\n",
            "    \"Zhao CAO\",\n",
            "    \"Yi CHANG\",\n",
            "    \"Jiawei CHEN\",\n",
            "    \"Zhumin CHEN\",\n",
            "    \"Zhiyong CHENG\",\n",
            "    \"Shoubin DONG\",\n",
            "    \"Zhicheng DOU\",\n",
            "    \"Fuli FENG\",\n",
            "    \"Shen GAO\",\n",
            "    \"Jiafeng GUO\",\n",
            "    \"Xiangnan HE\",\n",
            "    \"Yanyan LAN\",\n",
            "    \"Chenliang LIl\",\n",
            "    \"Yiqun LIU\",\n",
            "    \"Ziyu LYU\",\n",
            "    \"Weizhi MA\",\n",
            "    \"Jun MA\",\n",
            "    \"Zhaochun REN\",\n",
            "    \"Pengjie REN\",\n",
            "    \"Zhiqiang WANG\",\n",
            "    \"Mingwen WAN\",\n",
            "    \"Ji-Rong WEN\",\n",
            "    \"Le WUP\",\n",
            "    \"Xin XIN\",\n",
            "    \"Jun XUi\",\n",
            "    \"Dawei YIN\",\n",
            "    \"Peng ZHANG\",\n",
            "    \"Fan ZHANG\",\n",
            "    \"Weinan ZHANG\",\n",
            "    \"Min ZHANG\",\n",
            "    \"Xiaofei ZHU\"\n",
            "  ],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\n",
            "    \"Information Retrieval\",\n",
            "    \"Large Language Models\",\n",
            "    \"LLMs\",\n",
            "    \"User Information Needs\",\n",
            "    \"Generative Retrieval\",\n",
            "    \"User Understanding\",\n",
            "    \"Model Evaluation\",\n",
            "    \"User-System Interactions\"\n",
            "  ],\n",
            "  \"Abstract\": \"The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans...\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community\",\n",
            "    \"Authors\": [\n",
            "        \"Qingyao AI\",\n",
            "        \"Ting BAI\",\n",
            "        \"Zhao CAO\",\n",
            "        \"Yi CHANG\",\n",
            "        \"Jiawei CHEN\",\n",
            "        \"Zhumin CHEN\",\n",
            "        \"Zhiyong CHENG\",\n",
            "        \"Shoubin DONG\",\n",
            "        \"Zhicheng DOU\",\n",
            "        \"Fuli FENG\",\n",
            "        \"Shen GAO\",\n",
            "        \"Jiafeng GUO\",\n",
            "        \"Xiangnan HE\",\n",
            "        \"Yanyan LAN\",\n",
            "        \"Chenliang LIl\",\n",
            "        \"Yiqun LIU\",\n",
            "        \"Ziyu LYU\",\n",
            "        \"Weizhi MA\",\n",
            "        \"Jun MA\",\n",
            "        \"Zhaochun REN\",\n",
            "        \"Pengjie REN\",\n",
            "        \"Zhiqiang WANG\",\n",
            "        \"Mingwen WAN\",\n",
            "        \"Ji-Rong WEN\",\n",
            "        \"Le WUP\",\n",
            "        \"Xin XIN\",\n",
            "        \"Jun XUi\",\n",
            "        \"Dawei YIN\",\n",
            "        \"Peng ZHANG\",\n",
            "        \"Fan ZHANG\",\n",
            "        \"Weinan ZHANG\",\n",
            "        \"Min ZHANG\",\n",
            "        \"Xiaofei ZHU\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Information Retrieval\",\n",
            "        \"Large Language Models\",\n",
            "        \"LLMs\",\n",
            "        \"User Information Needs\",\n",
            "        \"Generative Retrieval\",\n",
            "        \"User Understanding\",\n",
            "        \"Model Evaluation\",\n",
            "        \"User-System Interactions\"\n",
            "    ],\n",
            "    \"Abstract\": \"The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans...\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/14.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFbmWeUKycGG",
        "outputId": "72384e24-7468-4e93-9ed1-2e8fc06ea2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "LAME: Layout-Aware Metadata Extraction Approach for Research Articles \n",
            " \n",
            "JONGYUN CHOI1, HYESOO KONG2, HWAMOOK YOON2, HEUNG-SEON OH3, \n",
            "and YUCHUL JUNG1* \n",
            "1Department of Computer Engineering, Kumoh National Institute of Technology (KIT), Gumi, South Korea \n",
            "2Korea Institute of Science and Technology Information (KISTI), South Korea \n",
            "3School of Computer Science and Engineering, Korea University of Technology and Education (KOREATECH),  \n",
            "sCheonan, South Korea \n",
            " \n",
            "Abstract: The volume of academic literature, such as academic conference \n",
            "papers and journals, has increased rapidly worldwide, and research on metadata \n",
            "extraction is ongoing. However, high-performing metadata extraction is still \n",
            "challenging due to diverse layout formats according to journal publishers. To \n",
            "accommodate the diversity of the layouts of academic journals, we propose a \n",
            "novel LAyout-aware Metadata Extraction (LAME) framework equipped with the \n",
            "three characteristics (e.g., design of an automatic layout analysis, construction of \n",
            "a large meta-data training set, and construction of Layout-MetaBERT). We \n",
            "designed an automatic layout analysis using PDFMiner. Based on the layout \n",
            "analysis, a large volume of metadata-separated training data, including the title, \n",
            "abstract, author name, author affiliated organization, and keywords, were \n",
            "automatically extracted. Moreover, we constructed Layout-MetaBERT to extract \n",
            "the metadata from academic journals with varying layout formats. The \n",
            "experimental results with Layout-MetaBERT exhibited robust performance \n",
            "(Macro-F1, 93.27%) in metadata extraction for unseen journals with different \n",
            "layout formats. \n",
            "Keywords: Automatic layout analysis, Layout-MetaBERT, Metadata extraction, \n",
            "Research article \n",
            "1 Introduction \n",
            "With the development of science and technology, the number of related academic papers distributed \n",
            "periodically worldwide has reached more than several hundred thousand. However, their layout styles are \n",
            "as diverse as their subjects and pub\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"LAME: Layout-Aware Metadata Extraction Approach for Research Articles\",\n",
            "  \"Authors\": [\"JONGYUN CHOI\", \"HYESOO KONG\", \"HWAMOOK YOON\", \"HEUNG-SEON OH\", \"YUCHUL JUNG\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Automatic layout analysis\", \"Layout-MetaBERT\", \"Metadata extraction\", \"Research article\"],\n",
            "  \"Abstract\": \"The volume of academic literature, such as academic conference papers and journals, has increased rapidly worldwide, and research on metadata extraction is ongoing. However, high-performing metadata extraction is still challenging due to diverse layout formats according to journal publishers. To accommodate the diversity of the layouts of academic journals, we propose a novel LAyout-aware Metadata Extraction (LAME) framework equipped with the three characteristics (e.g., design of an automatic layout analysis, construction of a large meta-data training set, and construction of Layout-MetaBERT). We designed an automatic layout analysis using PDFMiner. Based on the layout analysis, a large volume of metadata-separated training data, including the title, abstract, author name, author affiliated organization, and keywords, were automatically extracted. Moreover, we constructed Layout-MetaBERT to extract the metadata from academic journals with varying layout formats. The experimental results with Layout-MetaBERT exhibited robust performance (Macro-F1, 93.27%) in metadata extraction for unseen journals with different layout formats.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"LAME: Layout-Aware Metadata Extraction Approach for Research Articles\",\n",
            "    \"Authors\": [\n",
            "        \"JONGYUN CHOI\",\n",
            "        \"HYESOO KONG\",\n",
            "        \"HWAMOOK YOON\",\n",
            "        \"HEUNG-SEON OH\",\n",
            "        \"YUCHUL JUNG\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Automatic layout analysis\",\n",
            "        \"Layout-MetaBERT\",\n",
            "        \"Metadata extraction\",\n",
            "        \"Research article\"\n",
            "    ],\n",
            "    \"Abstract\": \"The volume of academic literature, such as academic conference papers and journals, has increased rapidly worldwide, and research on metadata extraction is ongoing. However, high-performing metadata extraction is still challenging due to diverse layout formats according to journal publishers. To accommodate the diversity of the layouts of academic journals, we propose a novel LAyout-aware Metadata Extraction (LAME) framework equipped with the three characteristics (e.g., design of an automatic layout analysis, construction of a large meta-data training set, and construction of Layout-MetaBERT). We designed an automatic layout analysis using PDFMiner. Based on the layout analysis, a large volume of metadata-separated training data, including the title, abstract, author name, author affiliated organization, and keywords, were automatically extracted. Moreover, we constructed Layout-MetaBERT to extract the metadata from academic journals with varying layout formats. The experimental results with Layout-MetaBERT exhibited robust performance (Macro-F1, 93.27%) in metadata extraction for unseen journals with different layout formats.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/15.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y0H3foYs125v",
        "outputId": "0bb8c7da-deb2-4a5c-fe45-633f6bd07928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "\u0001\u0002\u0003\u0001\u0004\u0005\u0006\u0007\b\n",
            "\u0001\u0002\u0003\u0004\u0005\u0006\u0007\n",
            "Citation: Kim, H.; Choi, J.; Park, S.;\n",
            "Jung, Y. Layout Aware Semantic\n",
            "Element Extraction for Sustainable\n",
            "Science & Technology Decision\n",
            "Support. Sustainability 2022, 14, 2802.\n",
            "https://doi.org/10.3390/su14052802\n",
            "Academic Editor: Hamid Khayyam\n",
            "Received: 13 January 2022\n",
            "Accepted: 22 February 2022\n",
            "Published: 28 February 2022\n",
            "Publisher’s Note: MDPI stays neutral\n",
            "with regard to jurisdictional claims in\n",
            "published maps and institutional afﬁl-\n",
            "iations.\n",
            "Copyright:\n",
            "© 2022 by the authors.\n",
            "Licensee MDPI, Basel, Switzerland.\n",
            "This article is an open access article\n",
            "distributed\n",
            "under\n",
            "the\n",
            "terms\n",
            "and\n",
            "conditions of the Creative Commons\n",
            "Attribution (CC BY) license (https://\n",
            "creativecommons.org/licenses/by/\n",
            "4.0/).\n",
            "sustainability\n",
            "Article\n",
            "Layout Aware Semantic Element Extraction for Sustainable\n",
            "Science & Technology Decision Support\n",
            "Hyuntae Kim\n",
            ", Jongyun Choi, Soyoung Park and Yuchul Jung *\n",
            "Department of Computer Engineering, Kumoh National Institute of Technology, Gumi 39177, Korea;\n",
            "20216035@kumoh.ac.kr (H.K.); 20216093@kumoh.ac.kr (J.C.); haluna8836@kumoh.ac.kr (S.P.)\n",
            "* Correspondence: jyc@kumoh.ac.kr; Tel.: +82-54-478-7536\n",
            "Abstract: New scientiﬁc and technological (S&T) knowledge is being introduced rapidly, and hence,\n",
            "analysis efforts to understand and analyze new published S&T documents are increasing daily.\n",
            "Automated text mining and vision recognition techniques alleviate the burden somewhat, but the\n",
            "various document layout formats and knowledge content granularities across the S&T ﬁeld make\n",
            "it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph\n",
            "construction framework that simultaneously extracts meta-information and useful image objects from\n",
            "S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME),\n",
            "which can accurately extract metadata from various layout formats, and implement a transformer-\n",
            "based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize\n",
            "the vision\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Layout Aware Semantic Element Extraction for Sustainable Science & Technology Decision Support\",\n",
            "  \"Authors\": [\"Hyuntae Kim\", \"Jongyun Choi\", \"Soyoung Park\", \"Yuchul Jung\"],\n",
            "  \"DOI\": \"10.3390/su14052802\",\n",
            "  \"Keywords\": [\"Layout Aware\", \"Semantic Element Extraction\", \"Sustainable Science & Technology\", \"Decision Support\"],\n",
            "  \"Abstract\": \"New scientiﬁc and technological (S&T) knowledge is being introduced rapidly, and hence, analysis efforts to understand and analyze new published S&T documents are increasing daily. Automated text mining and vision recognition techniques alleviate the burden somewhat, but the various document layout formats and knowledge content granularities across the S&T ﬁeld make it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph construction framework that simultaneously extracts meta-information and useful image objects from S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME), which can accurately extract metadata from various layout formats, and implement a transformer-based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize the vision\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Layout Aware Semantic Element Extraction for Sustainable Science & Technology Decision Support\",\n",
            "    \"Authors\": [\n",
            "        \"Hyuntae Kim\",\n",
            "        \"Jongyun Choi\",\n",
            "        \"Soyoung Park\",\n",
            "        \"Yuchul Jung\"\n",
            "    ],\n",
            "    \"DOI\": \"10.3390/su14052802\",\n",
            "    \"Keywords\": [\n",
            "        \"Layout Aware\",\n",
            "        \"Semantic Element Extraction\",\n",
            "        \"Sustainable Science & Technology\",\n",
            "        \"Decision Support\"\n",
            "    ],\n",
            "    \"Abstract\": \"New scienti\\ufb01c and technological (S&T) knowledge is being introduced rapidly, and hence, analysis efforts to understand and analyze new published S&T documents are increasing daily. Automated text mining and vision recognition techniques alleviate the burden somewhat, but the various document layout formats and knowledge content granularities across the S&T \\ufb01eld make it challenging. Therefore, this paper proposes LA-SEE (LAME and Vi-SEE), a knowledge graph construction framework that simultaneously extracts meta-information and useful image objects from S&T documents in various layout formats. We adopt Layout-aware Metadata Extraction (LAME), which can accurately extract metadata from various layout formats, and implement a transformer-based instance segmentation (i.e., Vision based Semantic Elements Extraction (Vi-SEE)) to maximize the vision\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/16.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "59Nc7OCS4CZ9",
        "outputId": "e714439b-f073-4485-fd7d-8d0274cffad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Convolutional Neural Networks for Sentence Classiﬁcation\n",
            "Yoon Kim\n",
            "New York University\n",
            "yhk255@nyu.edu\n",
            "Abstract\n",
            "We report on a series of experiments with\n",
            "convolutional\n",
            "neural\n",
            "networks\n",
            "(CNN)\n",
            "trained on top of pre-trained word vec-\n",
            "tors for sentence-level classiﬁcation tasks.\n",
            "We show that a simple CNN with lit-\n",
            "tle hyperparameter tuning and static vec-\n",
            "tors achieves excellent results on multi-\n",
            "ple benchmarks.\n",
            "Learning task-speciﬁc\n",
            "vectors through ﬁne-tuning offers further\n",
            "gains in performance.\n",
            "We additionally\n",
            "propose a simple modiﬁcation to the ar-\n",
            "chitecture to allow for the use of both\n",
            "task-speciﬁc and static vectors. The CNN\n",
            "models discussed herein improve upon the\n",
            "state of the art on 4 out of 7 tasks, which\n",
            "include sentiment analysis and question\n",
            "classiﬁcation.\n",
            "1\n",
            "Introduction\n",
            "Deep learning models have achieved remarkable\n",
            "results in computer vision (Krizhevsky et al.,\n",
            "2012) and speech recognition (Graves et al., 2013)\n",
            "in recent years. Within natural language process-\n",
            "ing, much of the work with deep learning meth-\n",
            "ods has involved learning word vector representa-\n",
            "tions through neural language models (Bengio et\n",
            "al., 2003; Yih et al., 2011; Mikolov et al., 2013)\n",
            "and performing composition over the learned word\n",
            "vectors for classiﬁcation (Collobert et al., 2011).\n",
            "Word vectors, wherein words are projected from a\n",
            "sparse, 1-of-V encoding (here V is the vocabulary\n",
            "size) onto a lower dimensional vector space via a\n",
            "hidden layer, are essentially feature extractors that\n",
            "encode semantic features of words in their dimen-\n",
            "sions. In such dense representations, semantically\n",
            "close words are likewise close—in euclidean or\n",
            "cosine distance—in the lower dimensional vector\n",
            "space.\n",
            "Convolutional neural networks (CNN) utilize\n",
            "layers with convolving ﬁlters that are applied to\n",
            "local features (LeCun et al., 1998).\n",
            "Originally\n",
            "invented for computer vision, CNN models have\n",
            "subsequently been shown to be effective for NLP\n",
            "and have achieved excellent results in semantic\n",
            "parsing (Yih et al., 2014), sear\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Convolutional Neural Networks for Sentence Classiﬁcation\",\n",
            "  \"Authors\": [\"Yoon Kim\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Convolutional Neural Networks\", \"Sentence Classiﬁcation\", \"Word Vectors\", \"CNN\"],\n",
            "  \"Abstract\": \"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classiﬁcation tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-speciﬁc vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-speciﬁc and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classiﬁcation.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 1\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Convolutional Neural Networks for Sentence Classi\\ufb01cation\",\n",
            "    \"Authors\": [\n",
            "        \"Yoon Kim\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Convolutional Neural Networks\",\n",
            "        \"Sentence Classi\\ufb01cation\",\n",
            "        \"Word Vectors\",\n",
            "        \"CNN\"\n",
            "    ],\n",
            "    \"Abstract\": \"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classi\\ufb01cation tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-speci\\ufb01c vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-speci\\ufb01c and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classi\\ufb01cation.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 1\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/19.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if \"Error\" in extracted_text:\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mgO0EV2p4UZe",
        "outputId": "52d78976-f758-4195-d202-2b556f1b4b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Information Processing and Management 61 (2024) 103610\n",
            "Available online 14 December 2023\n",
            "0306-4573/©\n",
            "2023\n",
            "The\n",
            "Authors.\n",
            "Published\n",
            "by\n",
            "Elsevier\n",
            "Ltd.\n",
            "This\n",
            "is\n",
            "an\n",
            "open\n",
            "access\n",
            "article\n",
            "under\n",
            "the\n",
            "CC\n",
            "BY-NC\n",
            "license\n",
            "(http://creativecommons.org/licenses/by-nc/4.0/).\n",
            "Contents lists available at ScienceDirect\n",
            "Information Processing and Management\n",
            "journal homepage: www.elsevier.com/locate/ipm\n",
            "Predicting movies’ eudaimonic and hedonic scores: A machine\n",
            "learning approach using metadata, audio and visual features\n",
            "Elham Motamedi a,∗, Danial Khosh Kholgh b, Sorush Saghari c, Mehdi Elahi d,\n",
            "Francesco Barile e, Marko Tkalcic a\n",
            "a University of Primorska, Koper, Slovenia\n",
            "b University of Oulu, Oulu, Finland\n",
            "c K N Toosi University of Technology, Tehran, Iran\n",
            "d University of Bergen, Bergen, Norway\n",
            "e Maastricht University, Maastricht, Netherlands\n",
            "A R T I C L E\n",
            "I N F O\n",
            "Keywords:\n",
            "Eudaimonia\n",
            "Hedonia\n",
            "Machine learning approach\n",
            "Movie recommender systems\n",
            "A B S T R A C T\n",
            "In the task of modeling user preferences for movie recommender systems, recent research has\n",
            "demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and\n",
            "H scores), which reflect the depth of their message and the level of fun experience they provide,\n",
            "respectively. So far, the labeling of movies with their E and H scores has been done manually\n",
            "using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue,\n",
            "we propose an automatic approach for predicting E and H scores. Specifically, we collected\n",
            "E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this\n",
            "dataset with metadata, audio, and low-level and high-level visual features, and trained machine\n",
            "learning models for predicting the E and H scores of movies. This study investigates the use\n",
            "of machine learning models in predicting the E and H scores of movies using various feature\n",
            "sets, including audio, low-level and high-level visual features, and metadata. We compared the\n",
            "performance of pre\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Predicting movies’ eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features\",\n",
            "  \"Authors\": [\"Elham Motamedi\", \"Danial Khosh Kholgh\", \"Sorush Saghari\", \"Mehdi Elahi\", \"Francesco Barile\", \"Marko Tkalcic\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Eudaimonia\", \"Hedonia\", \"Machine learning approach\", \"Movie recommender systems\"],\n",
            "  \"Abstract\": \"In the task of modeling user preferences for movie recommender systems, recent research has demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and H scores), which reflect the depth of their message and the level of fun experience they provide, respectively. So far, the labeling of movies with their E and H scores has been done manually using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue, we propose an automatic approach for predicting E and H scores. Specifically, we collected E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this dataset with metadata, audio, and low-level and high-level visual features, and trained machine learning models for predicting the E and H scores of movies. This study investigates the use of machine learning models in predicting the E and H scores of movies using various feature sets, including audio, low-level and high-level visual features, and metadata. We compared the performance of pre...\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": null\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Predicting movies\\u2019 eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features\",\n",
            "    \"Authors\": [\n",
            "        \"Elham Motamedi\",\n",
            "        \"Danial Khosh Kholgh\",\n",
            "        \"Sorush Saghari\",\n",
            "        \"Mehdi Elahi\",\n",
            "        \"Francesco Barile\",\n",
            "        \"Marko Tkalcic\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Eudaimonia\",\n",
            "        \"Hedonia\",\n",
            "        \"Machine learning approach\",\n",
            "        \"Movie recommender systems\"\n",
            "    ],\n",
            "    \"Abstract\": \"In the task of modeling user preferences for movie recommender systems, recent research has demonstrated the benefits of describing movies with their eudaimonic and hedonic scores (E and H scores), which reflect the depth of their message and the level of fun experience they provide, respectively. So far, the labeling of movies with their E and H scores has been done manually using a dedicated instrument (a questionnaire), which is time-consuming. To address this issue, we propose an automatic approach for predicting E and H scores. Specifically, we collected E and H scores of 709 movies from 370 users (with a total of 3699 records), augmented this dataset with metadata, audio, and low-level and high-level visual features, and trained machine learning models for predicting the E and H scores of movies. This study investigates the use of machine learning models in predicting the E and H scores of movies using various feature sets, including audio, low-level and high-level visual features, and metadata. We compared the performance of pre...\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_json(text):\n",
        "    assistant_start = text.find(\"start_of_turn>model\")\n",
        "    if assistant_start == -1:\n",
        "        return {\"Error\": \"No assistant section found in output\"}\n",
        "\n",
        "    assistant_text = text[assistant_start:]\n",
        "\n",
        "    # Remove markdown-style code block markers (e.g. ```json or ```)\n",
        "    assistant_text = re.sub(r\"```(?:json)?|```\", \"\", assistant_text).strip()\n",
        "\n",
        "    start = assistant_text.find('{')\n",
        "    if start == -1:\n",
        "        return {\"Error\": \"No opening '{' found in assistant section\"}\n",
        "\n",
        "    brace_count = 0\n",
        "    for i in range(start, len(assistant_text)):\n",
        "        if assistant_text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif assistant_text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                json_str = assistant_text[start:i+1]\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except Exception as e:\n",
        "                    return {\"Error\": f\"JSON parse failed: {e}\"}\n",
        "\n",
        "    return {\"Error\": \"No complete JSON block found\"}\n",
        "\n",
        "def build_prompt(text):\n",
        "    instruction = f\"\"\"\n",
        "You are an AI that extracts structured metadata from research papers.\n",
        "\n",
        "Return ONLY valid JSON with the following structure and no extra text:\n",
        "\n",
        "{{\n",
        "  \"Title\": \"Paper title\",\n",
        "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
        "  \"DOI\": \"DOI if available\",\n",
        "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
        "  \"Abstract\": \"Abstract text\",\n",
        "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
        "  \"Number of References\": 10\n",
        "}}\n",
        "\n",
        "Here is the paper content:\n",
        "{text[:2000]}\n",
        "\"\"\"\n",
        "\n",
        "    return (\n",
        "        \"<start_of_turn>user\\n\"\n",
        "        + instruction.strip() +\n",
        "        \"\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    )\n",
        "\n",
        "def extract_metadata(generator, paper_text):\n",
        "    prompt = build_prompt(paper_text)\n",
        "    response = generator(prompt, max_new_tokens=1000, do_sample=False, temperature=0)\n",
        "    raw_output = response[0][\"generated_text\"]\n",
        "\n",
        "    print(\"\\n==== RAW MODEL OUTPUT ====\")\n",
        "    print(raw_output)\n",
        "    print(\"==== END RAW OUTPUT ====\\n\")\n",
        "\n",
        "    return extract_json(raw_output)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "    return text if text.strip() else \"Error: No extractable text found in PDF.\"\n",
        "\n",
        "\n",
        "pdf_path = \"/content/20.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if extracted_text.startswith(\"Error:\"):\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RKSmMtk5CcNN",
        "outputId": "996934a1-534a-473b-b1dd-0306878b47c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Contents lists available at ScienceDirect\n",
            "Information Processing and Management\n",
            "journal homepage: www.elsevier.com/locate/infoproman\n",
            "Deep Learning-based Extraction of Algorithmic Metadata in Full-\n",
            "Text Scholarly Documents\n",
            "Iqra Safdera, Saeed-Ul Hassana, Anna Visvizib, Thanapon Norasetc, Raheel Nawazd,\n",
            "Suppawong Tuarobc,⁎\n",
            "a Information Technology University, 346-B, Ferozepur Road, Lahore, Pakistan\n",
            "b Deree College - The American College of Greece, 6 Gravias Street, 153-42 Aghia Paraskevi, Athens, Greece\n",
            "c Faculty of Information and Communication Technology, Mahidol University, Thailand\n",
            "d Department of Operations, Technology Events and Hospitality Management, Manchester Metropolitan University, Manchester, United Kingdom\n",
            "A R T I C L E I N F O\n",
            "Keywords:\n",
            "Knowledge-based Systems\n",
            "Algorithmic Metadata\n",
            "Algorithm Search\n",
            "Deep Learning\n",
            "Bi-Directional LSTM\n",
            "Information Retrieval\n",
            "Full-text Articles\n",
            "A B S T R A C T\n",
            "The advancements of search engines for traditional text documents have enabled the effective\n",
            "retrieval of massive textual information in a resource-efficient manner. However, such conven-\n",
            "tional search methodologies often suffer from poor retrieval accuracy especially when documents\n",
            "exhibit unique properties that behoove specialized and deeper semantic extraction. Recently,\n",
            "AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and\n",
            "shallow textual metadata from scientific publications and treats them as traditional documents so\n",
            "that the conventional search engine methodology could be applied. However, such a system fails\n",
            "to facilitate user search queries that seek to identify algorithm-specific information, such as the\n",
            "datasets on which algorithms operate, the performance of algorithms, and runtime complexity,\n",
            "etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are\n",
            "presented. Specifically, we propose a set of methods to automatically identify and extract algo-\n",
            "rithmic pseudo-codes and the sente\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Deep Learning-based Extraction of Algorithmic Metadata in Full-Text Scholarly Documents\",\n",
            "  \"Authors\": [\"Iqra Safdera\", \"Saeed-Ul Hassana\", \"Anna Visvizib\", \"Thanapon Norasetc\", \"Raheel Nawaz\", \"Suppawong Tuarobc\", \"⁎\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Knowledge-based Systems\", \"Algorithmic Metadata\", \"Algorithm Search\", \"Deep Learning\", \"Bi-Directional LSTM\", \"Information Retrieval\", \"Full-text Articles\"],\n",
            "  \"Abstract\": \"The advancements of search engines for traditional text documents have enabled the effective retrieval of massive textual information in a resource-efficient manner. However, such conventional search methodologies often suffer from poor retrieval accuracy especially when documents exhibit unique properties that behoove specialized and deeper semantic extraction. Recently, AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and shallow textual metadata from scientific publications and treats them as traditional documents so that the conventional search engine methodology could be applied. However, such a system fails to facilitate user search queries that seek to identify algorithm-specific information, such as the datasets on which algorithms operate, the performance of algorithms, and runtime complexity, etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are presented. Specifically, we propose a set of methods to automatically identify and extract algorithmic pseudo-codes and the sente\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 0 \n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Deep Learning-based Extraction of Algorithmic Metadata in Full-Text Scholarly Documents\",\n",
            "    \"Authors\": [\n",
            "        \"Iqra Safdera\",\n",
            "        \"Saeed-Ul Hassana\",\n",
            "        \"Anna Visvizib\",\n",
            "        \"Thanapon Norasetc\",\n",
            "        \"Raheel Nawaz\",\n",
            "        \"Suppawong Tuarobc\",\n",
            "        \"\\u204e\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Knowledge-based Systems\",\n",
            "        \"Algorithmic Metadata\",\n",
            "        \"Algorithm Search\",\n",
            "        \"Deep Learning\",\n",
            "        \"Bi-Directional LSTM\",\n",
            "        \"Information Retrieval\",\n",
            "        \"Full-text Articles\"\n",
            "    ],\n",
            "    \"Abstract\": \"The advancements of search engines for traditional text documents have enabled the effective retrieval of massive textual information in a resource-efficient manner. However, such conventional search methodologies often suffer from poor retrieval accuracy especially when documents exhibit unique properties that behoove specialized and deeper semantic extraction. Recently, AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and shallow textual metadata from scientific publications and treats them as traditional documents so that the conventional search engine methodology could be applied. However, such a system fails to facilitate user search queries that seek to identify algorithm-specific information, such as the datasets on which algorithms operate, the performance of algorithms, and runtime complexity, etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are presented. Specifically, we propose a set of methods to automatically identify and extract algorithmic pseudo-codes and the sente\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/13.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if extracted_text.startswith(\"Error:\"):\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T95vucbBCdgg",
        "outputId": "a75a18d5-49b5-4ed0-a2ec-5e85169377e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "RESEARCH ARTICLE\n",
            "Building an annotated corpus for automatic\n",
            "metadata extraction from multilingual journal\n",
            "article references\n",
            "Wonjun Choi, Hwa-Mook Yoon, Mi-Hwan Hyun, Hye-Jin Lee, Jae-Wook Seol, Kangsan\n",
            "Dajeong Lee, Young Joon Yoon, Hyesoo KongID*\n",
            "Digital Curation Center, Korea Institute of Science and Technology Information, Daejeon, Republic of Korea\n",
            "* hyesoo@kisti.re.kr\n",
            "Abstract\n",
            "Bibliographic references containing citation information of academic literature play an impor-\n",
            "tant role as a medium connecting earlier and recent studies. As references contain\n",
            "machine-readable metadata such as author name, title, or publication year, they have been\n",
            "widely used in the field of citation information services including search services for scholarly\n",
            "information and research trend analysis. Many institutions around the world manually\n",
            "extract and continuously accumulate reference metadata to provide various scholarly ser-\n",
            "vices. However, manually collection of reference metadata every year continues to be a bur-\n",
            "den because of the associated cost and time consumption. With the accumulation of a large\n",
            "volume of academic literature, several tools, including GROBID and CERMINE, that auto-\n",
            "matically extract reference metadata have been released. However, these tools have some\n",
            "limitations. For example, they are only applicable to references written in English, the types\n",
            "of extractable metadata are limited for each tool, and the performance of the tools is insuffi-\n",
            "cient to replace the manual extraction of reference metadata. Therefore, in this study, we\n",
            "focused on constructing a high-quality corpus to automatically extract metadata from multi-\n",
            "lingual journal article references. Using our constructed corpus, we trained and evaluated a\n",
            "BERT-based transfer-learning model. Furthermore, we compared the performance of the\n",
            "BERT-based model with that of the existing model, GROBID. Currently, our corpus contains\n",
            "3,815,987 multilingual references, mainly in English and Korean, with labels f\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Building an annotated corpus for automatic metadata extraction from multilingual journal article references\",\n",
            "  \"Authors\": [\"Wonjun Choi\", \"Hwa-Mook Yoon\", \"Mi-Hwan Hyun\", \"Hye-Jin Lee\", \"Jae-Wook Seol\", \"Kangsan Dajeong Lee\", \"Young Joon Yoon\", \"Hyesoo Kong\"],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\"Bibliographic references\", \"citation information\", \"metadata extraction\", \"multilingual\", \"journal article references\"],\n",
            "  \"Abstract\": \"Bibliographic references containing citation information of academic literature play an important role as a medium connecting earlier and recent studies. As references contain machine-readable metadata such as author name, title, or publication year, they have been widely used in the field of citation information services including search services for scholarly information and research trend analysis. Many institutions around the world manually extract and continuously accumulate reference metadata to provide various scholarly services. However, manually collection of reference metadata every year continues to be a burden because of the associated cost and time consumption. With the accumulation of a large volume of academic literature, several tools, including GROBID and CERMINE, that automatically extract reference metadata have been released. However, these tools have some limitations. For example, they are only applicable to references written in English, the types of extractable metadata are limited for each tool, and the performance of the tools is insufficient to replace the manual extraction of reference metadata. Therefore, in this study, we focused on constructing a high-quality corpus to automatically extract metadata from multilingual journal article references. Using our constructed corpus, we trained and evaluated a BERT-based transfer-learning model. Furthermore, we compared the performance of the BERT-based model with that of the existing model, GROBID. Currently, our corpus contains 3,815,987 multilingual references, mainly in English and Korean, with labels f\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 3815987\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Building an annotated corpus for automatic metadata extraction from multilingual journal article references\",\n",
            "    \"Authors\": [\n",
            "        \"Wonjun Choi\",\n",
            "        \"Hwa-Mook Yoon\",\n",
            "        \"Mi-Hwan Hyun\",\n",
            "        \"Hye-Jin Lee\",\n",
            "        \"Jae-Wook Seol\",\n",
            "        \"Kangsan Dajeong Lee\",\n",
            "        \"Young Joon Yoon\",\n",
            "        \"Hyesoo Kong\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Bibliographic references\",\n",
            "        \"citation information\",\n",
            "        \"metadata extraction\",\n",
            "        \"multilingual\",\n",
            "        \"journal article references\"\n",
            "    ],\n",
            "    \"Abstract\": \"Bibliographic references containing citation information of academic literature play an important role as a medium connecting earlier and recent studies. As references contain machine-readable metadata such as author name, title, or publication year, they have been widely used in the field of citation information services including search services for scholarly information and research trend analysis. Many institutions around the world manually extract and continuously accumulate reference metadata to provide various scholarly services. However, manually collection of reference metadata every year continues to be a burden because of the associated cost and time consumption. With the accumulation of a large volume of academic literature, several tools, including GROBID and CERMINE, that automatically extract reference metadata have been released. However, these tools have some limitations. For example, they are only applicable to references written in English, the types of extractable metadata are limited for each tool, and the performance of the tools is insufficient to replace the manual extraction of reference metadata. Therefore, in this study, we focused on constructing a high-quality corpus to automatically extract metadata from multilingual journal article references. Using our constructed corpus, we trained and evaluated a BERT-based transfer-learning model. Furthermore, we compared the performance of the BERT-based model with that of the existing model, GROBID. Currently, our corpus contains 3,815,987 multilingual references, mainly in English and Korean, with labels f\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 3815987\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/17.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if extracted_text.startswith(\"Error:\"):\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "onED39hk16Tr",
        "outputId": "4d4d6486-0570-49ad-9fa5-34dfa69f479e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Information extraction from research papers\n",
            "using conditional random ﬁelds q\n",
            "Fuchun Peng a,*, Andrew McCallum b\n",
            "a BBN Technologies, 50 Moulton Street, Cambridge, MA 02138, United States\n",
            "b Department of Computer Science, University of Massachusetts Amherst, 140 Governors Drive,\n",
            "Amherst, MA 01003, United States\n",
            "Received 31 March 2005\n",
            "Available online 6 December 2005\n",
            "Abstract\n",
            "With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring deci-\n",
            "sions, the accuracy of such systems is of paramount importance. This article employs conditional random ﬁelds (CRFs) for\n",
            "the task of extracting various common ﬁelds from the headers and citation of research papers. CRFs provide a principled\n",
            "way for incorporating various local features, external lexicon features and globle layout features. The basic theory of CRFs\n",
            "is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We\n",
            "make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for\n",
            "improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for con-\n",
            "straint co-reference information extraction; i.e., improving extraction performance given that we know some citations refer\n",
            "to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in\n",
            "average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares\n",
            "even more favorably against HMMs. On four co-reference IE datasets, our system signiﬁcantly improves extraction per-\n",
            "formance, with an error rate reduction of 6–14%.\n",
            "\u0001 2005 Elsevier Ltd. All rights reserved.\n",
            "Keywords: Information extraction; Constraint information extraction; Conditional random ﬁelds; Regularization\n",
            "1. Introduction\n",
            "Research paper search engines, such as CiteSeer (Lawrence, Giles, & Bollacker, 1999)\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Information extraction from research papers using conditional random ﬁelds q\",\n",
            "  \"Authors\": [\n",
            "    \"Fuchun Peng\",\n",
            "    \"Andrew McCallum\"\n",
            "  ],\n",
            "  \"DOI\": \"null\",\n",
            "  \"Keywords\": [\n",
            "    \"Information extraction\",\n",
            "    \"Constraint information extraction\",\n",
            "    \"Conditional random ﬁelds\",\n",
            "    \"Regularization\"\n",
            "  ],\n",
            "  \"Abstract\": \"With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring deci-sions, the accuracy of such systems is of paramount importance. This article employs conditional random ﬁelds (CRFs) for the task of extracting various common ﬁelds from the headers and citation of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features and globle layout features. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint co-reference information extraction; i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system signiﬁcantly improves extraction per-formance, with an error rate reduction of 6–14%.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Information extraction from research papers using conditional random \\ufb01elds q\",\n",
            "    \"Authors\": [\n",
            "        \"Fuchun Peng\",\n",
            "        \"Andrew McCallum\"\n",
            "    ],\n",
            "    \"DOI\": \"null\",\n",
            "    \"Keywords\": [\n",
            "        \"Information extraction\",\n",
            "        \"Constraint information extraction\",\n",
            "        \"Conditional random \\ufb01elds\",\n",
            "        \"Regularization\"\n",
            "    ],\n",
            "    \"Abstract\": \"With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring deci-sions, the accuracy of such systems is of paramount importance. This article employs conditional random \\ufb01elds (CRFs) for the task of extracting various common \\ufb01elds from the headers and citation of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features and globle layout features. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint co-reference information extraction; i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system signi\\ufb01cantly improves extraction per-formance, with an error rate reduction of 6\\u201314%.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/18.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if extracted_text.startswith(\"Error:\"):\n",
        "        #st.error(\"No extractable text found in the PDF.\")\n",
        "  print(\"No extractable text found in the PDF.\")\n",
        "else:\n",
        "        metadata = extract_metadata(generator, extracted_text)\n",
        "        print(\"\\n==== Extracted Metadata ====\")\n",
        "        print(json.dumps(metadata, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "6b421c48-ff9d-46f6-b4d3-6c28bd25a9f9",
        "id": "94tM7n_r7tF1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RAW MODEL OUTPUT ====\n",
            "<start_of_turn>user\n",
            "You are an AI that extracts structured metadata from research papers.\n",
            "\n",
            "Return ONLY valid JSON with the following structure and no extra text:\n",
            "\n",
            "{\n",
            "  \"Title\": \"Paper title\",\n",
            "  \"Authors\": [\"Author 1\", \"Author 2\"],\n",
            "  \"DOI\": \"DOI if available\",\n",
            "  \"Keywords\": [\"Keyword1\", \"Keyword2\"],\n",
            "  \"Abstract\": \"Abstract text\",\n",
            "  \"Document Type\": \"Research Paper, Thesis, etc.\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "\n",
            "Here is the paper content:\n",
            "Information extraction from scientific articles: a survey\n",
            "Zara Nasar1\n",
            "• Syed Waqar Jaffry1 • Muhammad Kamran Malik1\n",
            "Received: 19 May 2018 / Published online: 29 September 2018\n",
            "\u0002 Akade´miai Kiado´, Budapest, Hungary 2018\n",
            "Abstract\n",
            "In last few decades, with the advent of World Wide Web (WWW), world is being over-\n",
            "loaded with huge data. This huge data carries potential information that once extracted, can\n",
            "be used for betterment of humanity. Information from this data can be extracted using\n",
            "manual and automatic analysis. Manual analysis is not scalable and efﬁcient, whereas, the\n",
            "automatic analysis involves computing mechanisms that aid in automatic information\n",
            "extraction over huge amount of data. WWW has also affected overall growth in scientiﬁc\n",
            "literature that makes the process of literature review quite laborious, time consuming and\n",
            "cumbersome job for researchers. Hence a dire need is felt to automatically extract potential\n",
            "information out of immense set of scientiﬁc articles to automate the process of literature\n",
            "review. Therefore, in this study, aim is to present the overall progress concerning automatic\n",
            "information extraction from scientiﬁc articles. The information insights extracted from\n",
            "scientiﬁc articles are classiﬁed in two broad categories i.e. metadata and key-insights. As\n",
            "available benchmark datasets carry a signiﬁcant role in overall development in this\n",
            "research domain, existing datasets against both categories are extensively reviewed. Later,\n",
            "research studies in literature that have applied various computational approaches applied\n",
            "on these datasets are consolidated. Major computational approaches in this regard include\n",
            "Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support\n",
            "Vector Machines, Naı¨ve-Bayes classiﬁcation and Deep Learning approaches. Currently,\n",
            "there are multiple projects going on that are focused towards the dataset construction\n",
            "tailored to speciﬁc information needs from scientiﬁc articles. Hence, in this study, state-o\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```json\n",
            "{\n",
            "  \"Title\": \"Information extraction from scientific articles: a survey\",\n",
            "  \"Authors\": [\n",
            "    \"Zara Nasar\",\n",
            "    \"Syed Waqar Jaffry\",\n",
            "    \"Muhammad Kamran Malik\"\n",
            "  ],\n",
            "  \"DOI\": null,\n",
            "  \"Keywords\": [\n",
            "    \"Information extraction\",\n",
            "    \"scientific articles\",\n",
            "    \"automatic analysis\",\n",
            "    \"literature review\"\n",
            "  ],\n",
            "  \"Abstract\": \"In last few decades, with the advent of World Wide Web (WWW), world is being over-loaded with huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and efﬁcient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scientiﬁc literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scientiﬁc articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scientiﬁc articles. The information insights extracted from scientiﬁc articles are classiﬁed in two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a signiﬁcant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Naı¨ve-Bayes classiﬁcation and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to speciﬁc information needs from scientiﬁc articles. Hence, in this study, state-of-the-art techniques and challenges in information extraction from scientific articles are discussed.\",\n",
            "  \"Document Type\": \"Research Paper\",\n",
            "  \"Number of References\": 10\n",
            "}\n",
            "```\n",
            "==== END RAW OUTPUT ====\n",
            "\n",
            "\n",
            "==== Extracted Metadata ====\n",
            "{\n",
            "    \"Title\": \"Information extraction from scientific articles: a survey\",\n",
            "    \"Authors\": [\n",
            "        \"Zara Nasar\",\n",
            "        \"Syed Waqar Jaffry\",\n",
            "        \"Muhammad Kamran Malik\"\n",
            "    ],\n",
            "    \"DOI\": null,\n",
            "    \"Keywords\": [\n",
            "        \"Information extraction\",\n",
            "        \"scientific articles\",\n",
            "        \"automatic analysis\",\n",
            "        \"literature review\"\n",
            "    ],\n",
            "    \"Abstract\": \"In last few decades, with the advent of World Wide Web (WWW), world is being over-loaded with huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and ef\\ufb01cient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scienti\\ufb01c literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scienti\\ufb01c articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scienti\\ufb01c articles. The information insights extracted from scienti\\ufb01c articles are classi\\ufb01ed in two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a signi\\ufb01cant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Na\\u0131\\u00a8ve-Bayes classi\\ufb01cation and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to speci\\ufb01c information needs from scienti\\ufb01c articles. Hence, in this study, state-of-the-art techniques and challenges in information extraction from scientific articles are discussed.\",\n",
            "    \"Document Type\": \"Research Paper\",\n",
            "    \"Number of References\": 10\n",
            "}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f96366508264dc388f83a493598a920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55d5b4a2521b48c6884d2244f8b0d523",
              "IPY_MODEL_4c8a360a4e5145edb19d53922ec05b16",
              "IPY_MODEL_0a4959d104214b8daece5e70961cdd66"
            ],
            "layout": "IPY_MODEL_085d6494959d4db1b16bf5f16c235165"
          }
        },
        "55d5b4a2521b48c6884d2244f8b0d523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c475af795ecd4739a1eb4588c9456fed",
            "placeholder": "​",
            "style": "IPY_MODEL_cfaf70fa0a2e40f481066ac4b62b0685",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4c8a360a4e5145edb19d53922ec05b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4bd1bcf9104458087b94cf4c1acca5a",
            "max": 46996,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cdf80a9e44e471382d023e879fcaaac",
            "value": 46996
          }
        },
        "0a4959d104214b8daece5e70961cdd66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_328e424404dd46d7bfef65e669d0a34f",
            "placeholder": "​",
            "style": "IPY_MODEL_7cad1efa206c40fda7406e32811eebb4",
            "value": " 47.0k/47.0k [00:00&lt;00:00, 1.90MB/s]"
          }
        },
        "085d6494959d4db1b16bf5f16c235165": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c475af795ecd4739a1eb4588c9456fed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfaf70fa0a2e40f481066ac4b62b0685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4bd1bcf9104458087b94cf4c1acca5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cdf80a9e44e471382d023e879fcaaac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "328e424404dd46d7bfef65e669d0a34f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cad1efa206c40fda7406e32811eebb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "801ef077f5b143c1ae5d7ace50233561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a1a0d3e77814464bf9ee31b6b098ec0",
              "IPY_MODEL_a25644c164a44cf498997254aa7a9af6",
              "IPY_MODEL_41033f8cd0a1479b8bd9285dc22eec95"
            ],
            "layout": "IPY_MODEL_6580634b5c944193899f6db72e542e75"
          }
        },
        "8a1a0d3e77814464bf9ee31b6b098ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e55e9a01eaf48ab95f1d3c9f6681d9a",
            "placeholder": "​",
            "style": "IPY_MODEL_5c0c21b5848748c5a08c0b8ad4d30866",
            "value": "tokenizer.model: 100%"
          }
        },
        "a25644c164a44cf498997254aa7a9af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2596443f26ea4558a4fd89fe167055fc",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1a7832a94314401847910e10474504a",
            "value": 4241003
          }
        },
        "41033f8cd0a1479b8bd9285dc22eec95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e89cba5eca374c8e91f49661a2e09517",
            "placeholder": "​",
            "style": "IPY_MODEL_f7deb42401a74e05a5748ee2d04d76f4",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 15.9MB/s]"
          }
        },
        "6580634b5c944193899f6db72e542e75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e55e9a01eaf48ab95f1d3c9f6681d9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0c21b5848748c5a08c0b8ad4d30866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2596443f26ea4558a4fd89fe167055fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1a7832a94314401847910e10474504a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e89cba5eca374c8e91f49661a2e09517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7deb42401a74e05a5748ee2d04d76f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc3070d5c94c4c1d8abb3967c0843c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a0ee169fd8d4394a9d7aadac7952a2a",
              "IPY_MODEL_d0bc55f571954f639d34c8d63ac89b4c",
              "IPY_MODEL_08be68294e3f4125a5f6c999d9871a02"
            ],
            "layout": "IPY_MODEL_31837897e0ed473f8a1d26ede1fee5e5"
          }
        },
        "1a0ee169fd8d4394a9d7aadac7952a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b17a1c8f540a4f9fad7b2db27d52c878",
            "placeholder": "​",
            "style": "IPY_MODEL_4cf3d12f90f14d829180e4eaa5579719",
            "value": "tokenizer.json: 100%"
          }
        },
        "d0bc55f571954f639d34c8d63ac89b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73bb07c01a82495189615025efc0f2e0",
            "max": 17525357,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4267c27eeb284d7ea67816a07d1b9989",
            "value": 17525357
          }
        },
        "08be68294e3f4125a5f6c999d9871a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ff9f0a26a884465ae6b84d588ac4fab",
            "placeholder": "​",
            "style": "IPY_MODEL_c3aa46f2533049a5b43dce122db5f2a2",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 68.1MB/s]"
          }
        },
        "31837897e0ed473f8a1d26ede1fee5e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17a1c8f540a4f9fad7b2db27d52c878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cf3d12f90f14d829180e4eaa5579719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73bb07c01a82495189615025efc0f2e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4267c27eeb284d7ea67816a07d1b9989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ff9f0a26a884465ae6b84d588ac4fab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3aa46f2533049a5b43dce122db5f2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c97703599f749bd9a4a1268ed8f1d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61e765b171c34e7ab29cf731d817a2d0",
              "IPY_MODEL_5a4efa6ef4254884a1ebb380e36ea7e1",
              "IPY_MODEL_2f9b0cec96194a41aa009bf87fc8f88f"
            ],
            "layout": "IPY_MODEL_6267104b002a4949a35e390ece2c63cd"
          }
        },
        "61e765b171c34e7ab29cf731d817a2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea0aed4f26c84eeeafb15221e90c0fb0",
            "placeholder": "​",
            "style": "IPY_MODEL_f94e481b0743425aa5ebfb939b18d976",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5a4efa6ef4254884a1ebb380e36ea7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4258d139f124e129ae7760a3951d813",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a580a76ef8a34340b96461d9c4e48c0d",
            "value": 636
          }
        },
        "2f9b0cec96194a41aa009bf87fc8f88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b53d9c937eb240c1bc8c35bd92a6d48c",
            "placeholder": "​",
            "style": "IPY_MODEL_28e86def2a3840d4be4d1475676026cb",
            "value": " 636/636 [00:00&lt;00:00, 57.4kB/s]"
          }
        },
        "6267104b002a4949a35e390ece2c63cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea0aed4f26c84eeeafb15221e90c0fb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f94e481b0743425aa5ebfb939b18d976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4258d139f124e129ae7760a3951d813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a580a76ef8a34340b96461d9c4e48c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b53d9c937eb240c1bc8c35bd92a6d48c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28e86def2a3840d4be4d1475676026cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c31fb987b5284d1fb85c731d2b50c0e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d85782446d54496c860e173e0e97c818",
              "IPY_MODEL_eb13b086520846e0ad825045dccbf458",
              "IPY_MODEL_8d7bf53f998e4d3488bceec5d4bcec2a"
            ],
            "layout": "IPY_MODEL_442814801721414280ddbc56e25e3380"
          }
        },
        "d85782446d54496c860e173e0e97c818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04fa2e6965934fb5922bdef45a5b8c83",
            "placeholder": "​",
            "style": "IPY_MODEL_fc9020a59977457a87414202adc45875",
            "value": "config.json: 100%"
          }
        },
        "eb13b086520846e0ad825045dccbf458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2790b416844b491c8f7648a3f108daa7",
            "max": 838,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bef93f2552cb496991943f5a9f911d18",
            "value": 838
          }
        },
        "8d7bf53f998e4d3488bceec5d4bcec2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e9188c5beb642289db6e96ff055c6b7",
            "placeholder": "​",
            "style": "IPY_MODEL_db35067edbf44c48a95cd68018e8d974",
            "value": " 838/838 [00:00&lt;00:00, 34.1kB/s]"
          }
        },
        "442814801721414280ddbc56e25e3380": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04fa2e6965934fb5922bdef45a5b8c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc9020a59977457a87414202adc45875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2790b416844b491c8f7648a3f108daa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef93f2552cb496991943f5a9f911d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e9188c5beb642289db6e96ff055c6b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db35067edbf44c48a95cd68018e8d974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f23f3d105224b9b82cee9b2d12060ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f28562248c244cebba9ab33d6253faa",
              "IPY_MODEL_0eead89f3e474ae4a3f5ed13a2a8956a",
              "IPY_MODEL_92f25615a4054b8b9b36d4677090e82e"
            ],
            "layout": "IPY_MODEL_28316964628b46d1a219ccdda4f69eb4"
          }
        },
        "7f28562248c244cebba9ab33d6253faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e33095c4aae43faa9e49ce398a09589",
            "placeholder": "​",
            "style": "IPY_MODEL_c9e20381f32646d38b70dde2c420d64a",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "0eead89f3e474ae4a3f5ed13a2a8956a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f61a73f2da44826bb9973d96108760e",
            "max": 24223,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_094b7a50e7634d0789c54bd64e002368",
            "value": 24223
          }
        },
        "92f25615a4054b8b9b36d4677090e82e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce84ce34b7654184972a04253ac47602",
            "placeholder": "​",
            "style": "IPY_MODEL_a075e2242c584bacae8e2b1e34e1f72e",
            "value": " 24.2k/24.2k [00:00&lt;00:00, 1.48MB/s]"
          }
        },
        "28316964628b46d1a219ccdda4f69eb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e33095c4aae43faa9e49ce398a09589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9e20381f32646d38b70dde2c420d64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f61a73f2da44826bb9973d96108760e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "094b7a50e7634d0789c54bd64e002368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce84ce34b7654184972a04253ac47602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a075e2242c584bacae8e2b1e34e1f72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d04a078964748b98399c7f9b398ec43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ba94b2852f148e5a0fa15c5225a4f80",
              "IPY_MODEL_095f323eb81a4d2080744d6d3355c392",
              "IPY_MODEL_c14cb93b2f9742c99ba6681f8b5370ea"
            ],
            "layout": "IPY_MODEL_f914e60b4f1c4f22a16c4e032d59b611"
          }
        },
        "4ba94b2852f148e5a0fa15c5225a4f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4818208cb534cb18be32fd6c6c90800",
            "placeholder": "​",
            "style": "IPY_MODEL_4f8c2510e68d42629c2a5be5dacfe6b2",
            "value": "Fetching 2 files: 100%"
          }
        },
        "095f323eb81a4d2080744d6d3355c392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3d4e105625a4096a67626c82afabf6c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5edb8dd173c426c8720d1a3bf173a83",
            "value": 2
          }
        },
        "c14cb93b2f9742c99ba6681f8b5370ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_776b316f5bd94748b9f643b1b8375b34",
            "placeholder": "​",
            "style": "IPY_MODEL_9605c2668999421282cd0b38a3e878b3",
            "value": " 2/2 [00:25&lt;00:00, 25.86s/it]"
          }
        },
        "f914e60b4f1c4f22a16c4e032d59b611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4818208cb534cb18be32fd6c6c90800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8c2510e68d42629c2a5be5dacfe6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3d4e105625a4096a67626c82afabf6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5edb8dd173c426c8720d1a3bf173a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "776b316f5bd94748b9f643b1b8375b34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9605c2668999421282cd0b38a3e878b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e3a54f60f7d405a9d6ec28feb4d6ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54235e29b1654288a0784a03f7481244",
              "IPY_MODEL_8f628296853842c687f780809585d4ed",
              "IPY_MODEL_22f2bb3900aa4029b0cb255421a4748d"
            ],
            "layout": "IPY_MODEL_30ba4fa3dcb641778ead4028e43cd4a3"
          }
        },
        "54235e29b1654288a0784a03f7481244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c792ceaf7c3457691e9a6f5f183b9b8",
            "placeholder": "​",
            "style": "IPY_MODEL_94b10860e96149308c4fc37eaf260ad5",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "8f628296853842c687f780809585d4ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5858a0be324b3facfe869f961e1db9",
            "max": 4988025760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7dadd37ec1547888da3643729334606",
            "value": 4988025760
          }
        },
        "22f2bb3900aa4029b0cb255421a4748d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2925c3eea8b54a4e90a988b6e6abf924",
            "placeholder": "​",
            "style": "IPY_MODEL_7a4d3f7dfbd84a0f934be514a19273ce",
            "value": " 4.99G/4.99G [00:25&lt;00:00, 232MB/s]"
          }
        },
        "30ba4fa3dcb641778ead4028e43cd4a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c792ceaf7c3457691e9a6f5f183b9b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b10860e96149308c4fc37eaf260ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c5858a0be324b3facfe869f961e1db9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7dadd37ec1547888da3643729334606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2925c3eea8b54a4e90a988b6e6abf924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4d3f7dfbd84a0f934be514a19273ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38dae17b491843ad93fd4fc14b9d21c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e82fc5b2ae047b2a8f894d1daf53fcf",
              "IPY_MODEL_f42e3edc203d4be1b36fd81fbb2f0637",
              "IPY_MODEL_9b82c06f561d4ea0a5338f36a7c07e94"
            ],
            "layout": "IPY_MODEL_405d88633f1e478cbcd939ead514398a"
          }
        },
        "4e82fc5b2ae047b2a8f894d1daf53fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8be5f6f952d4f9dad85d884997fdad5",
            "placeholder": "​",
            "style": "IPY_MODEL_1a778cb87dc8463db3d92e1be8bfb0aa",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "f42e3edc203d4be1b36fd81fbb2f0637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d00d381ce084d56b342edd43eb811c9",
            "max": 240691728,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9fed4b69bca453c846965117dae67fc",
            "value": 240691728
          }
        },
        "9b82c06f561d4ea0a5338f36a7c07e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a14709862d34d82970a7afa6a895f03",
            "placeholder": "​",
            "style": "IPY_MODEL_dba81a4fbd6d435ea6c2160a41626874",
            "value": " 241M/241M [00:02&lt;00:00, 126MB/s]"
          }
        },
        "405d88633f1e478cbcd939ead514398a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8be5f6f952d4f9dad85d884997fdad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a778cb87dc8463db3d92e1be8bfb0aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d00d381ce084d56b342edd43eb811c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9fed4b69bca453c846965117dae67fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a14709862d34d82970a7afa6a895f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dba81a4fbd6d435ea6c2160a41626874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76b8403ece0d48d18e6fdf1982396314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e0804df5101435690ecf10bbc70dea9",
              "IPY_MODEL_ef329ff456b94932b09aac82604bd9ba",
              "IPY_MODEL_69d2d3790366414ba431a436b0f8420b"
            ],
            "layout": "IPY_MODEL_01f32fce5411441096d29930568ca69e"
          }
        },
        "7e0804df5101435690ecf10bbc70dea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc241feec8414d94b9d7cd79d227cce6",
            "placeholder": "​",
            "style": "IPY_MODEL_922447a303d041118ac684ae99c430cc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ef329ff456b94932b09aac82604bd9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a28ba6dc52f24b80b6743c4368f8e4b4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af2a6f59e804480fae6ba34a3ff76f26",
            "value": 2
          }
        },
        "69d2d3790366414ba431a436b0f8420b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_114fa0c4dd1d4784aeed1ac8c5519357",
            "placeholder": "​",
            "style": "IPY_MODEL_c1df4de312814167a9b74546b44bf3cb",
            "value": " 2/2 [00:23&lt;00:00,  9.89s/it]"
          }
        },
        "01f32fce5411441096d29930568ca69e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc241feec8414d94b9d7cd79d227cce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "922447a303d041118ac684ae99c430cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a28ba6dc52f24b80b6743c4368f8e4b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af2a6f59e804480fae6ba34a3ff76f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "114fa0c4dd1d4784aeed1ac8c5519357": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1df4de312814167a9b74546b44bf3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9b1eadad4a24514a1bac9d429d03ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f125000fd50f4fe19168116fe9cd935e",
              "IPY_MODEL_22e05f9efb42497cb26a9b9f221f94f5",
              "IPY_MODEL_50e4259814ab4d42b66b24f01b7281d2"
            ],
            "layout": "IPY_MODEL_c615e297425a4af0b44002bb42ef9877"
          }
        },
        "f125000fd50f4fe19168116fe9cd935e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_302bd1c6e1b042d899dcbf8b5a26318b",
            "placeholder": "​",
            "style": "IPY_MODEL_8effe46e4d19462ea7e7e0b92d6d1551",
            "value": "generation_config.json: 100%"
          }
        },
        "22e05f9efb42497cb26a9b9f221f94f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b22dbc08f2db41d6b50b4121d49869ea",
            "max": 187,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91af9e5d423e443eb8dfe56289815901",
            "value": 187
          }
        },
        "50e4259814ab4d42b66b24f01b7281d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fe5f0205ec84ddb8da11ce516ca9a62",
            "placeholder": "​",
            "style": "IPY_MODEL_ee0d5db122344bdab19698765198cdf7",
            "value": " 187/187 [00:00&lt;00:00, 22.1kB/s]"
          }
        },
        "c615e297425a4af0b44002bb42ef9877": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "302bd1c6e1b042d899dcbf8b5a26318b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8effe46e4d19462ea7e7e0b92d6d1551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b22dbc08f2db41d6b50b4121d49869ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91af9e5d423e443eb8dfe56289815901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fe5f0205ec84ddb8da11ce516ca9a62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee0d5db122344bdab19698765198cdf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}